{"config":{"lang":["es"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Big Data con Python - De Cero a Produccion","text":"<p>Aprende a procesar millones de registros sin que tu computadora explote</p> <p>Repositorio educativo completo para dominar Big Data con Python, desde conceptos basicos hasta produccion.</p> <p> </p> <p>Alumnos del Curso: Como Entregar tus Trabajos</p> <p>Antes de empezar cualquier ejercicio, lee la Guia de Entregas.</p> <p>Ahi encontraras:</p> <ul> <li>Como hacer fork y clonar el repositorio</li> <li>Donde colocar tus archivos de entrega</li> <li>Como subir tu trabajo (sin Pull Request)</li> <li>Como funciona la evaluacion automatica</li> </ul>"},{"location":"#que-es-esto-y-por-que-existe","title":"Que es Esto y Por Que Existe?","text":""},{"location":"#el-problema","title":"El Problema","text":"<p>Imagina esto: Tienes un archivo Excel con 5 anos de ventas (500,000 filas). Excel se congela. Python con Pandas se queda sin memoria. Tu jefe necesita el analisis manana.</p> <p>Te suena familiar?</p> <p>Este es el problema que enfrentan miles de analistas, cientificos de datos y empresas diariamente. Los datos crecen exponencialmente, pero las herramientas tradicionales no escalan.</p>"},{"location":"#la-solucion","title":"La Solucion","text":"<p>Este repositorio te ensena a:</p> <pre><code># \u274c Antes: Excel y Pandas basico\ndf = pd.read_csv(\"ventas_5_anos.csv\")  # \ud83d\udca5 MemoryError\ndf.groupby(\"region\").sum()              # \ud83d\udc0c 20 minutos\n\n# \u2705 Despues: Big Data con Python\ndf = dd.read_csv(\"ventas_5_anos.csv\")  # \u26a1 Carga lazy\ndf.groupby(\"region\").sum().compute()    # \ud83d\ude80 2 segundos\n</code></pre> <p>Resultado: Procesas 100GB de datos en tu laptop como si fueran 10MB.</p>"},{"location":"#por-que-este-repositorio","title":"Por Que Este Repositorio","text":"<p>Este material surge de 230 horas de curso presencial donde enseno Big Data a profesionales. He destilado:</p> <ul> <li> 10+ anos de experiencia en analisis de datos</li> <li> Errores comunes que cometen los principiantes (y como evitarlos)</li> <li> Mejores practicas de la industria</li> <li> Proyectos reales adaptados para aprender</li> </ul> <p>No es solo teoria. Cada ejercicio esta disenado para enfrentarte a problemas del mundo real.</p>"},{"location":"#para-quien-es-este-repositorio","title":"Para Quien es Este Repositorio?","text":"Alumnos del Curso PresencialAutodidactas y CuriososEmpresas y Profesionales <p>Si estas inscrito en mi curso presencial:</p> <ul> <li> Este repo es tu material de apoyo completo</li> <li> Aqui encontraras todos los ejercicios del curso</li> <li> Puedes practicar antes, durante y despues de las clases</li> <li> Tienes soporte directo en las sesiones presenciales</li> </ul> <p>Guia de Entregas</p> <p>Antes de entregar, lee la Guia de Entregas</p> <p>Ventaja</p> <p>Mientras otros solo tienen diapositivas, tu tienes un repositorio completo con codigo ejecutable.</p> <p>Si encontraste este repositorio por tu cuenta:</p> <ul> <li> Todo el contenido es gratuito y de codigo abierto</li> <li> Puedes aprender a tu ritmo sin presion</li> <li> Practica con ejercicios reales de Big Data</li> <li> No incluye soporte (solo para alumnos presenciales)</li> </ul> <p>Ventaja</p> <p>Material profesional de calidad sin costo, perfecto para tu portafolio.</p> <p>Si buscas soluciones para tu empresa:</p> <ul> <li> Portfolio real de capacidades en Big Data</li> <li> Muestra como entreno equipos profesionales</li> <li> Consultoria y capacitacion in-company disponible</li> <li> Proyectos de analisis de datos a medida</li> </ul> <p>Ventaja</p> <p>Ve exactamente que nivel de calidad ofrezco antes de contratarme.</p>"},{"location":"#que-aprenderas","title":"Que Aprenderas?","text":""},{"location":"#tecnologias-que-dominaras","title":"Tecnologias que Dominaras","text":"Tecnologia Que Hace Cuando Usarla Python Lenguaje base Siempre Pandas Datos en memoria (&lt; 5GB) Analisis exploratorio Dask Datos &gt; RAM (5-100GB) Datasets grandes en 1 maquina PySpark Datos masivos (&gt; 100GB) Clusters, produccion SQLite Base de datos embebida Prototipos, proyectos pequenos Parquet Formato columnar Almacenar datos procesados Git/GitHub Control de versiones Todo proyecto profesional Flask Web framework Dashboards, APIs"},{"location":"#ejemplos-de-que-podras-hacer","title":"Ejemplos de Que Podras Hacer","text":"<p>Ejemplo 1: Analizar 10 Millones de Viajes de Taxi</p> <pre><code># Dataset: NYC Taxi (121 MB CSV, 10M+ registros)\n# Pregunta: Cual es el ingreso promedio por hora del dia?\n\nimport dask.dataframe as dd\n\n# Cargar 121 MB como si fueran 10 MB \u26a1\ndf = dd.read_csv(\"yellow_tripdata_2021-01.csv\")\n\n# Analisis que en Pandas tomaria 5 minutos, aqui: 10 segundos\nresultado = (df.groupby(df['tpep_pickup_datetime'].dt.hour)\n              ['total_amount']\n              .mean()\n              .compute())\n\nprint(resultado)\n# Resultado: Hora 23 es la mas rentable ($18.50 promedio)\n</code></pre> <p>Ejemplo 2: Dashboard en Tiempo Real</p> <p>Crear un dashboard interactivo que muestra:</p> <ul> <li> Distribucion de viajes por hora</li> <li> Mapa de calor de zonas mas rentables</li> <li> Ingresos totales por dia/semana/mes</li> <li> Tendencias temporales</li> </ul> <p>Ejemplo 3: Pipeline ETL de Produccion</p> <pre><code>CSV (100GB) \u2192 Limpiar \u2192 Transformar \u2192 Parquet \u2192 Dashboard\n              (Dask)    (PySpark)    (10GB)     (Flask)\n</code></pre>"},{"location":"#como-empezar","title":"Como Empezar?","text":"<p>Primera Vez con Git y Python?</p> <p>Empieza con nuestra Guia de Instalacion donde te explicamos paso a paso como instalar todas las herramientas necesarias.</p> <p>Ya tienes Git y Python?</p> <p>Ve directo a Tu Primer Ejercicio para comenzar a trabajar.</p> <p>Desarrollador Experimentado?</p> <p>Revisa el Roadmap del Curso para ver todos los ejercicios disponibles y elegir por donde empezar.</p>"},{"location":"#estadisticas-del-repositorio","title":"Estadisticas del Repositorio","text":""},{"location":"#servicios-profesionales","title":"Servicios Profesionales","text":""},{"location":"#consultoria-en-big-data","title":"Consultoria en Big Data","text":"<p>Necesitas ayuda con un proyecto de datos en tu empresa?</p> <p>Ofrezco:</p> <ul> <li> Desarrollo de Pipelines ETL/ELT con Python y Spark</li> <li> Capacitacion Empresarial (cursos personalizados para tu equipo)</li> <li> Analisis de Datos para insights accionables</li> <li> Automatizacion de Procesos de datos</li> <li> Migracion a Big Data (de Excel/SQL a Dask/Spark)</li> </ul> <p>Casos de Uso</p> <p>Empresa A: \"Tenemos 5 anos de ventas en Excel y toma 2 horas generar reportes\"</p> <p>\u2192 Solucion: Pipeline automatizado con Dask + Dashboard en tiempo real \u2192 Resultado: Reportes en 30 segundos</p> <p>Empresa B: \"Queremos capacitar a 15 analistas en Big Data\"</p> <p>\u2192 Solucion: Curso in-company de 40 horas adaptado a su industria \u2192 Resultado: Equipo autonomo procesando TB de datos</p> <p>Startup C: \"Necesitamos procesar logs de servidores (1TB/dia)\"</p> <p>\u2192 Solucion: Pipeline PySpark en AWS EMR \u2192 Resultado: Analisis en tiempo real con costos optimizados</p>"},{"location":"#contacto","title":"Contacto","text":"<p> Email: cursos@todoeconometria.com LinkedIn: Juan Gutierrez Web: www.todoeconometria.com</p>"},{"location":"#contribuciones","title":"Contribuciones","text":"<p>Este repositorio esta en constante evolucion. Si encuentras:</p> <ul> <li> Errores o bugs</li> <li> Mejoras en la documentacion</li> <li> Ideas para nuevos ejercicios</li> <li> Ejemplos de dashboards</li> </ul> <p>Crea un Issue o Pull Request</p> <ol> <li>Fork este repositorio</li> <li>Crea una rama (<code>git checkout -b feature/nueva-funcionalidad</code>)</li> <li>Commit tus cambios (<code>git commit -m 'Agregar nueva funcionalidad'</code>)</li> <li>Push a la rama (<code>git push origin feature/nueva-funcionalidad</code>)</li> <li>Abre un Pull Request</li> </ol>"},{"location":"#licencia","title":"Licencia","text":"<p>Este proyecto esta bajo la Licencia MIT - ver el archivo LICENSE para detalles.</p> <p>En resumen: Puedes usar este material para aprender, ensenar, o modificar, siempre que des credito.</p>"},{"location":"#listo-para-empezar","title":"Listo para Empezar?","text":"<pre><code># 1. Haz fork de este repositorio (boton arriba a la derecha)\n\n# 2. Clona TU fork\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# 3. Instala dependencias\ncd ejercicios-bigdata\npip install -r requirements.txt\n\n# 4. Empieza con el Ejercicio 01\ncd ejercicios\npython 01_cargar_sqlite.py\n\n# 5. Aprende, practica, crece!\n</code></pre> <p> Tu carrera en Big Data empieza aqui.   Preguntas? Abre un Issue o contactame en LinkedIn </p> <p>   Hecho con \u2764\ufe0f por TodoEconometria </p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/","title":"Guia de Entrega de Dashboards","text":"<p>Esta guia explica como crear y entregar dashboards para el curso.</p> <p>Sistema de Evaluacion por PROMPTS</p> <p>NO se usan Pull Requests. El sistema evalua tu archivo <code>PROMPTS.md</code> directamente en tu fork. Solo necesitas hacer <code>git push</code>.</p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#resumen-del-flujo","title":"Resumen del Flujo","text":"<pre><code>1. Trabajas en TU fork (tu copia del repositorio)\n2. Creas tu dashboard en la carpeta correcta\n3. Documentas tus prompts de IA en PROMPTS.md\n4. Subes con: git add . &amp;&amp; git commit -m \"mensaje\" &amp;&amp; git push\n5. El sistema evalua automaticamente tu PROMPTS.md\n</code></pre>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<pre><code>dashboards/\n\u251c\u2500\u2500 nyc_taxi_eda/          # Ejemplo de referencia del profesor\n\u2502   \u251c\u2500\u2500 app.py\n\u2502   \u251c\u2500\u2500 templates/\n\u2502   \u2514\u2500\u2500 README.md\n\u2514\u2500\u2500 tu-nombre-dashboard/   # Tu dashboard aqui\n    \u251c\u2500\u2500 app.py\n    \u251c\u2500\u2500 templates/\n    \u251c\u2500\u2500 PROMPTS.md         # LO MAS IMPORTANTE - Tus prompts de IA\n    \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#pasos-para-entregar-tu-dashboard","title":"Pasos para Entregar tu Dashboard","text":""},{"location":"GUIA_ENTREGA_DASHBOARDS/#1-actualizar-tu-fork","title":"1. Actualizar tu Fork","text":"<p>Antes de empezar, asegurate de tener la ultima version:</p> <pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#2-crear-tu-carpeta-de-dashboard","title":"2. Crear tu Carpeta de Dashboard","text":"<p>Crea una carpeta dentro de <code>dashboards/</code> con tu nombre:</p> <pre><code>dashboards/\n\u2514\u2500\u2500 tu-nombre-dashboard/\n    \u251c\u2500\u2500 app.py              # Tu aplicacion Flask\n    \u251c\u2500\u2500 templates/          # Tus archivos HTML\n    \u2502   \u2514\u2500\u2500 index.html\n    \u251c\u2500\u2500 static/             # CSS, JS, imagenes (opcional)\n    \u251c\u2500\u2500 PROMPTS.md          # OBLIGATORIO - Tus prompts de IA\n    \u2514\u2500\u2500 README.md           # Documentacion de tu dashboard\n</code></pre> <p>Ejemplos de nombre de carpeta: - <code>garcia-taxi-eda</code> - <code>lopez-analisis-nyc</code> - <code>martinez-dashboard</code></p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#3-el-archivo-mas-importante-promptsmd","title":"3. El Archivo mas Importante: PROMPTS.md","text":"<p>PROMPTS.md es lo que se evalua</p> <p>No el codigo, no el diseno. Tus prompts de IA.</p> <p>Tu archivo <code>PROMPTS.md</code> debe contener:</p> <pre><code># Prompts de IA - Dashboard [Tu Nombre]\n\n## Prompt A: [Titulo descriptivo]\n\n**IA usada:** ChatGPT / Claude / Copilot / etc.\n\n**Prompt exacto (copiado tal cual):**\n&gt; [Pega aqui tu prompt REAL, con errores y todo]\n\n**Captura:** Ver `capturas/prompt_A.png`\n\n---\n\n## Prompt B: [Titulo descriptivo]\n\n[Mismo formato...]\n\n---\n\n## Prompt C: [Titulo descriptivo]\n\n[Mismo formato...]\n\n---\n\n## Blueprint Final\n\n[Al terminar, pide a tu IA que genere un resumen tecnico\nde lo que construiste: stack, arquitectura, decisiones]\n</code></pre> <p>NO limpies tus prompts</p> <p>Si escribiste \"como ago q flask lea el csv\" con errores, pega ESO. El sistema detecta si los prompts fueron \"limpiados\".</p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#4-requisitos-minimos-del-dashboard","title":"4. Requisitos Minimos del Dashboard","text":"<p>Tu dashboard debe incluir:</p> <ol> <li>Al menos 3 visualizaciones diferentes</li> <li>Estadisticas descriptivas (media, mediana, conteos)</li> <li>Analisis de calidad de datos (nulos, tipos, outliers)</li> <li>README.md explicando tu trabajo</li> <li>PROMPTS.md con tus prompts de IA</li> </ol>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#5-probar-localmente","title":"5. Probar Localmente","text":"<p>Antes de subir, verifica que funciona:</p> <pre><code>cd dashboards/tu-nombre-dashboard/\npython app.py\n</code></pre> <p>Abre http://localhost:5000 y verifica:</p> <ul> <li>El dashboard carga sin errores</li> <li>Las visualizaciones se muestran</li> <li>Los datos se cargan correctamente</li> </ul>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#6-subir-tu-trabajo","title":"6. Subir tu Trabajo","text":"<pre><code># Desde la raiz del repositorio\ngit add dashboards/tu-nombre-dashboard/\ngit commit -m \"Dashboard EDA - [Tu Nombre]\"\ngit push\n</code></pre> <p>Listo!</p> <p>No necesitas hacer nada mas. El sistema evalua tu PROMPTS.md automaticamente.</p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#checklist-de-entrega","title":"Checklist de Entrega","text":"<p>Antes de hacer push, verifica:</p> <ul> <li> Mi carpeta esta en <code>dashboards/mi-nombre-dashboard/</code></li> <li> Incluyo <code>PROMPTS.md</code> con mis prompts reales</li> <li> Incluyo capturas de pantalla de mis prompts</li> <li> Incluyo <code>app.py</code> funcional</li> <li> Incluyo <code>README.md</code> documentando mi trabajo</li> <li> Mi dashboard tiene al menos 3 visualizaciones</li> <li> Probe localmente y funciona</li> <li> Hice <code>git push</code> a mi fork</li> </ul>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#que-no-subir","title":"Que NO Subir","text":"<p>El <code>.gitignore</code> protege esto, pero recuerda:</p> <ul> <li>Archivos de datos (<code>.csv</code>, <code>.parquet</code>, <code>.db</code>)</li> <li>Entornos virtuales (<code>venv/</code>, <code>.venv/</code>)</li> <li>Archivos <code>__pycache__/</code></li> <li>Credenciales (<code>.env</code>)</li> </ul>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#como-se-evalua","title":"Como se Evalua","text":"Aspecto Peso Que se evalua PROMPTS.md 40% Calidad y autenticidad de tus prompts Funcionalidad 30% Dashboard carga y funciona Analisis 20% Visualizaciones e interpretacion Documentacion 10% README claro <p>El codigo NO se revisa linea por linea</p> <p>Lo importante es tu proceso de aprendizaje documentado en PROMPTS.md.</p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#ejemplo-de-referencia","title":"Ejemplo de Referencia","text":"<p>Puedes consultar el dashboard de ejemplo en <code>dashboards/nyc_taxi_eda/</code>.</p> <p>No copies el codigo. Usalo como referencia para entender la estructura.</p>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#errores-comunes","title":"Errores Comunes","text":""},{"location":"GUIA_ENTREGA_DASHBOARDS/#no-such-file-or-directory-datosnyc_taxicsv","title":"\"No such file or directory: datos/nyc_taxi.csv\"","text":"<pre><code>DATA_PATH = os.path.join('..', '..', 'datos', 'nyc_taxi.csv')\n</code></pre>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#port-5000-is-already-in-use","title":"\"Port 5000 is already in use\"","text":"<pre><code>app.run(debug=True, port=5001)  # Cambia el puerto\n</code></pre>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#modulenotfounderror","title":"\"ModuleNotFoundError\"","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"GUIA_ENTREGA_DASHBOARDS/#recursos","title":"Recursos","text":"<ul> <li>Flask Docs</li> <li>Chart.js</li> <li>Plotly Python</li> <li>Guia de Entregas General</li> </ul> <p>Ultima actualizacion: 2026-02-04</p>"},{"location":"INSTRUCCIONES_ALUMNOS/","title":"Instrucciones para Alumnos","text":"<p>Esta pagina ha sido reemplazada por la nueva guia unificada.</p>"},{"location":"INSTRUCCIONES_ALUMNOS/#nueva-guia-de-entregas","title":"Nueva Guia de Entregas","text":"<p>Por favor, consulta la Guia de Entregas para instrucciones actualizadas sobre:</p> <ul> <li>Como hacer fork y clonar el repositorio</li> <li>Donde colocar tus archivos de entrega</li> <li>Como subir tu trabajo (sin Pull Request)</li> <li>El archivo PROMPTS.md y como se evalua</li> <li>Problemas comunes y soluciones</li> </ul> <p>Sistema Actualizado (2026)</p> <p>Ya NO se usan Pull Requests para entregar.</p> <p>El nuevo sistema evalua tu archivo <code>PROMPTS.md</code> directamente en tu fork. Solo necesitas hacer <code>git push</code> a tu fork.</p> <p>Ir a: Guia de Entregas</p>"},{"location":"faq/","title":"Preguntas Frecuentes (FAQ)","text":"<p>Respuestas a las preguntas mas comunes sobre el curso.</p>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#necesito-experiencia-previa-en-big-data","title":"Necesito experiencia previa en Big Data?","text":"<p>No. El curso empieza desde cero. Solo necesitas:</p> <ul> <li>Conocimientos basicos de Python</li> <li>Saber usar la terminal/consola</li> <li>Ganas de aprender</li> </ul> <p>No tienes Python?</p> <p>Ve a la Guia de Instalacion donde te explicamos como instalar todo desde cero.</p>"},{"location":"faq/#cuanto-tiempo-toma-completar-los-ejercicios","title":"Cuanto tiempo toma completar los ejercicios?","text":"<p>Depende de tu nivel:</p> Nivel Tiempo Total Horas/Semana Duracion Principiante 120-140 horas 10-15h 10-12 semanas Intermedio 60-80 horas 8-10h 6-8 semanas Avanzado 40-50 horas 5-8h 4-5 semanas <p>No hay prisa. Aprende a tu ritmo. Lo importante es entender bien cada concepto.</p>"},{"location":"faq/#los-datos-son-reales-o-sinteticos","title":"Los datos son reales o sinteticos?","text":"<p>Reales. Usamos datasets publicos reales:</p> <ul> <li>NYC Taxi &amp; Limousine Commission (TLC)</li> <li>Weather data de NOAA</li> <li>Otros datasets publicos de Kaggle</li> </ul> <p>Esto te da experiencia con datos del mundo real (sucios, incompletos, grandes).</p>"},{"location":"faq/#puedo-usar-esto-en-mi-portafolio","title":"Puedo usar esto en mi portafolio?","text":"<p>Si! De hecho, te lo recomendamos.</p> <p>Muchos alumnos han conseguido trabajo mostrando:</p> <ul> <li>Sus soluciones de los ejercicios</li> <li>El dashboard que crearon</li> <li>Su fork de GitHub con commits profesionales</li> </ul> <p>Consejo</p> <p>Haz tu fork publico y agrega un README personalizado explicando tu aprendizaje.</p>"},{"location":"faq/#hay-certificado-al-terminar","title":"Hay certificado al terminar?","text":"<p>Para alumnos del curso presencial: Si, certificado de 230 horas.</p> <p>Para autodidactas: No hay certificado oficial, pero tu GitHub es tu certificado. Los empleadores valoran mas ver tu codigo que un PDF.</p> <p>Tu GitHub es tu Certificado</p> <ul> <li>Commits profesionales</li> <li>Codigo bien documentado</li> <li>Proyectos completos</li> <li>Contribuciones activas</li> </ul>"},{"location":"faq/#tecnico","title":"Tecnico","text":""},{"location":"faq/#que-computadora-necesito","title":"Que computadora necesito?","text":"<p>Minimo:</p> <ul> <li>8GB RAM</li> <li>20GB espacio en disco</li> <li>Procesador i5 o equivalente</li> <li>Windows 10+, macOS 10.14+, o Linux (Ubuntu 20.04+)</li> </ul> <p>Recomendado:</p> <ul> <li>16GB RAM</li> <li>50GB espacio en disco SSD</li> <li>Procesador i7 o equivalente</li> </ul> <p>No tienes buenos recursos?</p> <p>Puedes usar Google Colab o GitHub Codespaces (gratis) para trabajar en la nube.</p>"},{"location":"faq/#funciona-en-windowsmaclinux","title":"Funciona en Windows/Mac/Linux?","text":"<p>Si. El curso es compatible con los tres sistemas operativos.</p> <ul> <li>Windows: Preferiblemente Windows 10 o superior</li> <li>macOS: macOS 10.14 (Mojave) o superior</li> <li>Linux: Ubuntu 20.04+, Fedora, Arch, etc.</li> </ul> <p>La Guia de Instalacion tiene instrucciones especificas para cada sistema.</p>"},{"location":"faq/#puedo-usar-otro-ide-en-lugar-de-pycharm","title":"Puedo usar otro IDE en lugar de PyCharm?","text":"<p>Si. PyCharm es recomendado pero no obligatorio.</p> <p>Alternativas:</p> <ul> <li>Visual Studio Code - Ligero y muy popular</li> <li>Jupyter Lab - Excelente para notebooks</li> <li>Sublime Text - Editor de texto avanzado</li> <li>Vim/Emacs - Si eres usuario avanzado</li> </ul> <p>Lo importante es que te sientas comodo con tu herramienta.</p>"},{"location":"faq/#como-descargo-los-datos","title":"Como descargo los datos?","text":"<p>Los datos se descargan automaticamente con un script:</p> <pre><code># Ir a la carpeta de datos\ncd datos\n\n# Ejecutar script de descarga\npython descargar_datos.py\n</code></pre> <p>El script descarga y descomprime automaticamente todos los datasets necesarios.</p> <p>Espacio en disco</p> <p>Los datasets completos ocupan ~5GB. Asegurate de tener suficiente espacio.</p>"},{"location":"faq/#git-y-github","title":"Git y GitHub","text":""},{"location":"faq/#nunca-he-usado-git-es-muy-dificil","title":"Nunca he usado Git. Es muy dificil?","text":"<p>No es dificil, pero requiere practica.</p> <p>Tenemos guias completas paso a paso:</p> <ol> <li>Fork y Clone - Lo mas basico</li> <li>Tu Primer Ejercicio - Workflow completo</li> <li>Comandos Utiles - Referencia rapida</li> </ol> <p>Aprende haciendo</p> <p>La mejor forma de aprender Git es usandolo. Los primeros commits seran raros, pero mejoras rapido.</p>"},{"location":"faq/#que-es-un-fork-por-que-necesito-uno","title":"Que es un Fork? Por que necesito uno?","text":"<p>Fork = Tu copia personal del repositorio en GitHub.</p> <p>Lo necesitas porque:</p> <ul> <li> No puedes modificar el repositorio del profesor directamente</li> <li> El fork es TU espacio para trabajar</li> <li> Puedes sincronizarlo con el original</li> <li> El sistema evalua automaticamente tus PROMPTS en tu fork</li> </ul> <p>Ver guia completa: Fork y Clone</p>"},{"location":"faq/#como-mantengo-mi-fork-actualizado","title":"Como mantengo mi Fork actualizado?","text":"<p>Cuando el profesor agregue ejercicios nuevos, debes sincronizar tu fork.</p> <p>Metodo facil (GitHub Web):</p> <ol> <li>Ve a tu fork en GitHub</li> <li>Click \"Sync fork\" \u2192 \"Update branch\"</li> <li>En tu PC: <code>git pull origin main</code></li> </ol> <p>Metodo completo (Terminal):</p> <pre><code>git checkout main\ngit fetch upstream\ngit merge upstream/main\ngit push origin main\n</code></pre> <p>Ver guia completa: Sincronizar Fork</p>"},{"location":"faq/#hice-un-commit-mal-como-lo-deshago","title":"Hice un commit mal. Como lo deshago?","text":"<p>Antes de hacer push:</p> <pre><code># Deshacer ultimo commit (mantiene cambios)\ngit reset --soft HEAD~1\n\n# Deshacer ultimo commit (descarta cambios)\ngit reset --hard HEAD~1\n</code></pre> <p>Despues de hacer push:</p> <pre><code># Crear nuevo commit que revierte el anterior\ngit revert HEAD\ngit push origin tu-rama\n</code></pre> <p>Evita force push</p> <p>Nunca uses <code>git push --force</code> en ramas compartidas o Pull Requests abiertos.</p>"},{"location":"faq/#ejercicios","title":"Ejercicios","text":""},{"location":"faq/#no-puedo-completar-un-ejercicio-que-hago","title":"No puedo completar un ejercicio. Que hago?","text":"<p>Paso 1: Lee el error cuidadosamente</p> <p>La mayoria de las veces el error te dice exactamente que esta mal.</p> <p>Paso 2: Busca en Google</p> <p>Copia el mensaje de error y buscalo. Probablemente alguien mas ya lo tuvo.</p> <p>Paso 3: Revisa la documentacion</p> <ul> <li>Pandas Docs</li> <li>SQLite Tutorial</li> <li>Python Docs</li> </ul> <p>Paso 4: Pide ayuda</p> <ul> <li>Alumnos presenciales: Consulta en clase</li> <li>Autodidactas: Crea un Issue en GitHub explicando tu problema</li> </ul> <p>Como pedir ayuda</p> <p>Incluye:</p> <ul> <li>Que intentaste hacer</li> <li>Que error obtuviste (mensaje completo)</li> <li>Que ya probaste</li> <li>Tu codigo relevante</li> </ul>"},{"location":"faq/#puedo-hacer-los-ejercicios-en-desorden","title":"Puedo hacer los ejercicios en desorden?","text":"<p>No recomendado. Los ejercicios estan disenados para:</p> <ul> <li>Construir sobre conocimientos previos</li> <li>Aumentar dificultad gradualmente</li> <li>Introducir conceptos en orden logico</li> </ul> <p>Excepcion</p> <p>Si ya tienes experiencia con Python y Pandas, puedes empezar en el NIVEL 2 (Ejercicio 03).</p>"},{"location":"faq/#cuantas-veces-puedo-intentar-un-ejercicio","title":"Cuantas veces puedo intentar un ejercicio?","text":"<p>Las que necesites. No hay limite de intentos.</p> <p>El objetivo es aprender, no aprobar rapidamente.</p> <ul> <li>Puedes actualizar tu fork cuantas veces quieras con <code>git push</code></li> <li>El sistema de evaluacion automatica revisa tu archivo PROMPTS.md</li> <li>Aprendes mas de los errores que de los aciertos</li> </ul>"},{"location":"faq/#puedo-usar-librerias-adicionales","title":"Puedo usar librerias adicionales?","text":"<p>Si, pero:</p> <ol> <li>Justifica por que las necesitas</li> <li>Agregalas a <code>requirements.txt</code></li> <li>Documenta como instalarlas</li> <li>Menciona en PROMPTS.md que librerias usaste</li> </ol> <p>Ejemplo</p> <p>Si usas <code>seaborn</code> para visualizaciones:</p> <pre><code># requirements.txt\npandas==2.0.0\nseaborn==0.12.0  # Para visualizaciones avanzadas\n</code></pre> <p>Y en tu archivo <code>PROMPTS.md</code> menciona: \"Use seaborn para crear graficos mas profesionales\"</p>"},{"location":"faq/#soporte","title":"Soporte","text":""},{"location":"faq/#ofrecen-soporte-si-me-atoro","title":"Ofrecen soporte si me atoro?","text":"<p>Para alumnos del curso presencial:</p> <ul> <li> Soporte completo en las sesiones</li> <li> Consultas por email</li> <li> Revision automatica de entregas</li> </ul> <p>Para autodidactas:</p> <ul> <li> No hay soporte directo</li> <li> Puedes crear Issues en GitHub</li> <li> Comunidad puede ayudarte</li> <li> Documentacion completa disponible</li> </ul>"},{"location":"faq/#como-contacto-al-instructor","title":"Como contacto al instructor?","text":"<p>Para consultas del curso:</p> <ul> <li>GitHub Issues: Crear Issue</li> <li>Email: cursos@todoeconometria.com</li> </ul> <p>Para consultoria empresarial:</p> <ul> <li>Email: cursos@todoeconometria.com</li> <li>LinkedIn: Juan Gutierrez</li> <li>Web: TodoEconometria</li> </ul> <p>Tiempo de respuesta</p> <ul> <li>Alumnos presenciales: 24-48 horas</li> <li>Autodidactas via Issues: Cuando este disponible</li> <li>Empresas: 24 horas</li> </ul>"},{"location":"faq/#problemas-tecnicos","title":"Problemas Tecnicos","text":""},{"location":"faq/#python-no-se-reconoce-como-comando","title":"Python no se reconoce como comando","text":"<p>Windows:</p> <ol> <li>Reinstala Python</li> <li>Marca \"Add Python to PATH\"</li> <li>Reinicia la terminal</li> </ol> <p>macOS/Linux:</p> <p>Usa <code>python3</code> en lugar de <code>python</code>:</p> <pre><code>python3 --version\npip3 install pandas\n</code></pre>"},{"location":"faq/#error-modulenotfounderror","title":"Error: ModuleNotFoundError","text":"<p>Causa: No instalaste las dependencias.</p> <p>Solucion:</p> <pre><code># Activa el entorno virtual\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\n\n# Instala dependencias\npip install -r requirements.txt\n</code></pre>"},{"location":"faq/#git-dice-fatal-not-a-git-repository","title":"Git dice \"fatal: not a git repository\"","text":"<p>Causa: No estas en la carpeta del proyecto.</p> <p>Solucion:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica\ngit status  # Deberia funcionar\n</code></pre>"},{"location":"faq/#no-puedo-hacer-push-permission-denied","title":"No puedo hacer push: \"Permission denied\"","text":"<p>Causa: Problemas de autenticacion con GitHub.</p> <p>Solucion rapida (HTTPS):</p> <pre><code># Cambiar a HTTPS\ngit remote set-url origin https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Intentar push de nuevo\ngit push origin tu-rama\n</code></pre> <p>Solucion permanente (SSH):</p> <p>Configura SSH keys: GitHub SSH Guide</p>"},{"location":"faq/#el-dashboard-no-carga-los-datos","title":"El dashboard no carga los datos","text":"<p>Verificar:</p> <ol> <li> <p>La base de datos existe?    <pre><code>ls datos/taxi.db\n</code></pre></p> </li> <li> <p>Flask esta corriendo?    <pre><code>python app.py\n</code></pre></p> </li> <li> <p>Puerto correcto? (default: 5000)    <pre><code>http://localhost:5000\n</code></pre></p> </li> <li> <p>Revisar consola de errores del navegador (F12)</p> </li> </ol>"},{"location":"faq/#carrera-y-empleo","title":"Carrera y Empleo","text":""},{"location":"faq/#este-curso-me-ayudara-a-conseguir-trabajo","title":"Este curso me ayudara a conseguir trabajo?","text":"<p>Puede ayudar mucho, especialmente si:</p> <ul> <li>Completas todos los ejercicios con calidad</li> <li>Creas un dashboard profesional</li> <li>Documentas bien tu codigo</li> <li>Mantienes un GitHub activo</li> </ul> <p>Que valoran los empleadores</p> <ol> <li>Portfolio de proyectos (tu GitHub)</li> <li>Codigo limpio y documentado</li> <li>Experiencia con datos reales</li> <li>Capacidad de resolver problemas</li> </ol> <p>Un repositorio bien trabajado vale mas que 10 certificados.</p>"},{"location":"faq/#que-trabajos-puedo-conseguir-con-estas-habilidades","title":"Que trabajos puedo conseguir con estas habilidades?","text":"<p>Con las habilidades del curso puedes aplicar a:</p> <ul> <li>Data Analyst - Analisis de datos con Python/SQL</li> <li>Junior Data Scientist - Modelado y analisis avanzado</li> <li>Data Engineer - Pipelines ETL, procesamiento de datos</li> <li>Business Intelligence Developer - Dashboards y reportes</li> <li>Python Developer - Desarrollo backend con datos</li> </ul>"},{"location":"faq/#necesito-un-titulo-universitario","title":"Necesito un titulo universitario?","text":"<p>Depende del empleador.</p> <ul> <li>Empresas tech: Valoran mas el portfolio que el titulo</li> <li>Empresas tradicionales: Pueden requerir titulo</li> <li>Startups: Portfolio &gt; Titulo</li> <li>Freelance: Solo importa tu trabajo</li> </ul> <p>Compensar falta de titulo</p> <ul> <li>Portfolio solido en GitHub</li> <li>Certificaciones relevantes</li> <li>Proyectos personales impresionantes</li> <li>Contribuciones open source</li> </ul>"},{"location":"faq/#otros","title":"Otros","text":""},{"location":"faq/#puedo-compartir-mi-solucion-publicamente","title":"Puedo compartir mi solucion publicamente?","text":"<p>Si, pero considera:</p> <ul> <li>Despues de completar: Compartir despues de recibir tu nota</li> <li>Con creditos: Menciona que es del curso de TodoEconometria</li> <li>No spoilers: No compartas soluciones para ayudar a hacer trampa</li> </ul> <p>Compartir es bueno</p> <p>Compartir tu codigo ayuda a:</p> <ul> <li>Otros a aprender</li> <li>Construir tu marca personal</li> <li>Demostrar habilidades a empleadores</li> </ul>"},{"location":"faq/#el-curso-se-actualiza","title":"El curso se actualiza?","text":"<p>Si. El repositorio se actualiza regularmente con:</p> <ul> <li>Nuevos ejercicios</li> <li>Mejoras en los existentes</li> <li>Actualizaciones de librerias</li> <li>Nuevos datasets</li> <li>Correcciones de bugs</li> </ul> <p>Mant\u00e9n tu fork sincronizado para obtener actualizaciones: Sincronizar Fork</p>"},{"location":"faq/#puedo-contribuir-al-curso","title":"Puedo contribuir al curso?","text":"<p>Si! Las contribuciones son bienvenidas.</p> <p>Puedes contribuir:</p> <ul> <li> Reportando bugs via Issues</li> <li> Mejorando documentacion</li> <li> Sugiriendo nuevos ejercicios</li> <li> Compartiendo tu dashboard en la galeria</li> </ul> <p>Ver seccion de Contribuciones</p>"},{"location":"faq/#donde-puedo-aprender-mas","title":"Donde puedo aprender mas?","text":"<p>Recursos recomendados:</p> <ul> <li>Python for Data Analysis - Libro de Wes McKinney</li> <li>SQL Tutorial - SQL interactivo</li> <li>Dask Tutorial - Tutorial oficial de Dask</li> <li>r/datascience - Comunidad en Reddit</li> </ul> <p>Cursos complementarios:</p> <ul> <li>Python for Data Science (Coursera)</li> <li>SQL for Data Science (DataCamp)</li> <li>Apache Spark (Udacity)</li> </ul>"},{"location":"faq/#preguntas-no-resueltas","title":"Preguntas no Resueltas?","text":"<p>No encontraste tu respuesta?</p> <p>Alumnos presenciales: Consulta en la proxima sesion o envia un email</p> <p>Autodidactas: Crea un Issue en GitHub:</p> <p>Crear Issue</p> <p>Incluye: - Titulo descriptivo - Descripcion detallada de tu pregunta - Contexto (que ejercicio, que intentaste, etc.)</p>"},{"location":"faq/#recursos-adicionales","title":"Recursos Adicionales","text":"<ul> <li>Guia de Inicio - Empezar desde cero</li> <li>Git y GitHub - Guias de Git</li> <li>Ejercicios - Lista de ejercicios</li> <li>Roadmap - Plan de estudio</li> </ul>"},{"location":"infraestructura/","title":"\ud83c\udfd7\ufe0f Infraestructura del Laboratorio","text":"<p>\"Evita que Big Data colapse tu disco principal\"</p> <p>Para realizar este curso, hemos dise\u00f1ado una arquitectura h\u00edbrida que combina la facilidad de uso de Windows con la potencia de servidores Linux aislados en Docker.</p> <p>Esta gu\u00eda te explica qu\u00e9 est\u00e1 pasando \"bajo el cap\u00f3\" cuando ejecutas el script de instalaci\u00f3n.</p>"},{"location":"infraestructura/#arquitectura-hibrida","title":"\ud83e\udde9 Arquitectura H\u00edbrida","text":"<p>Nuestro entorno utiliza tres capas:</p> <ol> <li>Capa de Usuario (T\u00fa): Escribes c\u00f3digo Python en PyCharm/VSCode.</li> <li>Capa de C\u00f3mputo (Docker): Servicios pesados (Spark, Postgres) corren aislados.</li> <li>Capa de Datos (Storage): Los archivos masivos se guardan en un disco dedicado.</li> </ol> <pre><code>graph TD\n    subgraph \"Tu Ordenador\"\n        IDE[PyCharm / VSCode] --&gt;|Ejecuta| PY[Scripts Python]\n        PY --&gt;|Conecta puerto 5432| PG[(PostgreSQL)]\n        PY --&gt;|Conecta puerto 7077| SPARK[Spark Cluster]\n    end\n\n    subgraph \"Docker (Contenedores)\"\n        PG\n        SPARK\n        ADMIN[pgAdmin4]\n    end\n\n    subgraph \"Almacenamiento F\u00edsico\"\n        SSD[SSD Externo / Partici\u00f3n Datos]\n        HDD[Disco Sistema C:]\n    end\n\n    PG --&gt;|Guarda| SSD\n    SPARK --&gt;|Lee| SSD\n    PY -.-&gt;|Junction Link| SSD</code></pre>"},{"location":"infraestructura/#el-dilema-del-almacenamiento-hdd-vs-ssd","title":"\ud83d\udcbe El Dilema del Almacenamiento (HDD vs SSD)","text":"<p>En Proyectos de Datos, el I/O (Velocidad de disco) es cr\u00edtico.</p>"},{"location":"infraestructura/#opcion-a-ssd-externo-modo-pro","title":"\ud83d\ude80 Opci\u00f3n A: SSD Externo (Modo PRO)","text":"<p>Si tienes el laboratorio configurado en un SSD externo (<code>E:\\BIGDATA_LAB_STORAGE</code>): 1.  Velocidad: Docker tiene un carril exclusivo para leer/escribir datos. 2.  Seguridad: Si descargas un dataset de 50GB, tu Windows no se queda sin espacio. 3.  El \"T\u00fanel M\u00e1gico\" (Junction): Windows crea un enlace simb\u00f3lico. T\u00fa ves la carpeta <code>datos/</code> en tu proyecto, pero f\u00edsicamente los bytes est\u00e1n en el SSD externo.</p>"},{"location":"infraestructura/#opcion-b-disco-local","title":"\ud83d\udc22 Opci\u00f3n B: Disco Local","text":"<p>Si usas tu disco principal (<code>C:</code>): - El sistema funcionar\u00e1 igual, pero debes vigilar el espacio libre. - Docker y Windows competir\u00e1n por el uso del disco (performance menor).</p>"},{"location":"infraestructura/#servicios-docker-el-stack","title":"\ud83d\udc33 Servicios Docker (El \"Stack\")","text":"<p>El script <code>setup_cluster.ps1</code> levanta estos servicios autom\u00e1ticamente:</p>"},{"location":"infraestructura/#1-postgresql-el-data-warehouse","title":"1. PostgreSQL (El Data Warehouse)","text":"<ul> <li>Por qu\u00e9 SQL: Aunque procesamos con Spark/Dask, los datos finales \"de valor\" deben residir en una base de datos estructurada para ser consumidos por Dashboards o Analistas.</li> <li>Por qu\u00e9 no Mongo/Cassandra: Para este nivel de datos (Gigabytes estructurados), PostgreSQL es m\u00e1s eficiente en recursos locales y es el est\u00e1ndar de oro para la capa de \"Serving\".</li> <li>Acceso: <code>localhost:5432</code></li> </ul>"},{"location":"infraestructura/#2-apache-spark-el-motor-de-proceso","title":"2. Apache Spark (El Motor de Proceso)","text":"<ul> <li>Master + Worker: Simulamos un cluster real. El \"Master\" reparte tareas y el \"Worker\" las ejecuta.</li> <li>Acceso: <code>localhost:8081</code> (Dashboard)</li> </ul>"},{"location":"infraestructura/#3-pgadmin-4-la-interfaz","title":"3. pgAdmin 4 (La Interfaz)","text":"<ul> <li>Una web para gestionar tu base de datos SQL sin comandos.</li> <li>Acceso: <code>http://localhost:8080</code></li> </ul>"},{"location":"infraestructura/#docker-rescue","title":"\ud83d\ude91 Docker Rescue","text":"<p>\u00bfTe ha pasado que Docker \"no arranca\"? El script de setup incluye un m\u00f3dulo de rescate: 1.  Verifica si el servicio de Windows <code>com.docker.service</code> est\u00e1 corriendo. 2.  Si est\u00e1 detenido, intenta reiniciarlo con permisos de administrador. 3.  Espera a que el motor est\u00e9 listo antes de intentar levantar los contenedores.</p>"},{"location":"infraestructura/#como-iniciar","title":"\ud83d\udee0\ufe0f C\u00f3mo Iniciar","text":"<p>Para levantar el stack de Docker, abre PowerShell como Administrador y ejecuta:</p> <pre><code># Navegar a la carpeta del proyecto\ncd \"C:\\Users\\TU_USUARIO\\Documents\\ejercicios_bigdata\"\n\n# Levantar los servicios\ndocker-compose up -d\n\n# Verificar que est\u00e1n corriendo\ndocker ps\n</code></pre> <p>Servicios disponibles despu\u00e9s de iniciar: - PostgreSQL: <code>localhost:5432</code> - Spark Master UI: <code>localhost:8081</code> - pgAdmin: <code>localhost:8080</code></p>"},{"location":"archive/LEEME/","title":"Ejercicios de Big Data para Certificaci\u00f3n MCO","text":"<p>Bienvenido a esta serie de ejercicios pr\u00e1cticos dise\u00f1ados para aprender los fundamentos de Big Data utilizando Python. Este material est\u00e1 pensado para alumnos que se est\u00e1n iniciando en la programaci\u00f3n y el an\u00e1lisis de datos.</p>"},{"location":"archive/LEEME/#indice-de-documentacion","title":"\u00cdndice de Documentaci\u00f3n","text":""},{"location":"archive/LEEME/#para-alumnos","title":"Para Alumnos","text":"<ul> <li>INSTRUCCIONES_ALUMNOS.md - Gu\u00eda de Git y PyCharm</li> <li>GUIA_ENTREGA_DASHBOARDS.md - C\u00f3mo crear y entregar dashboards</li> <li>ENTENDIENDO_GIT_Y_RAMAS.md - Explicaci\u00f3n visual de Git y ramas</li> </ul>"},{"location":"archive/LEEME/#para-referencia","title":"Para Referencia","text":"<ul> <li>ARQUITECTURA_Y_STACK.md - Explicaci\u00f3n del stack tecnol\u00f3gico</li> <li>ESTRUCTURA_PROYECTO.md - Buenas pr\u00e1cticas y organizaci\u00f3n</li> </ul>"},{"location":"archive/LEEME/#para-profesor-solo-instructor","title":"Para Profesor (Solo Instructor)","text":"<ul> <li>GUIA_PROFESOR.md - Gesti\u00f3n del proyecto colaborativo</li> <li>EJEMPLO_PRIMERA_TAREA.md - Ejemplo completo de asignaci\u00f3n de tarea</li> <li>PROXIMOS_PASOS.md - Setup inicial del repositorio</li> </ul>"},{"location":"archive/LEEME/#que-vamos-a-aprender","title":"\u00bfQu\u00e9 vamos a aprender?","text":"<p>En estos ejercicios no solo ejecutaremos c\u00f3digo, sino que entenderemos qu\u00e9 estamos haciendo y por qu\u00e9. Cubriremos:</p> <ol> <li>Obtenci\u00f3n de datos: C\u00f3mo descargar informaci\u00f3n real de internet usando c\u00f3digo.</li> <li>Almacenamiento: Por qu\u00e9 guardamos los datos en bases de datos (SQLite) y formatos modernos (Parquet).</li> <li>Limpieza: C\u00f3mo preparar los datos \"sucios\" del mundo real para que sean \u00fatiles.</li> <li>Procesamiento: C\u00f3mo usar herramientas profesionales (Dask, PySpark) para manejar grandes vol\u00famenes de informaci\u00f3n.</li> </ol>"},{"location":"archive/LEEME/#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<p>El proyecto est\u00e1 organizado en carpetas para mantener el orden, algo vital en proyectos profesionales:</p> <pre><code>ejercicios_bigdata/\n\u251c\u2500\u2500 __init__.py           # Archivo que indica a Python que esta carpeta es un \"paquete\"\n\u251c\u2500\u2500 requirements.txt      # Lista de \"ingredientes\" (librer\u00edas) que necesita nuestro proyecto\n\u251c\u2500\u2500 LEEME.md              # Este archivo de instrucciones\n\u2502\n\u251c\u2500\u2500 datos/                # Carpeta donde guardaremos los datos descargados\n\u2502   \u2514\u2500\u2500 descargar_datos.py # Script (programa) para bajar los datos de internet\n\u2502\n\u251c\u2500\u2500 ejercicios/           # Carpeta con los ejercicios paso a paso\n\u2502   \u251c\u2500\u2500 01_cargar_sqlite.py   # Ejercicio 1: Bases de datos SQL\n\u2502   \u251c\u2500\u2500 02_limpieza_datos.py  # Ejercicio 2: Limpieza con Pandas\n\u2502   \u251c\u2500\u2500 03_parquet_dask.py    # Ejercicio 3: Formatos Big Data y Dask\n\u2502   \u2514\u2500\u2500 04_pyspark_query.py   # Ejercicio 4: Introducci\u00f3n a Apache Spark\n\u2502\n\u2514\u2500\u2500 dashboards/           # Dashboards de visualizaci\u00f3n (Flask)\n    \u2514\u2500\u2500 nyc_taxi_eda/     # Ejemplo de dashboard con EDA\n        \u251c\u2500\u2500 app.py\n        \u251c\u2500\u2500 templates/\n        \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"archive/LEEME/#conceptos-clave-para-principiantes","title":"Conceptos Clave para Principiantes","text":""},{"location":"archive/LEEME/#que-es-una-libreria","title":"\u00bfQu\u00e9 es una \"Librer\u00eda\"?","text":"<p>Imagina que quieres construir una casa. Podr\u00edas fabricar tus propios ladrillos, cemento y herramientas, pero tardar\u00edas a\u00f1os. En programaci\u00f3n, una librer\u00eda es como ir a una ferreter\u00eda y comprar herramientas ya hechas por expertos. - pandas: Es como una hoja de c\u00e1lculo de Excel superpotente pero sin interfaz gr\u00e1fica. - requests: Nos permite \"navegar\" por internet y descargar archivos usando c\u00f3digo. - sqlalchemy: Nos ayuda a hablar con bases de datos SQL.</p>"},{"location":"archive/LEEME/#que-es-un-entorno-virtual","title":"\u00bfQu\u00e9 es un \"Entorno Virtual\"?","text":"<p>Es como tener una caja de herramientas separada para cada proyecto. Si en un proyecto necesitas un martillo grande y en otro uno peque\u00f1o, no quieres mezclarlos. El entorno virtual asegura que las versiones de las librer\u00edas de este proyecto no interfieran con otros.</p>"},{"location":"archive/LEEME/#configuracion-del-entorno-paso-a-paso","title":"Configuraci\u00f3n del Entorno (Paso a Paso)","text":"<ol> <li> <p>Crear el entorno virtual:     Abre tu terminal en PyCharm y escribe:     <pre><code>python -m venv venv\n</code></pre>     Esto crea una carpeta <code>venv</code> con una copia aislada de Python.</p> </li> <li> <p>Activar el entorno:</p> <ul> <li>En Windows: <code>venv\\Scripts\\activate</code></li> <li>Ver\u00e1s que aparece <code>(venv)</code> al principio de la l\u00ednea de comandos.</li> </ul> </li> <li> <p>Instalar las librer\u00edas:     Le decimos a Python que instale lo que hay en <code>requirements.txt</code>:     <pre><code>pip install -r ejercicios_bigdata/requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"archive/LEEME/#como-realizar-los-ejercicios","title":"C\u00f3mo realizar los ejercicios","text":"<p>Ejecuta los scripts en orden. Lee los comentarios dentro de cada archivo, \u00a1ah\u00ed est\u00e1 la explicaci\u00f3n detallada de cada l\u00ednea!</p> <ol> <li>Descargar los datos:     <pre><code>python ejercicios_bigdata/datos/descargar_datos.py\n</code></pre></li> <li>Ejercicio 1 (Base de Datos):     <pre><code>python ejercicios_bigdata/ejercicios/01_cargar_sqlite.py\n</code></pre></li> <li>Ejercicio 2 (Limpieza):     <pre><code>python ejercicios_bigdata/ejercicios/02_limpieza_datos.py\n</code></pre></li> <li>Ejercicio 3 (Big Data con Dask):     <pre><code>python ejercicios_bigdata/ejercicios/03_parquet_dask.py\n</code></pre></li> <li>Ejercicio 4 (Apache Spark):     <pre><code>python ejercicios_bigdata/ejercicios/04_pyspark_query.py\n</code></pre></li> </ol> <p>\u00a1Mucho \u00e9xito en tu aprendizaje!</p>"},{"location":"dashboards/","title":"Galeria de Visualizaciones y Dashboards","text":"<p>Resultados de los algoritmos de Machine Learning y NLP del curso. Cada pagina incluye analisis, graficos y codigo fuente.</p>"},{"location":"dashboards/#machine-learning","title":"Machine Learning","text":""},{"location":"dashboards/#analisis-completos","title":"Analisis completos","text":"<ul> <li>PCA + Clustering K-Means: Dataset Iris -- Reduccion dimensional y agrupamiento sobre el dataset clasico de Fisher</li> <li>Manual PCA estilo FactoMineR -- Replicando el estandar de oro del analisis multivariante en Python</li> </ul>"},{"location":"dashboards/#series-temporales","title":"Series Temporales","text":"<ul> <li>ARIMA/SARIMA - Metodologia Box-Jenkins -- Identificacion, estimacion, diagnostico y pronostico</li> <li>Dashboard ARIMA (interactivo) -- 6 pestanas: serie, descomposicion, ACF/PACF, diagnostico, forecast, radar</li> <li>Dashboard ARIMA PRO -- Tema financiero tipo Bloomberg, KPIs, 7 pestanas, comparativa de modelos</li> <li>Dashboard ARIMA PRO (interactivo) -- Diseno inspirado en OECD Explorer y Portfolio Optimizer</li> </ul>"},{"location":"dashboards/#computer-vision-transfer-learning","title":"Computer Vision (Transfer Learning)","text":"<ul> <li>Clasificacion de Flores con Transfer Learning -- MobileNetV2 + ML tradicional sobre embeddings</li> <li>Dashboard Flores (interactivo) -- t-SNE, comparativa modelos, confusion matrix, radar</li> </ul>"},{"location":"dashboards/#dashboards-interactivos-html","title":"Dashboards interactivos (HTML)","text":"<ul> <li>Dashboard PCA Iris (interactivo) -- Visualizacion completa en pantalla</li> <li>Dashboard FactoMineR (interactivo) -- Circulos de correlacion y biplots</li> </ul>"},{"location":"dashboards/#nlp-y-text-mining","title":"NLP y Text Mining","text":""},{"location":"dashboards/#ejercicios-guiados","title":"Ejercicios guiados","text":"<ul> <li>Ejercicio 01: Anatomia del Texto y Conteo de Palabras -- Frecuencia, tokenizacion y visualizacion</li> <li>Ejercicio 02: Filtro de Stopwords -- Limpieza de ruido semantico</li> </ul>"},{"location":"dashboards/#analisis-completos_1","title":"Analisis completos","text":"<ul> <li>Similitud de Jaccard -- Comparacion de documentos y sistema de recomendacion</li> <li>Vectorizacion y Clustering de Documentos -- TF-IDF, K-Means y analisis de topicos</li> </ul>"},{"location":"dashboards/#analisis-de-datos-de-panel-big-data","title":"Analisis de Datos de Panel (Big Data)","text":""},{"location":"dashboards/#pipeline-completo-spark-postgresql-ml","title":"Pipeline completo: Spark + PostgreSQL + ML","text":"<ul> <li>Modulo 06: QoG - 4 Lineas de Investigacion + ML -- Asia Central, Seguridad Hidrica, Terrorismo, Maghreb, PCA + K-Means</li> <li>Dashboard QoG (interactivo) -- 5 pestanas con graficos Plotly interactivos</li> </ul>"},{"location":"dashboards/#codigo-fuente","title":"Codigo fuente","text":"<p>Los scripts que generan estas visualizaciones estan en:</p> <ul> <li><code>ejercicios/04_machine_learning/</code> -- PCA, K-Means, Silhouette</li> <li><code>ejercicios/05_nlp_text_mining/</code> -- Conteo, limpieza, sentimiento, Jaccard</li> <li><code>ejercicios/06_an\u00e1lisis_datos_de_panel/</code> -- Pipeline QoG con Apache Spark</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/","title":"Manual de PCA en Python: Estilo FactoMineR","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#replicando-el-estandar-de-oro-del-analisis-multivariante","title":"Replicando el Estandar de Oro del Analisis Multivariante","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#informacion-de-certificacion-y-referencia","title":"Informacion de Certificacion y Referencia","text":"<p>Autor original/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Metodologia: Cursos Avanzados de Big Data, Ciencia de Datos, Desarrollo de aplicaciones con IA &amp; Econometria Aplicada. Hash ID de Certificacion: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Repositorio: https://github.com/TodoEconometria/certificaciones</p> <p>REFERENCIA ACADEMICA PRINCIPAL:</p> <ul> <li>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. CRC Press.</li> <li>Tutorial FactoMineR: http://factominer.free.fr/course/FactoTuto.html</li> <li>Le, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: An R Package for Multivariate Analysis. Journal of Statistical Software, 25(1), 1-18.</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#1-introduccion-por-que-factominer-es-el-estandar-de-oro","title":"1. Introduccion: Por que FactoMineR es el Estandar de Oro","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#11-el-problema-de-los-datos-multivariantes","title":"1.1. El Problema de los Datos Multivariantes","text":"<p>Imagina que tienes datos de 41 atletas que compitieron en decatlon (10 pruebas deportivas). Cada atleta tiene 10 mediciones: tiempos en carreras, distancias en saltos y lanzamientos. La pregunta es: Como podemos \"ver\" patrones en 10 dimensiones?</p> <p>El cerebro humano solo puede visualizar 2 o 3 dimensiones. El Analisis de Componentes Principales (PCA) es la herramienta matematica que nos permite \"comprimir\" esas 10 dimensiones en 2, perdiendo la menor cantidad de informacion posible.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#12-filosofia-factominer-vs-scikit-learn","title":"1.2. Filosofia: FactoMineR vs. Scikit-Learn","text":"<p>Este manual replica la logica de FactoMineR (el estandar de oro en R) usando Python.</p> Aspecto Scikit-learn FactoMineR / Prince Enfoque Machine Learning Exploracion de Datos Objetivo Reducir dimensiones para un modelo predictivo Entender que variables mueven a que individuos Metricas clave <code>explained_variance_ratio_</code> Contribuciones, \\(\\cos^2\\), correlaciones Variables suplementarias No soportadas Soporte nativo Graficos Basicos Circulo de correlacion, biplot <p>La diferencia fundamental: Scikit-learn te dice \"estos son tus datos comprimidos\". FactoMineR te dice \"por que tus datos se comprimieron asi, que variables son responsables, y que tan confiable es la representacion de cada punto\".</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2-antes-de-empezar-tratamiento-de-datos-perdidos","title":"2. ANTES DE EMPEZAR: Tratamiento de Datos Perdidos","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#21-el-error-mas-comun-meto-la-media-y-listo","title":"2.1. El Error Mas Comun: \"Meto la Media y Listo\"","text":"<p>Uno de los errores mas frecuentes en analisis multivariante es imputar datos perdidos con la media sin pensar. Esta practica, aunque comun, puede destruir la estructura real de tus datos y llevarte a conclusiones erroneas.</p> <p>REGLA DE ORO: Antes de imputar cualquier valor perdido, debes entender POR QUE falta ese dato. La estrategia de imputacion depende del mecanismo de perdida.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#22-los-tres-tipos-de-datos-perdidos-rubin-1976","title":"2.2. Los Tres Tipos de Datos Perdidos (Rubin, 1976)","text":"Tipo Nombre Significado Ejemplo Se puede usar la media? MCAR Missing Completely At Random El dato falta por puro azar, sin relacion con ninguna variable Un sensor fallo aleatoriamente SI, pero con precaucion MAR Missing At Random El dato falta por una razon relacionada con OTRAS variables observadas Los jovenes no responden preguntas de salario (pero sabemos su edad) DEPENDE del metodo MNAR Missing Not At Random El dato falta por una razon relacionada con EL MISMO valor perdido Los ricos no declaran su patrimonio PORQUE es alto NO, sesga los resultados"},{"location":"dashboards/02_PCA_FactoMineR_style/#23-por-que-la-media-puede-destruir-tu-analisis","title":"2.3. Por que la Media Puede Destruir tu Analisis","text":"<p>Imagina este escenario en el decatlon:</p> <pre><code>Atleta      | 100m  | Salto Largo | Lanzamiento\n------------|-------|-------------|------------\nVelocista A | 10.5s | 7.8m        | ???\nVelocista B | 10.7s | 7.6m        | ???\nLanzador C  | 11.8s | 6.2m        | 18.5m\nLanzador D  | 12.0s | 6.0m        | 19.2m\n</code></pre> <p>Problema: Los velocistas (A y B) no completaron el lanzamiento (por lesion).</p> <p>Si imputas con la media del lanzamiento: - Media de lanzamiento = (18.5 + 19.2) / 2 = 18.85m - Asignas 18.85m a los velocistas A y B</p> <p>CONSECUENCIA CATASTROFICA: - En el PCA, los velocistas aparecen como buenos lanzadores - El circulo de correlacion mostrara que velocidad y lanzamiento estan correlacionados positivamente - ESTO ES FALSO - en realidad, los velocistas suelen ser peores lanzadores</p> <p>El problema: El dato NO falta al azar (MCAR). Falta porque los velocistas se lesionaron en la prueba de lanzamiento (probablemente por falta de tecnica). Es un caso de MAR o MNAR.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#24-como-diagnosticar-el-tipo-de-dato-perdido","title":"2.4. Como Diagnosticar el Tipo de Dato Perdido","text":"<p>Antes de imputar, realiza estos tests de diagnostico:</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#test-1-patron-visual-de-perdida","title":"Test 1: Patron Visual de Perdida","text":"<pre><code>import missingno as msno\nimport matplotlib.pyplot as plt\n\n# Visualizar patron de datos perdidos\nmsno.matrix(df)\nplt.show()\n\n# Visualizar correlacion entre datos perdidos\nmsno.heatmap(df)\nplt.show()\n</code></pre> <p>Interpretacion: - Si los huecos estan dispersos aleatoriamente: probablemente MCAR - Si los huecos se agrupan en filas/columnas especificas: probablemente MAR o MNAR</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#test-2-test-de-little-mcar-test","title":"Test 2: Test de Little (MCAR Test)","text":"<pre><code># Instalar: pip install pyampute\nfrom pyampute.exploration import mcar_test\n\n# Test estadistico para MCAR\nresult = mcar_test(df)\nprint(f\"p-valor: {result}\")\n# Si p &gt; 0.05: No se rechaza MCAR (los datos PUEDEN ser aleatorios)\n# Si p &lt; 0.05: Se rechaza MCAR (los datos NO son aleatorios)\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#test-3-comparar-grupos-datos-completos-vs-incompletos","title":"Test 3: Comparar Grupos (Datos Completos vs Incompletos)","text":"<pre><code># Crear indicador de dato perdido\ndf['tiene_na'] = df['lanzamiento'].isna()\n\n# Comparar medias de otras variables\nprint(df.groupby('tiene_na')['100m'].mean())\nprint(df.groupby('tiene_na')['salto_largo'].mean())\n\n# Si las medias son MUY diferentes, el dato NO es MCAR\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#25-estrategias-de-imputacion-segun-el-tipo","title":"2.5. Estrategias de Imputacion Segun el Tipo","text":"Tipo de Perdida Estrategia Recomendada Estrategia a EVITAR MCAR (aleatorio puro) Media, mediana, KNN, MICE - MAR (depende de otras variables) KNN, MICE, Regresion multiple Media simple (sesga) MNAR (depende del valor perdido) Modelos especificos, Analisis de sensibilidad Media, KNN, MICE (todos sesgan)"},{"location":"dashboards/02_PCA_FactoMineR_style/#26-alternativas-a-la-media-codigo-python","title":"2.6. Alternativas a la Media (Codigo Python)","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-1-eliminar-filas-con-na-solo-si-son-pocas-y-mcar","title":"Opcion 1: Eliminar Filas con NA (solo si son pocas y MCAR)","text":"<pre><code># Solo si tienes POCOS datos perdidos (&lt;5%) y son MCAR\ndf_limpio = df.dropna()\nprint(f\"Filas eliminadas: {len(df) - len(df_limpio)}\")\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-2-imputacion-por-knn-respeta-la-estructura-local","title":"Opcion 2: Imputacion por KNN (respeta la estructura local)","text":"<pre><code>from sklearn.impute import KNNImputer\n\n# KNN busca los K vecinos mas similares y promedia sus valores\nimputer = KNNImputer(n_neighbors=5)\ndf_imputado = pd.DataFrame(\n    imputer.fit_transform(df[columnas_numericas]),\n    columns=columnas_numericas\n)\n</code></pre> <p>Ventaja: Un velocista con NA en lanzamiento recibira el valor de otros velocistas similares, no el promedio global.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-3-imputacion-multiple-mice-el-estandar-de-oro","title":"Opcion 3: Imputacion Multiple (MICE) - El Estandar de Oro","text":"<pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# MICE: Multiple Imputation by Chained Equations\nimputer = IterativeImputer(max_iter=10, random_state=42)\ndf_imputado = pd.DataFrame(\n    imputer.fit_transform(df[columnas_numericas]),\n    columns=columnas_numericas\n)\n</code></pre> <p>Ventaja: Usa TODAS las variables para predecir el valor perdido mediante regresiones iterativas.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-4-imputacion-por-grupo-cuando-hay-categorias","title":"Opcion 4: Imputacion por Grupo (cuando hay categorias)","text":"<pre><code># Si sabes que el dato depende de una categoria\ndf['lanzamiento'] = df.groupby('tipo_atleta')['lanzamiento'].transform(\n    lambda x: x.fillna(x.mean())\n)\n</code></pre> <p>Ejemplo: Imputar el lanzamiento de un velocista con la media de otros velocistas, no con la media global.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#27-checklist-antes-de-hacer-pca","title":"2.7. Checklist ANTES de Hacer PCA","text":"<p>Antes de ejecutar tu analisis PCA, verifica:</p> <ul> <li> Cuantos NA tengo? <code>df.isna().sum()</code> - Si &gt; 20% en una variable, considera eliminarla</li> <li> Donde estan los NA? <code>msno.matrix(df)</code> - Busca patrones</li> <li> Son aleatorios? Test de Little o comparacion de grupos</li> <li> Que metodo uso?</li> <li>MCAR + pocos NA: Media o eliminar filas</li> <li>MAR: KNN o MICE</li> <li>MNAR: Consulta con experto del dominio o analisis de sensibilidad</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#28-ejemplo-practico-que-hacer-en-el-decatlon","title":"2.8. Ejemplo Practico: Que Hacer en el Decatlon","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\n\n# Cargar datos\ndf = pd.read_csv('decathlon.csv')\n\n# 1. Diagnostico\nprint(\"Datos perdidos por columna:\")\nprint(df.isna().sum())\n\n# 2. Si hay NA, verificar patron\nif df.isna().any().any():\n    # Comparar atletas con/sin NA\n    df['tiene_na'] = df.isna().any(axis=1)\n    print(\"\\nComparacion de medias (con NA vs sin NA):\")\n    print(df.groupby('tiene_na')[['100m', 'Long.jump']].mean())\n\n    # Si las medias son similares, probablemente MCAR -&gt; podemos imputar\n    # Si las medias son muy diferentes, cuidado -&gt; usar KNN o MICE\n\n    # 3. Imputar con KNN (respeta estructura)\n    imputer = KNNImputer(n_neighbors=3)\n    columnas_activas = ['100m', 'Long.jump', 'Shot.put', 'High.jump',\n                        '400m', '110m.hurdle', 'Discus', 'Pole.vault',\n                        'Javeline', '1500m']\n\n    df[columnas_activas] = imputer.fit_transform(df[columnas_activas])\n    print(\"\\nImputacion completada con KNN\")\n\n# 4. Ahora SI podemos hacer PCA\nfrom prince import PCA\npca = PCA(n_components=5, rescale_with_std=True)\npca.fit(df[columnas_activas])\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#29-resumen-la-regla-de-los-3-pasos","title":"2.9. Resumen: La Regla de los 3 Pasos","text":"<pre><code>PASO 1: DIAGNOSTICAR\n   \"Por que falta este dato?\"\n   -&gt; MCAR, MAR, o MNAR?\n\nPASO 2: DECIDIR\n   \"Puedo imputar sin sesgar?\"\n   -&gt; Si MNAR: NO imputes con media\n   -&gt; Si MAR: Usa KNN o MICE\n   -&gt; Si MCAR: Media es aceptable\n\nPASO 3: DOCUMENTAR\n   \"Que hice y por que?\"\n   -&gt; Reporta el % de NA\n   -&gt; Reporta el metodo usado\n   -&gt; Reporta si los resultados cambian\n</code></pre> <p>MORALEJA: Meter la media \"porque si\" es como llenar un hueco en un puzzle con una pieza de otro puzzle. Puede que encaje, pero la imagen final estara distorsionada.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#210-fundamental-por-que-el-0-es-valido-y-no-siempre-hay-que-imputar","title":"2.10. FUNDAMENTAL: Por Que el 0 es Valido y NO Siempre Hay que Imputar","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#2101-el-problema-de-fondo-confundir-base-completa-con-base-correcta","title":"2.10.1. El Problema de Fondo: Confundir \"Base Completa\" con \"Base Correcta\"","text":"<p>Uno de los errores conceptuales mas graves que cometen los estudiantes (y muchos profesionales) es pensar que:</p> <p>FALSO: \"Una base de datos debe estar completa para ser analizada\"</p> <p>Esta creencia viene de la programacion (donde <code>NULL</code> causa errores) y del machine learning (donde muchos algoritmos no aceptan <code>NaN</code>). Pero en analisis de datos reales, especialmente en encuestas y Big Data, los datos faltantes son INFORMACION, no un problema a \"arreglar\".</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#la-verdad-incomoda","title":"La Verdad Incomoda:","text":"Creencia Erronea Realidad \"Debo llenar todos los NA\" NO. Debes entender POR QUE faltan \"Una base completa es mejor\" NO. Una base con imputaciones incorrectas es PEOR que una base con NA \"El 0 significa que no hay dato\" DEPENDE. En encuestas, 0 puede ser una respuesta valida \"NaN y 0 son lo mismo\" FALSO. Son conceptualmente diferentes"},{"location":"dashboards/02_PCA_FactoMineR_style/#2102-la-diferencia-fundamental-0-vs-nan-vs-no-respuesta","title":"2.10.2. La Diferencia Fundamental: 0 vs NaN vs \"No Respuesta\"","text":"<p>Imagina una encuesta sobre ingresos y gastos:</p> Pregunta Respuesta Persona A Respuesta Persona B Respuesta Persona C \"Cuantos cigarrillos fuma al dia?\" 0 (no fuma) NaN (se nego a responder) 20 (fuma 20) \"Cuanto gasto en tabaco este mes?\" 0 (coherente: no fuma) NaN (no respondio) 150 (coherente: fuma)"},{"location":"dashboards/02_PCA_FactoMineR_style/#analisis-de-cada-caso","title":"Analisis de Cada Caso:","text":"<p>Persona A - El 0 es VALIDO: - <code>cigarrillos = 0</code> significa \"NO FUMA\", es una respuesta real - <code>gasto_tabaco = 0</code> es coherente con no fumar - IMPUTAR estos 0 con la media seria CATASTROFICO: convertirias a un no fumador en fumador promedio</p> <p>Persona B - El NaN es INFORMATIVO: - <code>cigarrillos = NaN</code> significa \"NO QUISO RESPONDER\" - Puede ser porque:   - Fuma ilegalmente (menor de edad)   - Le da verguenza admitir que fuma mucho   - Simplemente no quiere compartir esa informacion - IMPUTAR con la media asume que es un fumador promedio, lo cual puede ser FALSO</p> <p>Persona C - Dato Completo: - Respondio todo, no hay ambiguedad</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2103-conceptos-de-muestreo-por-que-los-datos-faltantes-son-parte-del-diseno","title":"2.10.3. Conceptos de Muestreo: Por Que los Datos Faltantes son Parte del Diseno","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#el-contexto-de-las-encuestas","title":"El Contexto de las Encuestas","text":"<p>Cuando trabajamos con encuestas (surveys) o bases de datos de Big Data, los datos faltantes son una consecuencia natural del proceso de recoleccion. Segun la teoria de muestreo (ver TodoEconometria - Estrategias de Muestreo):</p> <p>Muestreo Aleatorio Simple: La seleccion de la muestra representativa es altamente complicada. Por ejemplo, si queremos estudiar personas con diabetes tipo 2, no todos responderan todas las preguntas, y eso es esperado y valido.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#los-tres-escenarios-reales","title":"Los Tres Escenarios Reales:","text":"<p>Escenario 1: Preguntas de Control (Filtros)</p> <p>En encuestas bien disenadas, hay preguntas que intencionalmente generan <code>NaN</code> en otras variables:</p> <pre><code>P1: \"Tiene automovil?\"\n    [ ] Si  -&gt; Pasar a P2\n    [ ] No  -&gt; Saltar a P5\n\nP2: \"Que marca es su automovil?\" (Solo si respondio Si en P1)\n    _____________\n</code></pre> <p>Resultado en la base de datos:</p> Persona Tiene_Auto Marca_Auto Juan Si Toyota Maria No NaN <p>Pregunta: Debemos imputar la marca de auto de Maria?</p> <p>Respuesta: NO. El <code>NaN</code> es correcto. Maria no tiene auto, por lo tanto no puede tener marca. Imputar \"Toyota\" (la media/moda) seria absurdo.</p> <p>Escenario 2: No Respuesta por Sensibilidad</p> <p>Algunas preguntas son sensibles:</p> <pre><code>P10: \"Cual es su ingreso mensual?\"\n     [ ] Menos de $1000\n     [ ] $1000 - $3000\n     [ ] $3000 - $5000\n     [ ] Mas de $5000\n     [ ] Prefiero no responder\n</code></pre> <p>Resultado:</p> Persona Ingreso Pedro \\(1000-\\)3000 Ana NaN (prefiere no responder) <p>Pregunta: Debemos imputar el ingreso de Ana con la mediana?</p> <p>Respuesta: DEPENDE del tipo de perdida: - Si Ana es rica y no quiere declararlo: MNAR (el dato falta porque es alto) - Si Ana respondio al azar \"no responder\": MCAR (aleatorio)</p> <p>Imputar con la mediana en caso MNAR introducira sesgo hacia abajo (asumimos que los ricos que no responden tienen ingresos promedio).</p> <p>Escenario 3: Datos de Big Data con Sensores</p> <p>En bases de datos de IoT, sensores, o logs:</p> <pre><code>Sensor de Temperatura:\nHora  | Temperatura\n------|------------\n10:00 | 25.3\u00b0C\n10:05 | 25.5\u00b0C\n10:10 | NaN  (sensor fallo)\n10:15 | 26.1\u00b0C\n</code></pre> <p>Pregunta: Debemos imputar 10:10 con la media de 10:05 y 10:15?</p> <p>Respuesta: DEPENDE: - Si el sensor falla aleatoriamente: MCAR -&gt; Imputacion lineal es razonable - Si el sensor falla cuando hace mucho calor (se sobrecalienta): MNAR -&gt; Imputar con la media subestimara la temperatura real</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2104-el-error-conceptual-necesito-datos-completos-para-analizar","title":"2.10.4. El Error Conceptual: \"Necesito Datos Completos para Analizar\"","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#mito-1-los-algoritmos-no-aceptan-nan","title":"Mito 1: \"Los algoritmos no aceptan NaN\"","text":"<p>Realidad: - Muchos algoritmos SI aceptan NaN: arboles de decision, Random Forest, XGBoost - Los que no aceptan NaN (regresion lineal, PCA clasico) tienen versiones robustas:   - PCA con <code>prince</code> acepta NA   - Regresion con <code>statsmodels</code> tiene <code>missing='drop'</code></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#mito-2-mas-datos-mejor-modelo","title":"Mito 2: \"Mas datos = Mejor modelo\"","text":"<p>Realidad: - Datos incorrectos (imputados mal) son peores que menos datos correctos - Ejemplo:</p> <pre><code># Opcion A: Imputar con media (MALO si es MNAR)\ndf['ingreso'].fillna(df['ingreso'].mean())  # 1000 filas, pero con sesgo\n\n# Opcion B: Eliminar filas con NA (BUENO si son pocas)\ndf.dropna(subset=['ingreso'])  # 800 filas, pero sin sesgo\n</code></pre> <p>Cual es mejor? Depende del % de NA: - Si NA &lt; 5%: Eliminar es seguro - Si NA &gt; 20%: Investigar por que faltan antes de decidir</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2105-caso-practico-encuesta-de-habitos-de-consumo","title":"2.10.5. Caso Practico: Encuesta de Habitos de Consumo","text":"<p>Imagina esta base de datos:</p> Persona Fuma Cigarrillos_Dia Gasto_Tabaco Bebe_Alcohol Copas_Semana A No 0 0 Si 3 B Si 10 50 No 0 C Si NaN NaN Si NaN D No 0 0 No 0"},{"location":"dashboards/02_PCA_FactoMineR_style/#analisis-correcto","title":"Analisis Correcto:","text":"<p>Persona A: - <code>Cigarrillos_Dia = 0</code> es VALIDO (no fuma) - <code>Gasto_Tabaco = 0</code> es VALIDO (coherente) - NO IMPUTAR</p> <p>Persona B: - <code>Copas_Semana = 0</code> es VALIDO (no bebe) - NO IMPUTAR</p> <p>Persona C: - <code>Cigarrillos_Dia = NaN</code> es PROBLEMATICO - Opciones:   1. Eliminar fila (si es la unica con NA)   2. Imputar con KNN (buscar fumadores similares)   3. Dejar como NA y usar algoritmos que lo soporten</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#analisis-incorrecto-comun-en-principiantes","title":"Analisis INCORRECTO (Comun en Principiantes):","text":"<pre><code># ERROR: Imputar TODO con la media\ndf.fillna(df.mean())\n</code></pre> <p>Consecuencia: - Persona A (no fumador) ahora tiene <code>Cigarrillos_Dia = 5</code> (media de B y C) - Persona D (no fumador, no bebedor) ahora fuma y bebe - La base de datos esta DESTRUIDA</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2106-reglas-de-oro-para-estudiantes","title":"2.10.6. Reglas de Oro para Estudiantes","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#regla-1-antes-de-tocar-un-na-preguntate","title":"Regla 1: Antes de Tocar un NA, Preguntate:","text":"<pre><code>1. Por que falta este dato?\n   - Pregunta de filtro? -&gt; NA es CORRECTO, NO imputar\n   - No respuesta? -&gt; Investigar tipo (MCAR/MAR/MNAR)\n   - Error tecnico? -&gt; Puede ser MCAR\n\n2. Es un 0 o un NaN?\n   - 0 = Respuesta valida (\"no tengo\", \"no hago\")\n   - NaN = Ausencia de respuesta\n\n3. Que pasa si imputo mal?\n   - Sesgo en resultados\n   - Conclusiones erroneas\n   - Modelo invalido\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#regla-2-workflow-de-decision","title":"Regla 2: Workflow de Decision","text":"<pre><code>PASO 1: Contar NA\n   df.isna().sum()\n   -&gt; Si &lt; 5%: Considera eliminar filas\n   -&gt; Si &gt; 20%: Investiga patron\n\nPASO 2: Visualizar patron\n   import missingno as msno\n   msno.matrix(df)\n   -&gt; Aleatorio? -&gt; MCAR\n   -&gt; Agrupado? -&gt; MAR/MNAR\n\nPASO 3: Test estadistico\n   from pyampute.exploration import mcar_test\n   mcar_test(df)\n   -&gt; p &gt; 0.05: MCAR (puedes imputar)\n   -&gt; p &lt; 0.05: MAR/MNAR (cuidado)\n\nPASO 4: Decidir estrategia\n   - MCAR + pocos NA: Media o eliminar\n   - MAR: KNN o MICE\n   - MNAR: NO imputar o modelo especifico\n   - Pregunta de filtro: DEJAR como NA\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#regla-3-documentar-siempre","title":"Regla 3: Documentar SIEMPRE","text":"<pre><code># BUENA PRACTICA\nprint(\"=\"*50)\nprint(\"REPORTE DE DATOS FALTANTES\")\nprint(\"=\"*50)\nprint(f\"Total de observaciones: {len(df)}\")\nprint(f\"Variables con NA: {df.isna().any().sum()}\")\nprint(f\"\\nDetalle por variable:\")\nprint(df.isna().sum())\nprint(f\"\\nPorcentaje de NA:\")\nprint((df.isna().sum() / len(df) * 100).round(2))\nprint(\"\\nEstrategia aplicada:\")\nprint(\"- Variables con 0 valido: ['Cigarrillos_Dia', 'Copas_Semana']\")\nprint(\"- Variables imputadas con KNN: ['Ingreso']\")\nprint(\"- Filas eliminadas: 3 (0.5% del total)\")\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2107-ejemplo-completo-codigo-correcto-vs-incorrecto","title":"2.10.7. Ejemplo Completo: Codigo Correcto vs Incorrecto","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#codigo-incorrecto-principiante","title":"Codigo INCORRECTO (Principiante):","text":"<pre><code>import pandas as pd\n\n# Cargar datos\ndf = pd.read_csv('encuesta.csv')\n\n# ERROR: Imputar TODO con la media\ndf = df.fillna(df.mean())\n\n# ERROR: No verificar que paso\nprint(\"Listo, base completa!\")\n</code></pre> <p>Problemas: 1. No distingue entre 0 valido y NaN 2. No verifica tipo de perdida 3. No documenta que hizo 4. Puede haber destruido la estructura real</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#codigo-correcto-profesional","title":"Codigo CORRECTO (Profesional):","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# 1. Cargar y explorar\ndf = pd.read_csv('encuesta.csv')\nprint(\"DIAGNOSTICO INICIAL\")\nprint(\"=\"*50)\nprint(df.info())\nprint(\"\\nDatos faltantes:\")\nprint(df.isna().sum())\n\n# 2. Visualizar patron\nmsno.matrix(df)\nplt.title(\"Patron de Datos Faltantes\")\nplt.show()\n\n# 3. Separar variables con 0 valido de variables con NA\n# Variables donde 0 es valido (no imputar)\nvars_con_cero_valido = ['Cigarrillos_Dia', 'Gasto_Tabaco', 'Copas_Semana']\n\n# Variables donde NA debe tratarse\nvars_a_imputar = ['Ingreso', 'Edad', 'Horas_Trabajo']\n\n# 4. Verificar coherencia (0 en variables dependientes)\n# Ejemplo: Si no fuma, gasto en tabaco debe ser 0\nmask_no_fuma = df['Fuma'] == 'No'\nassert (df.loc[mask_no_fuma, 'Cigarrillos_Dia'] == 0).all(), \\\n    \"ERROR: Hay no fumadores con cigarrillos &gt; 0\"\n\n# 5. Imputar solo variables numericas que lo necesiten\nif df[vars_a_imputar].isna().any().any():\n    print(\"\\nAplicando KNN Imputer a:\", vars_a_imputar)\n    imputer = KNNImputer(n_neighbors=5)\n    df[vars_a_imputar] = imputer.fit_transform(df[vars_a_imputar])\n    print(\"Imputacion completada\")\n\n# 6. Documentar\nprint(\"\\n\" + \"=\"*50)\nprint(\"REPORTE FINAL\")\nprint(\"=\"*50)\nprint(f\"Filas totales: {len(df)}\")\nprint(f\"Variables con 0 valido (NO imputadas): {vars_con_cero_valido}\")\nprint(f\"Variables imputadas con KNN: {vars_a_imputar}\")\nprint(f\"NA restantes: {df.isna().sum().sum()}\")\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2108-resumen-la-filosofia-del-analista-de-datos","title":"2.10.8. Resumen: La Filosofia del Analista de Datos","text":"<p>\"No es tu trabajo llenar huecos. Es tu trabajo ENTENDER por que hay huecos.\"</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#principios-fundamentales","title":"Principios Fundamentales:","text":"<ol> <li>Los datos faltantes son informacion, no un error</li> <li>El 0 es una respuesta valida, no un dato faltante</li> <li>NaN significa \"no se\", y a veces \"no se\" es la respuesta correcta</li> <li>Imputar mal es peor que no imputar</li> <li>Documenta TODO lo que hagas con los NA</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#checklist-final-antes-de-hacer-pca-o-cualquier-analisis","title":"Checklist Final Antes de Hacer PCA (o Cualquier Analisis):","text":"<ul> <li> He identificado todas las variables con 0 valido?</li> <li> He verificado que los 0 sean coherentes con otras variables?</li> <li> He diagnosticado el tipo de perdida (MCAR/MAR/MNAR)?</li> <li> He decidido una estrategia justificada para cada variable?</li> <li> He documentado cuantos NA habia y que hice con ellos?</li> <li> He verificado que los resultados tengan sentido despues de imputar?</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#3-el-dataset-decathlon-contexto-y-preguntas-de-investigacion","title":"3. El Dataset: Decathlon - Contexto y Preguntas de Investigacion","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#31-el-escenario-real-que-problema-queremos-resolver","title":"3.1. El Escenario Real: Que Problema Queremos Resolver?","text":"<p>Imagina que eres el director tecnico de una federacion de atletismo. Tienes datos de 41 atletas que compitieron en decatlon (10 pruebas) en dos competiciones diferentes: los Juegos Olimpicos y el Meeting de Decastar.</p> <p>Te enfrentas a varias preguntas de negocio/investigacion:</p> Pregunta Por que importa Existen \"perfiles\" de atletas? (velocistas vs lanzadores vs completos) Para disenar programas de entrenamiento especializados Que pruebas estan relacionadas entre si? Para optimizar el entrenamiento cruzado Los atletas olimpicos tienen un perfil diferente a los de Decastar? Para entender que caracteriza a los de elite Que pruebas son \"redundantes\" (miden lo mismo)? Para simplificar las evaluaciones Que atletas tienen perfiles atipicos? Para identificar talentos unicos o detectar anomalias <p>CLAVE: No aplicamos PCA \"porque si\" o \"porque esta de moda\". Lo aplicamos porque tenemos preguntas especificas que esta tecnica puede responder de forma eficiente y valida.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#32-por-que-pca-es-la-herramienta-adecuada-y-no-otra","title":"3.2. Por que PCA es la Herramienta Adecuada (y no otra)","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#el-problema-matematico","title":"El Problema Matematico","text":"<p>Tenemos 10 variables numericas (las pruebas). Visualizar 10 dimensiones es imposible. Pero:</p> <ul> <li>No queremos perder informacion importante</li> <li>Queremos ver patrones (grupos de atletas, relaciones entre pruebas)</li> <li>Queremos reducir la complejidad sin destruir la estructura</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#por-que-no-otras-tecnicas","title":"Por que NO otras tecnicas?","text":"Tecnica Por que NO es adecuada aqui Regresion No tenemos una variable \"objetivo\" que predecir. Queremos explorar, no predecir. Clustering (K-Means) Agrupa individuos, pero no explica QUE variables los diferencian ni como se relacionan las variables entre si. Correlacion simple Solo compara variables de 2 en 2. Con 10 variables, tendriamos 45 correlaciones. Imposible de interpretar. t-SNE / UMAP Buenos para visualizar, pero no dan interpretacion de las dimensiones (son \"cajas negras\")."},{"location":"dashboards/02_PCA_FactoMineR_style/#por-que-si-usar-pca","title":"Por que SI usar PCA?","text":"Ventaja de PCA Aplicacion en Decathlon Reduce dimensiones preservando varianza De 10 pruebas a 2-3 dimensiones interpretables Identifica que variables \"van juntas\" Descubrir que 100m, salto largo y 400m estan correlacionadas Detecta individuos atipicos Identificar atletas con perfiles unicos Permite variables suplementarias Ver si \"Competicion\" explica diferencias sin contaminar el analisis Es interpretable Cada eje tiene un significado (ej: \"velocidad vs resistencia\")"},{"location":"dashboards/02_PCA_FactoMineR_style/#33-el-contexto-deportivo-del-decathlon","title":"3.3. El Contexto Deportivo del Decathlon","text":"<p>El decatlon es conocido como \"la prueba reina del atletismo\" porque evalua al atleta mas completo. Consta de 10 pruebas en 2 dias:</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#dia-1","title":"Dia 1","text":"Prueba Tipo Habilidad Principal 100m Carrera Velocidad explosiva Salto largo Salto Potencia de piernas Lanzamiento de peso Lanzamiento Fuerza bruta Salto alto Salto Tecnica + potencia 400m Carrera Velocidad-resistencia"},{"location":"dashboards/02_PCA_FactoMineR_style/#dia-2","title":"Dia 2","text":"Prueba Tipo Habilidad Principal 110m vallas Carrera Velocidad + tecnica Lanzamiento de disco Lanzamiento Fuerza + tecnica Salto con pertiga Salto Tecnica avanzada Lanzamiento de jabalina Lanzamiento Fuerza + coordinacion 1500m Carrera Resistencia pura <p>Hipotesis previa (antes del PCA): Esperamos encontrar que las pruebas se agrupen por \"tipo de habilidad\": velocidad (100m, 400m, vallas), fuerza (peso, disco, jabalina), y tecnica (pertiga, salto alto). El PCA nos dira si esta hipotesis es correcta.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#34-las-dos-competiciones-por-que-es-importante","title":"3.4. Las Dos Competiciones: Por que es Importante?","text":"Competicion Nivel Caracteristicas Juegos Olimpicos Elite mundial Solo los mejores del mundo. Alta presion. Maxima preparacion. Meeting de Decastar Alto nivel europeo Competidores de elite, pero no necesariamente los mejores del mundo. <p>Pregunta clave: Los atletas olimpicos tienen un perfil de rendimiento diferente? O simplemente son \"mejores en todo\"?</p> <ul> <li>Si son \"mejores en todo\", estaran en la misma direccion que los de Decastar pero mas lejos del origen.</li> <li>Si tienen un perfil diferente, estaran en una zona distinta del mapa.</li> </ul> <p>Esta pregunta se responde usando <code>Competition</code> como variable suplementaria: no participa en el calculo del PCA, pero la proyectamos para ver si los grupos se separan.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#35-estructura-de-los-datos","title":"3.5. Estructura de los Datos","text":"Tipo de Variable Variables Rol en el Analisis Activas 100m, Long.jump, Shot.put, High.jump, 400m, 110m.hurdle, Discus, Pole.vault, Javeline, 1500m Construyen los ejes PCA Suplementarias Cuantitativas Rank, Points Se proyectan pero NO construyen ejes Suplementarias Cualitativas Competition (Decastar/OlympicG) Para colorear y segmentar"},{"location":"dashboards/02_PCA_FactoMineR_style/#36-concepto-clave-variables-activas-vs-suplementarias","title":"3.6. Concepto Clave: Variables Activas vs. Suplementarias","text":"<p>Este es uno de los conceptos mas poderosos de FactoMineR:</p> <p>Variables Activas: Participan en el calculo matematico de los componentes principales. Son las que \"construyen\" los ejes. En nuestro caso, las 10 pruebas deportivas.</p> <p>Variables Suplementarias: NO participan en el calculo, pero se proyectan sobre el espacio factorial para ver como se relacionan con las dimensiones encontradas.</p> <p>Por que es util? Imagina que quieres saber si el tipo de competicion (Juegos Olimpicos vs Decastar) influye en el perfil de rendimiento de los atletas. Si incluyes \"Competition\" como variable activa, contaminas el analisis (introduces una variable categorica en un calculo numerico). Pero si la usas como suplementaria, puedes ver si los grupos se separan sin haber forzado esa separacion.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#37-otros-datasets-clasicos-de-pca-y-sus-contextos","title":"3.7. Otros Datasets Clasicos de PCA y sus Contextos","text":"<p>Para que entiendas que PCA se aplica en muchos dominios, aqui tienes otros datasets clasicos:</p> Dataset Contexto Pregunta de Investigacion Variables Iris (Fisher, 1936) Botanica Pueden las medidas de petalos/sepalos distinguir especies de flores? 4 medidas de flores, 3 especies Wine (UCI) Enologia Que propiedades quimicas caracterizan vinos de diferentes regiones? 13 propiedades quimicas, 3 origenes Breast Cancer (UCI) Medicina Que medidas de celulas distinguen tumores benignos de malignos? 30 medidas de nucleos celulares MNIST (LeCun) Vision artificial Pueden reducirse imagenes de 784 pixeles a pocas dimensiones? 784 pixeles, 10 digitos Encuestas sociales Sociologia Que dimensiones latentes explican las opiniones politicas? Muchas preguntas de encuesta <p>Patron comun: En todos los casos, tenemos muchas variables numericas y queremos descubrir estructura latente (dimensiones ocultas que explican la variacion).</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#38-checklist-antes-de-aplicar-pca-preguntate","title":"3.8. Checklist: Antes de Aplicar PCA, Preguntate...","text":"<ul> <li> Tengo una pregunta clara? \"Quiero explorar patrones\" es valido. \"Quiero predecir Y\" no es para PCA.</li> <li> Mis variables son numericas? PCA requiere variables cuantitativas (o convertibles a numericas).</li> <li> Tengo suficientes observaciones? Regla practica: al menos 5-10 observaciones por variable.</li> <li> Las variables estan en escalas comparables? Si no, debo estandarizar (lo hace Prince por defecto).</li> <li> He tratado los datos perdidos? (Ver seccion 2)</li> <li> Tengo variables suplementarias para validar? Muy util para interpretar los resultados.</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#4-resultados-del-analisis-panel-de-graficos","title":"4. Resultados del Analisis: Panel de Graficos","text":"<p>A continuacion se presenta el panel completo de graficos generados por el script <code>02_PCA_FactoMineR_style.py</code>:</p> <p></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#5-interpretacion-detallada-de-cada-grafico","title":"5. Interpretacion Detallada de Cada Grafico","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#41-scree-plot-grafico-de-sedimentacion","title":"4.1. SCREE PLOT (Grafico de Sedimentacion)","text":"<p>Ubicacion: Panel superior izquierdo</p> <p>Que muestra: La varianza explicada por cada componente principal.</p> <p></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#lectura-de-los-resultados","title":"Lectura de los Resultados:","text":"Dimension Varianza Explicada Varianza Acumulada Dim.1 16.7% 16.7% Dim.2 14.5% 31.2% Dim.3 12.8% 44.0% Dim.4 11.9% 55.9% Dim.5 9.6% 65.5%"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion","title":"Interpretacion:","text":"<ol> <li> <p>La linea punteada gris (Umbral = 10%) representa la contribucion esperada si todas las variables fueran independientes (100% / 10 variables = 10%). Las dimensiones por encima de esta linea aportan mas informacion que el promedio.</p> </li> <li> <p>Las primeras 4 dimensiones tienen autovalores mayores a 1 (Regla de Kaiser), lo que sugiere retener 4 componentes.</p> </li> <li> <p>La curva roja (acumulada) muestra que con 5 dimensiones capturamos el 65.5% de la variabilidad total. Esto es relativamente bajo, lo que indica que el decatlon es un deporte multidimensional donde no hay un unico \"factor\" que explique todo el rendimiento.</p> </li> </ol> <p>Insight Deportivo: A diferencia de deportes como natacion (donde la velocidad lo explica casi todo), el decatlon requiere multiples habilidades independientes: velocidad, fuerza, resistencia, tecnica. Por eso ninguna dimension domina claramente.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#42-circulo-de-correlacion-el-grafico-mas-importante","title":"4.2. CIRCULO DE CORRELACION (El Grafico Mas Importante)","text":"<p>Ubicacion: Panel superior centro</p> <p>Este es el grafico mas importante de FactoMineR. Muestra como las variables se relacionan entre si y con los ejes.</p> <p></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#como-leer-el-circulo","title":"Como Leer el Circulo:","text":"Situacion en el Grafico Significado Flecha cerca del circulo (borde) Variable bien representada en el plano Flecha corta (cerca del centro) Variable mal representada, su varianza esta en otras dimensiones Flechas en la misma direccion Variables positivamente correlacionadas Flechas opuestas (180 grados) Variables negativamente correlacionadas Flechas perpendiculares (90 grados) Variables no correlacionadas"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion-de-nuestros-resultados","title":"Interpretacion de Nuestros Resultados:","text":"<p>Grupo 1 - Cuadrante Inferior Izquierdo (Velocidad/Agilidad): - <code>Long.jump</code> (salto largo): Flecha larga hacia la izquierda - <code>400m</code>: Similar direccion - <code>110m.hurdle</code>: Similar direccion</p> <p>Interpretacion: Estas pruebas estan correlacionadas positivamente entre si. Un atleta bueno en salto largo tiende a ser bueno en 400m y vallas. Esto tiene sentido: todas requieren velocidad explosiva y potencia de piernas.</p> <p>Grupo 2 - Cuadrante Superior Izquierdo (Resistencia): - <code>1500m</code>: Flecha hacia arriba-izquierda - <code>Shot.put</code> (lanzamiento de peso): Hacia arriba</p> <p>Interpretacion: El 1500m (resistencia) y el lanzamiento de peso aparecen en direcciones diferentes al grupo de velocidad. Esto sugiere que la resistencia es una habilidad distinta de la velocidad explosiva.</p> <p>Grupo 3 - Cuadrante Inferior Derecho: - <code>Pole.vault</code> (salto con pertiga): Flecha larga hacia abajo - <code>Discus</code>: Similar direccion</p> <p>Interpretacion: Estas pruebas tecnicas forman su propio grupo.</p> <p>Variable Especial - <code>Long.jump</code>: - Es la flecha mas larga hacia la izquierda - Correlacion con Dim.1: -0.74 (muy alta)</p> <p>Conclusion: El salto largo es la variable que mejor define la primera dimension. Un atleta con alta puntuacion en Dim.1 (derecha del mapa) tiende a tener peor salto largo.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#43-mapa-de-individuos","title":"4.3. MAPA DE INDIVIDUOS","text":"<p>Ubicacion: Panel superior derecho</p> <p>Muestra donde se ubica cada atleta en el espacio de las dos primeras dimensiones, coloreado por la variable suplementaria <code>Competition</code>.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#lectura-del-grafico","title":"Lectura del Grafico:","text":"<ul> <li>Puntos azules: Atletas de la competencia Decastar</li> <li>Puntos rojos: Atletas de los Juegos Olimpicos (OlympicG)</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_1","title":"Interpretacion:","text":"<ol> <li> <p>Dispersion general: Los atletas se distribuyen ampliamente en el plano, sin formar grupos muy compactos. Esto confirma que hay diversidad de perfiles en el decatlon.</p> </li> <li> <p>Separacion por competicion: Observa que los puntos rojos (Olimpicos) tienden a estar mas dispersos y algunos estan en posiciones extremas (ej. BERNARD en el extremo derecho). Los atletas Olimpicos muestran mayor variabilidad en sus perfiles.</p> </li> <li> <p>Atletas destacados:</p> </li> <li>BERNARD (extremo derecho): Perfil muy particular, diferente al promedio</li> <li>Zsivoczky (abajo): Otro perfil atipico</li> <li> <p>BOURGUIGNON (arriba izquierda): Perfil diferente</p> </li> <li> <p>Interpretacion combinada con el circulo:</p> </li> <li>Los atletas a la izquierda del mapa tienden a ser buenos en <code>Long.jump</code>, <code>400m</code>, <code>110m.hurdle</code> (pruebas de velocidad)</li> <li>Los atletas arriba tienden a ser buenos en <code>Shot.put</code>, <code>1500m</code> (fuerza/resistencia)</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#44-contribuciones-de-variables-a-dim1","title":"4.4. CONTRIBUCIONES DE VARIABLES A DIM.1","text":"<p>Ubicacion: Panel inferior izquierdo</p> <p>Este grafico responde a la pregunta: Que variables \"construyeron\" el primer eje?</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#resultados","title":"Resultados:","text":"Variable Contribucion a Dim.1 Long.jump 32.9% 1500m 19.9% 400m 12.0% 110m.hurdle 10.8% High.jump 7.6% Javeline 6.6% Shot.put 5.0% 100m 4.0% Pole.vault 1.1% Discus 0.1%"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_2","title":"Interpretacion:","text":"<ul> <li>La linea roja punteada marca el umbral de contribucion esperada (10% = 100%/10 variables)</li> <li>Las barras rojas (sobre el umbral) son las variables que realmente definen esa dimension</li> <li>Las barras azules (bajo el umbral) contribuyen menos de lo esperado</li> </ul> <p>Conclusion para Dim.1: Esta dimension esta dominada por <code>Long.jump</code> (32.9%), seguida de <code>1500m</code> (19.9%) y <code>400m</code> (12.0%). Esto sugiere que Dim.1 representa un continuo entre atletas explosivos (buenos en salto largo, 400m) vs atletas de resistencia (1500m).</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#45-contribuciones-de-variables-a-dim2","title":"4.5. CONTRIBUCIONES DE VARIABLES A DIM.2","text":"<p>Ubicacion: Panel inferior centro</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#resultados_1","title":"Resultados:","text":"Variable Contribucion a Dim.2 Shot.put 34.1% Pole.vault 30.7% 1500m 10.6% High.jump 10.1% Javeline 8.7% Discus 3.0% Long.jump 1.2% 100m 0.9% 400m 0.7% 110m.hurdle 0.2%"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_3","title":"Interpretacion:","text":"<p>Conclusion para Dim.2: Esta dimension esta dominada por <code>Shot.put</code> (34.1%) y <code>Pole.vault</code> (30.7%). Representa un contraste entre atletas de lanzamiento (fuerza bruta) vs atletas tecnicos (salto con pertiga).</p> <p>Nota importante: Observa que <code>Shot.put</code> y <code>Pole.vault</code> apuntan en direcciones opuestas en el circulo de correlacion. Esto significa que: - Atletas arriba en el mapa: Buenos en lanzamiento de peso - Atletas abajo en el mapa: Buenos en salto con pertiga</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#46-biplot-individuos-variables","title":"4.6. BIPLOT (Individuos + Variables)","text":"<p>Ubicacion: Panel inferior derecho</p> <p>El biplot combina el mapa de individuos con las flechas de variables, permitiendo ver directamente que variables caracterizan a que atletas.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#como-leerlo","title":"Como Leerlo:","text":"<ol> <li>Proyecta mentalmente cada atleta sobre cada flecha de variable</li> <li>Si la proyeccion cae en la direccion de la flecha: el atleta tiene valor alto en esa variable</li> <li>Si la proyeccion cae en la direccion opuesta: el atleta tiene valor bajo en esa variable</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#ejemplo-de-interpretacion","title":"Ejemplo de Interpretacion:","text":"<ul> <li>BERNARD (extremo derecho):</li> <li>Proyeccion sobre <code>Long.jump</code> (izquierda): opuesta = mal salto largo</li> <li> <p>Proyeccion sobre <code>Shot.put</code> (arriba): perpendicular = promedio</p> </li> <li> <p>Atletas arriba-izquierda:</p> </li> <li>Buenos en <code>1500m</code> y <code>Shot.put</code></li> <li>Perfil de resistencia y fuerza</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#6-metricas-avanzadas-de-factominer","title":"6. Metricas Avanzadas de FactoMineR","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#51-calidad-de-representacion-cos2","title":"5.1. Calidad de Representacion (\\(\\cos^2\\))","text":"<p>El \\(\\cos^2\\) mide que tan bien representado esta un punto en el plano factorial.</p> \\[\\cos^2 = \\frac{\\text{distancia al origen en el plano}^2}{\\text{distancia al origen en el espacio original}^2}\\]"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_4","title":"Interpretacion:","text":"Valor de \\(\\cos^2\\) Significado Cercano a 1 El punto esta perfectamente representado en el plano Cercano a 0 El punto esta mal representado, su informacion esta en otras dimensiones"},{"location":"dashboards/02_PCA_FactoMineR_style/#resultados-para-los-atletas-top-5-mejor-representados","title":"Resultados para los Atletas (Top 5 mejor representados):","text":"Atleta \\(\\cos^2\\) en Dim1+Dim2 Interpretacion Bourguignon 0.755 75.5% de su variabilidad capturada BERNARD 0.725 72.5% de su variabilidad capturada Warners 0.717 71.7% de su variabilidad capturada Lorenzo 0.679 67.9% de su variabilidad capturada Clay 0.638 63.8% de su variabilidad capturada <p>Aplicacion practica: Si vas a interpretar la posicion de un atleta especifico, primero verifica su \\(\\cos^2\\). Si es bajo (&lt; 0.5), su posicion en el mapa puede ser enganosa.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#52-contribuciones","title":"5.2. Contribuciones","text":"<p>Las contribuciones indican cuanto aporta cada individuo/variable a la construccion de un eje.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#para-variables","title":"Para Variables:","text":"<p>Ya vimos que <code>Long.jump</code> contribuye 32.9% a Dim.1. Esto significa que si eliminaramos el salto largo del analisis, la primera dimension cambiaria drasticamente.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#para-individuos","title":"Para Individuos:","text":"Atleta Contribucion a Dim.1 BERNARD 12.82% Warners 12.42% Clay 8.62% <p>Interpretacion: BERNARD y Warners son atletas \"influyentes\" que \"tiran\" el eje hacia sus posiciones. Son atletas con perfiles extremos.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#7-tabla-de-equivalencias-rpython","title":"7. Tabla de Equivalencias R/Python","text":"<p>Para quienes vienen de R y quieren replicar analisis de FactoMineR:</p> FactoMineR (R) Prince (Python) Descripcion <code>res.pca$eig</code> <code>pca.eigenvalues_summary</code> Tabla de autovalores <code>res.pca$ind$coord</code> <code>pca.row_coordinates(df)</code> Coordenadas de individuos <code>res.pca$ind$contrib</code> <code>pca.row_contributions_</code> Contribuciones de individuos <code>res.pca$ind$cos2</code> <code>pca.row_cosine_similarities(df)</code> Cos2 de individuos <code>res.pca$var$coord</code> <code>pca.column_coordinates</code> Coordenadas de variables <code>res.pca$var$cor</code> <code>pca.column_correlations</code> Correlaciones de variables <code>res.pca$var$contrib</code> <code>pca.column_contributions_</code> Contribuciones de variables <code>res.pca$var$cos2</code> <code>pca.column_cosine_similarities_</code> Cos2 de variables"},{"location":"dashboards/02_PCA_FactoMineR_style/#8-conclusiones-del-analisis-del-decathlon","title":"8. Conclusiones del Analisis del Decathlon","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#71-hallazgos-principales","title":"7.1. Hallazgos Principales","text":"<ol> <li> <p>El decatlon es multidimensional: Ninguna dimension explica mas del 17% de la varianza. Se necesitan al menos 4 componentes para capturar el 56% de la informacion.</p> </li> <li> <p>Dimension 1 - Velocidad/Agilidad vs Resistencia:</p> </li> <li>Dominada por: <code>Long.jump</code> (32.9%), <code>1500m</code> (19.9%), <code>400m</code> (12.0%)</li> <li> <p>Separa atletas explosivos de atletas de resistencia</p> </li> <li> <p>Dimension 2 - Fuerza vs Tecnica:</p> </li> <li>Dominada por: <code>Shot.put</code> (34.1%), <code>Pole.vault</code> (30.7%)</li> <li> <p>Separa atletas de lanzamiento de atletas tecnicos</p> </li> <li> <p>Variable suplementaria (Competition):</p> </li> <li>Los atletas olimpicos muestran mayor variabilidad en sus perfiles</li> <li>No hay una separacion clara entre competiciones, lo que sugiere que el nivel de competencia no determina el tipo de perfil</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#72-implicaciones-practicas","title":"7.2. Implicaciones Practicas","text":"<ul> <li>Para entrenadores: Identificar en que cuadrante esta un atleta ayuda a disenar programas de entrenamiento personalizados</li> <li>Para analistas: Las contribuciones revelan que variables son mas importantes para caracterizar el rendimiento global</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#9-archivos-generados","title":"9. Archivos Generados","text":"Archivo Descripcion <code>02_PCA_FactoMineR_style.py</code> Codigo Python completo <code>02_PCA_FactoMineR_style.md</code> Este manual <code>02_PCA_FactoMineR_graficos.png</code> Panel de 6 graficos <code>02_PCA_circulo_correlacion.png</code> Circulo de correlacion individual <code>02_PCA_resultados_FactoMineR.xlsx</code> Excel con 7 hojas de resultados"},{"location":"dashboards/02_PCA_FactoMineR_style/#10-ejercicio-propuesto-para-el-alumno","title":"10. Ejercicio Propuesto para el Alumno","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#preguntas-de-comprension","title":"Preguntas de Comprension:","text":"<ol> <li> <p>Si un atleta tiene coordenada negativa en Dim.1, que puedes inferir sobre su salto largo?</p> </li> <li> <p>Por que <code>Discus</code> tiene contribucion casi nula (0.1%) a Dim.1?</p> </li> <li> <p>Si el \\(\\cos^2\\) de un atleta en el plano Dim1-Dim2 es 0.30, deberiamos confiar en su posicion en el mapa? Por que?</p> </li> <li> <p>Observando el biplot, que tipo de perfil tiene el atleta YURKOV (arriba en el mapa)?</p> </li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#respuestas","title":"Respuestas:","text":"<ol> <li> <p>Tiene buen salto largo (la correlacion de Long.jump con Dim.1 es negativa: -0.74)</p> </li> <li> <p>Porque la variabilidad del Discus esta representada principalmente en Dim.3 (contribucion 39.4%), no en Dim.1</p> </li> <li> <p>No deberiamos confiar mucho. Solo el 30% de su variabilidad esta capturada en ese plano. El 70% restante esta en otras dimensiones.</p> </li> <li> <p>YURKOV tiene valores altos en las variables que apuntan hacia arriba (<code>Shot.put</code>, <code>1500m</code>, <code>High.jump</code>): perfil de fuerza y resistencia.</p> </li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#11-referencias-academicas","title":"11. Referencias Academicas","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#referencia-principal-del-tutorial","title":"Referencia Principal del Tutorial","text":"<p>FactoMineR Tutorial Husson, F., &amp; Josse, J. http://factominer.free.fr/course/FactoTuto.html \"This course presents the main methods used in Exploratory Data Analysis with the FactoMineR package.\"</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#articulo-original-de-factominer","title":"Articulo Original de FactoMineR","text":"<p>Le, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: An R Package for Multivariate Analysis. Journal of Statistical Software, 25(1), 1-18. DOI: 10.18637/jss.v025.i01</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#libro-de-referencia","title":"Libro de Referencia","text":"<p>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. 2<sup>nd</sup> Edition. CRC Press. ISBN: 978-1138196346</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#informacion-institucional","title":"Informacion Institucional","text":"<p>Autor/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Area: Big Data, Ciencia de Datos &amp; Econometria Aplicada.</p> <p>Hash ID de Certificacion: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code></p>"},{"location":"dashboards/02_pca_iris_clustering/","title":"PCA + Clustering K-Means: Dataset Iris","text":"<p>Autor: @TodoEconometria | Profesor: Juan Marcelo Gutierrez Miranda</p>"},{"location":"dashboards/02_pca_iris_clustering/#tabla-de-contenidos","title":"\ud83d\udcda Tabla de Contenidos","text":"<ol> <li>Introducci\u00f3n</li> <li>El Dataset Iris: Un Cl\u00e1sico del Machine Learning</li> <li>Por Qu\u00e9 Combinar PCA + Clustering</li> <li>An\u00e1lisis de Componentes Principales (PCA)</li> <li>Clustering K-Means</li> <li>Interpretaci\u00f3n de Resultados</li> <li>Conclusiones y Recomendaciones</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>Este documento presenta un an\u00e1lisis completo del famoso dataset Iris combinando dos t\u00e9cnicas fundamentales del Machine Learning no supervisado:</p> <ul> <li>PCA (Principal Component Analysis): Reducci\u00f3n de dimensionalidad</li> <li>K-Means Clustering: Agrupaci\u00f3n de observaciones</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#objetivos-del-analisis","title":"\ud83c\udfaf Objetivos del An\u00e1lisis","text":"<ol> <li>Reducir las 4 dimensiones originales a 2 dimensiones principales</li> <li>Identificar grupos naturales en los datos (especies de flores)</li> <li>Visualizar patrones y relaciones en un espacio 2D</li> <li>Validar si el clustering no supervisado puede descubrir las 3 especies conocidas</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#2-el-dataset-iris-un-clasico-del-machine-learning","title":"2. El Dataset Iris: Un Cl\u00e1sico del Machine Learning","text":""},{"location":"dashboards/02_pca_iris_clustering/#historia-y-contexto","title":"\ud83d\udcd6 Historia y Contexto","text":"<p>El dataset Iris fue introducido por Ronald Fisher en 1936 en su paper seminal:</p> <p>Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.</p> <p>Es uno de los datasets m\u00e1s utilizados en:</p> <ul> <li>Ense\u00f1anza de Machine Learning</li> <li>Validaci\u00f3n de algoritmos de clasificaci\u00f3n</li> <li>Ejemplos de visualizaci\u00f3n de datos</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#descripcion-del-dataset","title":"\ud83c\udf38 Descripci\u00f3n del Dataset","text":"Caracter\u00edstica Descripci\u00f3n Observaciones 150 flores Especies 3 (Setosa, Versicolor, Virginica) Variables 4 medidas en cent\u00edmetros Distribuci\u00f3n 50 flores por especie (balanceado)"},{"location":"dashboards/02_pca_iris_clustering/#variables-medidas","title":"\ud83d\udccf Variables Medidas","text":"<ol> <li>Sepal Length (Largo del s\u00e9palo)</li> <li>Sepal Width (Ancho del s\u00e9palo)</li> <li>Petal Length (Largo del p\u00e9talo)</li> <li>Petal Width (Ancho del p\u00e9talo)</li> </ol> <p>NOTA BOT\u00c1NICA: El s\u00e9palo es la parte verde que protege la flor antes de abrirse. El p\u00e9talo es la parte colorida de la flor.</p>"},{"location":"dashboards/02_pca_iris_clustering/#por-que-es-importante-este-dataset","title":"\ud83d\udd0d \u00bfPor Qu\u00e9 es Importante Este Dataset?","text":"<ol> <li>Tama\u00f1o Manejable: 150 observaciones son suficientes para aprender sin ser abrumadoras</li> <li>Bien Balanceado: 50 flores de cada especie (no hay desbalance de clases)</li> <li>Separabilidad: Una especie (Setosa) es linealmente separable, las otras dos se superponen ligeramente</li> <li>Multivariate: 4 variables permiten practicar t\u00e9cnicas de reducci\u00f3n de dimensionalidad</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#3-por-que-combinar-pca-clustering","title":"3. Por Qu\u00e9 Combinar PCA + Clustering","text":""},{"location":"dashboards/02_pca_iris_clustering/#el-problema-de-la-dimensionalidad","title":"\ud83e\udd14 El Problema de la Dimensionalidad","text":"<p>Cuando tenemos m\u00e1s de 3 dimensiones, es imposible visualizar los datos directamente:</p> <ul> <li>1D: L\u00ednea (f\u00e1cil)</li> <li>2D: Plano (f\u00e1cil)</li> <li>3D: Espacio 3D (posible pero dif\u00edcil)</li> <li>4D+: \u274c Imposible de visualizar</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#la-solucion-pca-clustering","title":"\ud83d\udca1 La Soluci\u00f3n: PCA + Clustering","text":"<pre><code>Datos Originales (4D)\n        \u2193\n    PCA (Reducci\u00f3n)\n        \u2193\nDatos Reducidos (2D) \u2190 Ahora podemos VISUALIZAR\n        \u2193\n    K-Means (Agrupaci\u00f3n)\n        \u2193\n  Clusters Identificados\n</code></pre>"},{"location":"dashboards/02_pca_iris_clustering/#ventajas-de-esta-combinacion","title":"\u2705 Ventajas de Esta Combinaci\u00f3n","text":"Ventaja Explicaci\u00f3n Visualizaci\u00f3n PCA reduce a 2D para graficar Reducci\u00f3n de Ruido PCA elimina varianza no informativa Mejor Clustering K-Means funciona mejor en espacios de menor dimensi\u00f3n Interpretabilidad Podemos ver y entender los clusters en 2D"},{"location":"dashboards/02_pca_iris_clustering/#4-analisis-de-componentes-principales-pca","title":"4. An\u00e1lisis de Componentes Principales (PCA)","text":""},{"location":"dashboards/02_pca_iris_clustering/#que-es-pca","title":"\ud83c\udfaf \u00bfQu\u00e9 es PCA?","text":"<p>PCA es una t\u00e9cnica que:</p> <ol> <li>Encuentra las direcciones de m\u00e1xima varianza en los datos</li> <li>Proyecta los datos en esas direcciones (componentes principales)</li> <li>Reduce la dimensionalidad manteniendo la mayor informaci\u00f3n posible</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#resultados-del-pca-en-iris","title":"\ud83d\udcca Resultados del PCA en Iris","text":""},{"location":"dashboards/02_pca_iris_clustering/#varianza-explicada","title":"Varianza Explicada","text":"Dimensi\u00f3n Autovalor Varianza (%) Varianza Acumulada (%) Dim.1 ~2.92 ~73% ~73% Dim.2 ~0.91 ~23% ~96% Dim.3 ~0.15 ~4% ~99% Dim.4 ~0.02 ~1% ~100% <p>INTERPRETACI\u00d3N: Las primeras 2 dimensiones capturan ~96% de la varianza total. Esto significa que podemos reducir de 4D a 2D perdiendo solo ~4% de informaci\u00f3n.</p>"},{"location":"dashboards/02_pca_iris_clustering/#regla-de-kaiser","title":"Regla de Kaiser","text":"<p>La Regla de Kaiser dice: Retener componentes con autovalor &gt; 1</p> <ul> <li>Dim.1: Autovalor = 2.92 \u2705 (Retener)</li> <li>Dim.2: Autovalor = 0.91 \u26a0\ufe0f (Casi 1, retener para visualizaci\u00f3n)</li> <li>Dim.3: Autovalor = 0.15 \u274c (Descartar)</li> <li>Dim.4: Autovalor = 0.02 \u274c (Descartar)</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#interpretacion-de-las-dimensiones","title":"\ud83d\udd0d Interpretaci\u00f3n de las Dimensiones","text":""},{"location":"dashboards/02_pca_iris_clustering/#dimension-1-73-de-varianza","title":"Dimensi\u00f3n 1 (~73% de varianza)","text":"<p>Variables que m\u00e1s contribuyen:</p> <ul> <li>Petal Length (~42%)</li> <li>Petal Width (~42%)</li> </ul> <p>Interpretaci\u00f3n:</p> <p>Dim.1 representa el \"tama\u00f1o del p\u00e9talo\". Flores con valores altos en Dim.1 tienen p\u00e9talos grandes; valores bajos tienen p\u00e9talos peque\u00f1os.</p>"},{"location":"dashboards/02_pca_iris_clustering/#dimension-2-23-de-varianza","title":"Dimensi\u00f3n 2 (~23% de varianza)","text":"<p>Variables que m\u00e1s contribuyen:</p> <ul> <li>Sepal Width (~72%)</li> </ul> <p>Interpretaci\u00f3n:</p> <p>Dim.2 representa el \"ancho del s\u00e9palo\". Flores con valores altos en Dim.2 tienen s\u00e9palos anchos; valores bajos tienen s\u00e9palos estrechos.</p>"},{"location":"dashboards/02_pca_iris_clustering/#circulo-de-correlacion","title":"\ud83d\udcc8 C\u00edrculo de Correlaci\u00f3n","text":"<p>El c\u00edrculo de correlaci\u00f3n muestra c\u00f3mo las variables originales se relacionan con las dimensiones principales:</p> <pre><code>           Dim.2 (Sepal Width)\n                 \u2191\n                 |\n    Sepal Width  |\n         \u2191       |\n         |       |\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Dim.1 (Petal Size)\n         |       |\n         |   Petal Length \u2192\n         |   Petal Width \u2192\n         |\n</code></pre> <p>Observaciones:</p> <ul> <li>Petal Length y Petal Width est\u00e1n muy correlacionadas (flechas en la misma direcci\u00f3n)</li> <li>Sepal Width es casi perpendicular a las medidas de p\u00e9talo (baja correlaci\u00f3n)</li> <li>Sepal Length est\u00e1 entre ambas dimensiones</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#5-clustering-k-means","title":"5. Clustering K-Means","text":""},{"location":"dashboards/02_pca_iris_clustering/#que-es-k-means","title":"\ud83c\udfaf \u00bfQu\u00e9 es K-Means?","text":"<p>K-Means es un algoritmo de clustering que:</p> <ol> <li>Divide los datos en K grupos (clusters)</li> <li>Minimiza la distancia de cada punto a su centroide</li> <li>Itera hasta convergencia</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#determinacion-del-numero-optimo-de-clusters","title":"\ud83d\udd22 Determinaci\u00f3n del N\u00famero \u00d3ptimo de Clusters","text":""},{"location":"dashboards/02_pca_iris_clustering/#metodo-del-codo-elbow-method","title":"M\u00e9todo del Codo (Elbow Method)","text":"<p>Graficamos la inercia (suma de distancias al cuadrado) vs K:</p> <pre><code>Inercia\n  \u2502\n  \u2502 \u25cf\n  \u2502   \u25cf\n  \u2502     \u25cf  \u2190 \"Codo\" en K=3\n  \u2502       \u25cf\n  \u2502         \u25cf\n  \u2502           \u25cf\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 K\n    2  3  4  5  6  7\n</code></pre> <p>Interpretaci\u00f3n: El \"codo\" est\u00e1 en K=3, sugiriendo 3 clusters.</p>"},{"location":"dashboards/02_pca_iris_clustering/#silhouette-score","title":"Silhouette Score","text":"<p>El Silhouette Score mide qu\u00e9 tan bien separados est\u00e1n los clusters:</p> <ul> <li>Valor: Entre -1 y 1</li> <li>Interpretaci\u00f3n:</li> <li>Cercano a 1: Clusters bien separados \u2705</li> <li>Cercano a 0: Clusters superpuestos \u26a0\ufe0f</li> <li>Negativo: Puntos mal asignados \u274c</li> </ul> <p>Resultado para Iris: Silhouette Score \u2248 0.55 (buena separaci\u00f3n)</p>"},{"location":"dashboards/02_pca_iris_clustering/#resultados-del-clustering","title":"\ud83d\udcca Resultados del Clustering","text":""},{"location":"dashboards/02_pca_iris_clustering/#confusion-matrix-clusters-vs-especies-reales","title":"Confusion Matrix: Clusters vs Especies Reales","text":"Cluster 0 Cluster 1 Cluster 2 Setosa 50 0 0 Versicolor 0 48 2 Virginica 0 14 36 <p>Observaciones:</p> <ul> <li>Setosa: Perfectamente separada (100% en Cluster 0)</li> <li>Versicolor: Mayormente en Cluster 1 (96%)</li> <li>Virginica: Mayormente en Cluster 2 (72%), pero con superposici\u00f3n con Versicolor</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#pureza-de-clusters","title":"Pureza de Clusters","text":"<p>La pureza mide el porcentaje de observaciones correctamente agrupadas:</p> <pre><code>Pureza = (50 + 48 + 36) / 150 = 89.3%\n</code></pre> <p>INTERPRETACI\u00d3N: El algoritmo K-Means logr\u00f3 identificar correctamente las especies en 89.3% de los casos, sin conocer las etiquetas reales. Esto es excelente para un m\u00e9todo no supervisado.</p>"},{"location":"dashboards/02_pca_iris_clustering/#visualizacion-de-clusters","title":"\ud83c\udfa8 Visualizaci\u00f3n de Clusters","text":"<p>En el espacio 2D del PCA, los clusters se ven as\u00ed:</p> <pre><code>     Dim.2\n       \u2191\n       \u2502     \u25cf Cluster 2 (Virginica)\n       \u2502    \u25cf\u25cf\u25cf\n       \u2502   \u25cf\u25cf\u25cf\u25cf\n       \u2502  \u25cf\u25cf\u25cf\u25cf\n       \u2502 \u25cf\u25cf\u25cf\u25cf  \u25a0\u25a0\u25a0 Cluster 1 (Versicolor)\n       \u2502\u25cf\u25cf\u25cf   \u25a0\u25a0\u25a0\u25a0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Dim.1\n       \u2502\n       \u2502  \u25b2\u25b2\u25b2\n       \u2502 \u25b2\u25b2\u25b2\u25b2\u25b2\n       \u2502\u25b2\u25b2\u25b2\u25b2\u25b2\u25b2  Cluster 0 (Setosa)\n       \u2502\n</code></pre> <p>Centroides (marcados con X):</p> <ul> <li>Cluster 0: (-2.7, 0.3) \u2192 Setosa</li> <li>Cluster 1: (0.3, -0.5) \u2192 Versicolor</li> <li>Cluster 2: (1.7, 0.2) \u2192 Virginica</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#6-interpretacion-de-resultados","title":"6. Interpretaci\u00f3n de Resultados","text":""},{"location":"dashboards/02_pca_iris_clustering/#analisis-por-especie","title":"\ud83d\udd2c An\u00e1lisis por Especie","text":""},{"location":"dashboards/02_pca_iris_clustering/#setosa-cluster-0","title":"Setosa (Cluster 0)","text":"<p>Caracter\u00edsticas:</p> <ul> <li>Petal Length: Muy peque\u00f1o (~1.5 cm)</li> <li>Petal Width: Muy peque\u00f1o (~0.2 cm)</li> <li>Sepal Width: Relativamente grande</li> </ul> <p>Posici\u00f3n en PCA:</p> <ul> <li>Dim.1: Valores muy negativos (p\u00e9talos peque\u00f1os)</li> <li>Dim.2: Valores positivos (s\u00e9palos anchos)</li> </ul> <p>Separabilidad: \u2705 Perfecta (100% correctamente agrupada)</p>"},{"location":"dashboards/02_pca_iris_clustering/#versicolor-cluster-1","title":"Versicolor (Cluster 1)","text":"<p>Caracter\u00edsticas:</p> <ul> <li>Petal Length: Mediano (~4.3 cm)</li> <li>Petal Width: Mediano (~1.3 cm)</li> <li>Sepal Width: Mediano</li> </ul> <p>Posici\u00f3n en PCA:</p> <ul> <li>Dim.1: Valores cercanos a 0 (p\u00e9talos medianos)</li> <li>Dim.2: Valores ligeramente negativos</li> </ul> <p>Separabilidad: \u26a0\ufe0f Buena (96% correctamente agrupada, 4% confundida con Virginica)</p>"},{"location":"dashboards/02_pca_iris_clustering/#virginica-cluster-2","title":"Virginica (Cluster 2)","text":"<p>Caracter\u00edsticas:</p> <ul> <li>Petal Length: Grande (~5.5 cm)</li> <li>Petal Width: Grande (~2.0 cm)</li> <li>Sepal Width: Mediano</li> </ul> <p>Posici\u00f3n en PCA:</p> <ul> <li>Dim.1: Valores muy positivos (p\u00e9talos grandes)</li> <li>Dim.2: Valores cercanos a 0</li> </ul> <p>Separabilidad: \u26a0\ufe0f Moderada (72% correctamente agrupada, 28% confundida con Versicolor)</p>"},{"location":"dashboards/02_pca_iris_clustering/#metricas-de-evaluacion","title":"\ud83d\udcca M\u00e9tricas de Evaluaci\u00f3n","text":"M\u00e9trica Valor Interpretaci\u00f3n Silhouette Score 0.55 Buena separaci\u00f3n entre clusters Davies-Bouldin Index 0.66 Clusters compactos y separados (menor es mejor) Calinski-Harabasz Index 561.63 Alta separaci\u00f3n entre clusters (mayor es mejor) Pureza 89.3% Alta concordancia con especies reales"},{"location":"dashboards/02_pca_iris_clustering/#por-que-versicolor-y-virginica-se-superponen","title":"\ud83c\udfaf \u00bfPor Qu\u00e9 Versicolor y Virginica se Superponen?","text":"<p>Raz\u00f3n Biol\u00f3gica:</p> <ul> <li>Versicolor y Virginica son especies evolutivamente m\u00e1s cercanas</li> <li>Comparten caracter\u00edsticas morfol\u00f3gicas similares</li> <li>Setosa es m\u00e1s distinta (probablemente de un linaje diferente)</li> </ul> <p>Raz\u00f3n Estad\u00edstica:</p> <ul> <li>Las medidas de p\u00e9talo de Versicolor y Virginica tienen rangos superpuestos</li> <li>No existe una frontera clara en el espacio de 4 dimensiones</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#7-conclusiones-y-recomendaciones","title":"7. Conclusiones y Recomendaciones","text":""},{"location":"dashboards/02_pca_iris_clustering/#conclusiones-principales","title":"\u2705 Conclusiones Principales","text":"<ol> <li>PCA es Efectivo:</li> <li>Reduce de 4D a 2D manteniendo 96% de la informaci\u00f3n</li> <li> <p>Las 2 primeras dimensiones son suficientes para visualizaci\u00f3n y clustering</p> </li> <li> <p>Las Medidas de P\u00e9talo son Clave:</p> </li> <li>Petal Length y Petal Width son las variables m\u00e1s discriminantes</li> <li> <p>Dim.1 (que representa el tama\u00f1o del p\u00e9talo) explica 73% de la varianza</p> </li> <li> <p>K-Means Funciona Bien:</p> </li> <li>Identifica correctamente las 3 especies en 89.3% de los casos</li> <li>Setosa es perfectamente separable</li> <li> <p>Versicolor y Virginica tienen cierta superposici\u00f3n natural</p> </li> <li> <p>Validaci\u00f3n del M\u00e9todo No Supervisado:</p> </li> <li>Sin conocer las etiquetas, K-Means descubre los 3 grupos naturales</li> <li>Esto valida que las especies tienen diferencias morfol\u00f3gicas reales</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#lecciones-para-estudiantes","title":"\ud83c\udf93 Lecciones para Estudiantes","text":""},{"location":"dashboards/02_pca_iris_clustering/#leccion-1-la-importancia-de-la-reduccion-de-dimensionalidad","title":"Lecci\u00f3n 1: La Importancia de la Reducci\u00f3n de Dimensionalidad","text":"<p>ANTES DE PCA: 4 variables \u2192 Dif\u00edcil de visualizar \u2192 Dif\u00edcil de interpretar</p> <p>DESPU\u00c9S DE PCA: 2 dimensiones \u2192 F\u00e1cil de visualizar \u2192 Patrones claros</p> <p>Moraleja: No siempre necesitas todas las variables. A veces, menos es m\u00e1s.</p>"},{"location":"dashboards/02_pca_iris_clustering/#leccion-2-el-clustering-no-supervisado-puede-descubrir-estructura-real","title":"Lecci\u00f3n 2: El Clustering No Supervisado Puede Descubrir Estructura Real","text":"<p>SIN ETIQUETAS: K-Means encuentra 3 grupos</p> <p>CON ETIQUETAS: Hay 3 especies reales</p> <p>COINCIDENCIA: 89.3%</p> <p>Moraleja: Los datos tienen estructura natural. Los algoritmos pueden encontrarla.</p>"},{"location":"dashboards/02_pca_iris_clustering/#leccion-3-no-todos-los-grupos-son-perfectamente-separables","title":"Lecci\u00f3n 3: No Todos los Grupos son Perfectamente Separables","text":"<p>Setosa: 100% separable</p> <p>Versicolor/Virginica: Superposici\u00f3n natural</p> <p>Moraleja: En datos reales, la superposici\u00f3n es normal. No esperes clusters perfectos.</p>"},{"location":"dashboards/02_pca_iris_clustering/#leccion-4-validar-validar-validar","title":"Lecci\u00f3n 4: Validar, Validar, Validar","text":"<p>M\u00e9todo del Codo: Sugiere K=3</p> <p>Silhouette Score: Confirma K=3</p> <p>Pureza: Valida que K=3 es correcto</p> <p>Moraleja: Usa m\u00faltiples m\u00e9tricas para validar tus decisiones.</p>"},{"location":"dashboards/02_pca_iris_clustering/#recomendaciones-practicas","title":"\ud83d\udd27 Recomendaciones Pr\u00e1cticas","text":""},{"location":"dashboards/02_pca_iris_clustering/#para-clasificacion-de-especies-de-iris","title":"Para Clasificaci\u00f3n de Especies de Iris","text":"<ol> <li>Enfocarse en medidas de p\u00e9talo (son las m\u00e1s discriminantes)</li> <li>Usar PCA para visualizaci\u00f3n (reduce complejidad sin perder informaci\u00f3n)</li> <li>K=3 es \u00f3ptimo (validado por m\u00faltiples m\u00e9tricas)</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#para-analisis-de-datos-similares","title":"Para An\u00e1lisis de Datos Similares","text":"<ol> <li>Siempre hacer EDA primero (entender distribuciones y correlaciones)</li> <li>Estandarizar antes de PCA (variables en diferentes escalas sesgan resultados)</li> <li>Validar n\u00famero de clusters (no asumir K, usar Elbow + Silhouette)</li> <li>Comparar con ground truth (si est\u00e1 disponible, como en este caso)</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#extensiones-posibles","title":"\ud83d\ude80 Extensiones Posibles","text":"<ol> <li>Otros Algoritmos de Clustering:</li> <li>DBSCAN (para clusters de forma arbitraria)</li> <li>Hierarchical Clustering (para dendrogramas)</li> <li> <p>Gaussian Mixture Models (para clusters probabil\u00edsticos)</p> </li> <li> <p>Clasificaci\u00f3n Supervisada:</p> </li> <li>Usar las especies conocidas para entrenar un clasificador</li> <li> <p>Comparar con clustering no supervisado</p> </li> <li> <p>An\u00e1lisis de Variables Suplementarias:</p> </li> <li>Agregar informaci\u00f3n de ubicaci\u00f3n geogr\u00e1fica</li> <li>Agregar informaci\u00f3n de temporada de recolecci\u00f3n</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#referencias","title":"\ud83d\udcda Referencias","text":""},{"location":"dashboards/02_pca_iris_clustering/#papers-originales","title":"Papers Originales","text":"<ul> <li>Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.</li> <li> <p>El paper original que introdujo el dataset Iris</p> </li> <li> <p>Anderson, E. (1935). The irises of the Gaspe Peninsula. Bulletin of the American Iris Society, 59, 2-5.</p> </li> <li>El bot\u00e1nico que recolect\u00f3 los datos originales</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#libros-de-referencia","title":"Libros de Referencia","text":"<ul> <li>Husson, F., L\u00ea, S., &amp; Pag\u00e8s, J. (2017). Exploratory Multivariate Analysis by Example Using R. CRC Press.</li> <li> <p>Referencia principal para PCA estilo FactoMineR</p> </li> <li> <p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.</p> </li> <li>Cap\u00edtulos sobre PCA y Clustering</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#articulos-tecnicos","title":"Art\u00edculos T\u00e9cnicos","text":"<ul> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> <li> <p>Documentaci\u00f3n de las librer\u00edas utilizadas</p> </li> <li> <p>Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.</p> </li> <li>M\u00e9todo del Silhouette Score</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#recursos-adicionales","title":"\ud83d\udd17 Recursos Adicionales","text":""},{"location":"dashboards/02_pca_iris_clustering/#tutoriales-online","title":"Tutoriales Online","text":"<ul> <li>Scikit-learn: PCA Tutorial</li> <li>Scikit-learn: K-Means Tutorial</li> <li>FactoMineR Tutorial</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#datasets-similares","title":"Datasets Similares","text":"<ul> <li>Wine Dataset: 178 vinos, 13 variables qu\u00edmicas, 3 clases</li> <li>Breast Cancer Dataset: 569 tumores, 30 variables, 2 clases (maligno/benigno)</li> <li>Digits Dataset: 1797 im\u00e1genes de d\u00edgitos, 64 p\u00edxeles, 10 clases</li> </ul> <p>Autor: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Fecha: Enero 2026 Licencia: Uso educativo con atribuci\u00f3n</p>"},{"location":"dashboards/02_pca_iris_clustering/#preguntas-frecuentes-faq","title":"\ud83d\udcac Preguntas Frecuentes (FAQ)","text":""},{"location":"dashboards/02_pca_iris_clustering/#por-que-estandarizar-antes-de-pca","title":"\u00bfPor qu\u00e9 estandarizar antes de PCA?","text":"<p>Respuesta: Porque PCA es sensible a la escala de las variables. Si una variable tiene valores mucho mayores que otra (ej: ingresos en miles vs edad en decenas), dominar\u00e1 la varianza y sesgar\u00e1 los resultados.</p>"},{"location":"dashboards/02_pca_iris_clustering/#cuantas-componentes-debo-retener","title":"\u00bfCu\u00e1ntas componentes debo retener?","text":"<p>Respuesta: Depende del objetivo:</p> <ul> <li>Visualizaci\u00f3n: 2-3 componentes</li> <li>Regla de Kaiser: Componentes con autovalor &gt; 1</li> <li>Varianza Acumulada: Retener hasta alcanzar 80-95% de varianza</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#k-means-siempre-encuentra-los-clusters-correctos","title":"\u00bfK-Means siempre encuentra los clusters correctos?","text":"<p>Respuesta: No. K-Means tiene limitaciones:</p> <ul> <li>Asume clusters esf\u00e9ricos</li> <li>Sensible a inicializaci\u00f3n (usar <code>n_init</code> alto)</li> <li>Requiere especificar K de antemano</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#que-pasa-si-tengo-mas-de-3-especies","title":"\u00bfQu\u00e9 pasa si tengo m\u00e1s de 3 especies?","text":"<p>Respuesta: El proceso es el mismo:</p> <ol> <li>Usar Elbow + Silhouette para determinar K \u00f3ptimo</li> <li>Validar con m\u00e9tricas (pureza, confusion matrix)</li> <li>Visualizar en 2D con PCA (aunque haya m\u00e1s de 3 clusters)</li> </ol> <p>FIN DEL DOCUMENTO</p>"},{"location":"dashboards/04_series_temporales_arima/","title":"Series Temporales: ARIMA/SARIMA (Box-Jenkins)","text":"<p>Analisis completo de series temporales siguiendo la metodologia Box-Jenkins, desde la identificacion del modelo hasta el pronostico con intervalos de confianza.</p> <p>Abrir Dashboard Interactivo</p>"},{"location":"dashboards/04_series_temporales_arima/#contenido","title":"Contenido","text":"<p>El dashboard presenta 6 pestanas interactivas:</p> Pestana Contenido Serie Original Pasajeros aereos mensuales 1949-1960 (144 observaciones) Descomposicion Tendencia + Estacionalidad + Residuo (multiplicativa) ACF / PACF Autocorrelacion y autocorrelacion parcial de la serie diferenciada Diagnostico Residuos, histograma, ACF residual, Q-Q plot Pronostico Final Serie original + ajuste SARIMA + forecast 12 meses + IC 95% Metricas Radar RMSE, MAE, MAPE, R2 del modelo"},{"location":"dashboards/04_series_temporales_arima/#metodologia-box-jenkins","title":"Metodologia Box-Jenkins","text":"<ol> <li>Identificacion: ACF/PACF para determinar ordenes (p, d, q)(P, D, Q)[s]</li> <li>Estimacion: Ajuste por maxima verosimilitud</li> <li>Diagnostico: Tests de Ljung-Box, Jarque-Bera sobre residuos</li> <li>Pronostico: Forecast con intervalos de confianza al 95%</li> </ol> <p>Modelo seleccionado: SARIMA(1,1,0)(0,1,0)[12] \u2014 AIC = -445.41</p>"},{"location":"dashboards/04_series_temporales_arima/#codigo-fuente","title":"Codigo fuente","text":"<ul> <li>Script completo: <code>ejercicios/04_machine_learning/07_series_temporales_arima/</code></li> <li>Guia teorica: <code>07_series_temporales_arima/README.md</code></li> </ul>"},{"location":"dashboards/04_similitud_jaccard/","title":"\ud83e\uddf6 An\u00e1lisis de Similitud: El Mystery del Portal Web","text":"<p>\"Un sistema de recomendaci\u00f3n es como un bibliotecario que sabe exactamente qu\u00e9 revista te va a gustar sin haber le\u00eddo el contenido completo, solo mirando las palabras que se repiten. \ud83e\udd18\"</p>"},{"location":"dashboards/04_similitud_jaccard/#el-desafio-del-portal","title":"\ud83c\udfaf El Desaf\u00edo del Portal","text":"<p>Imagina que gestionas un portal din\u00e1mico. Tu jefe te ha puesto un reto: \"Agrupa estos art\u00edculos autom\u00e1ticamente. No tengo tiempo para leerlos todos.\"</p> <p></p> <p>Para resolverlo, usamos el \u00cdndice de Jaccard, una herramienta matem\u00e1tica que convierte el texto en \"conjuntos\" y mide cu\u00e1nto se solapan. \u26a1</p>"},{"location":"dashboards/04_similitud_jaccard/#el-corazon-del-algoritmo","title":"\ud83c\udfd7\ufe0f El Coraz\u00f3n del Algoritmo","text":"<p>La magia ocurre comparando lo que los documentos comparten frente a todo lo que dicen.</p> Atributo Explicaci\u00f3n Visual Intersecci\u00f3n Las palabras que aparecen en AMBOS textos. \u2694\ufe0f Uni\u00f3n Todas las palabras \u00fanicas de AMBOS textos. \ud83c\udf0c Resultado Un n\u00famero entre 0 (desconocidos) y 1 (almas gemelas). \ud83e\udd18 <p></p>"},{"location":"dashboards/04_similitud_jaccard/#resultados-reales-generados-por-tu-script","title":"\ud83c\udfb8 Resultados Reales (Generados por tu Script)","text":"<p>Aqu\u00ed es donde la teor\u00eda se encuentra con la realidad. Al ejecutar <code>04_similitud_jaccard.py</code>, el sistema \"ve\" el portal as\u00ed:</p>"},{"location":"dashboards/04_similitud_jaccard/#1-el-mapa-de-calor-del-saber","title":"1. El Mapa de Calor del Saber","text":"<p>En esta matriz, los colores c\u00e1lidos (rojos) indican alta similitud. Observa c\u00f3mo se forman cuadrados en la diagonal. \u00a1Eso son tus categor\u00edas de F\u00fatbol, Tecnolog\u00eda y Cocina detectadas autom\u00e1ticamente! \u26a1</p> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#2-la-prueba-del-algoritmo-clustermap","title":"2. La Prueba del Algoritmo (Clustermap)","text":"<p>\u00bfPuede la inteligencia artificial agrupar los temas sin ayuda? El Dendrograma (el \u00e1rbol lateral) nos dice que s\u00ed. Los art\u00edculos de la misma tem\u00e1tica se \"buscan\" y se agrupan en ramas comunes. \ud83d\udc80</p> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#aplicaciones-en-el-mundo-real","title":"\u2694\ufe0f Aplicaciones en el Mundo Real","text":"<p>No es solo un ejercicio acad\u00e9mico. Esta t\u00e9cnica se usa cada segundo en:</p> <ul> <li>\ud83c\udff4\u200d\u2620\ufe0f Detecci\u00f3n de Plagio: Comparar entregas de alumnos para ver si comparten \"demasiado\" vocabulario.</li> <li>\ud83d\udef8 Recomendadores: \"Si le\u00edste sobre el nuevo CPU, te recomiendo este art\u00edculo sobre memoria RAM\".</li> <li>\u26d3\ufe0f SEO y Buscadores: Para entender si dos p\u00e1ginas hablan de lo mismo y evitar contenido duplicado.</li> </ul>"},{"location":"dashboards/04_similitud_jaccard/#reflexion-final-para-el-alumno","title":"\ud83c\udf11 Reflexi\u00f3n Final para el Alumno","text":"<p>Mira las gr\u00e1ficas que se han guardado en tu carpeta:</p> <ol> <li>\u00bfVes alg\u00fan punto rojo fuera de la diagonal? Eso indicar\u00eda que dos temas diferentes comparten palabras.</li> <li>\u00bfQu\u00e9 pasar\u00eda si el corpus fuera de 10,000 documentos? El mapa de calor se volver\u00eda ilegible, pero el Clustermap seguir\u00eda d\u00e1ndonos la estructura. \ud83e\udd18</li> </ol> <p>Hash de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Autor: Juan Marcelo Gutierrez Miranda (@TodoEconometria)</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/","title":"\ud83c\udf93 Gu\u00eda Formativa: Vectorizaci\u00f3n y Clustering de Documentos","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#analisis-de-topicos-con-inteligencia-artificial-nlp-machine-learning","title":"\ud83d\ude80 An\u00e1lisis de T\u00f3picos con Inteligencia Artificial (NLP &amp; Machine Learning)","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#informacion-de-certificacion-y-referencia","title":"\ud83d\udcdd Informaci\u00f3n de Certificaci\u00f3n y Referencia","text":"<p>Autor original/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Metodolog\u00eda: Cursos Avanzados de Big Data, Ciencia de Datos, Desarrollo de aplicaciones con IA &amp; Econometr\u00eda Aplicada. Hash ID de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Repositorio: https://github.com/TodoEconometria/certificaciones </p> <p>REFERENCIA ACAD\u00c9MICA:</p> <ul> <li>McKinney, W. (2012). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media.</li> <li>Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362.</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#1-introduccion-al-laboratorio","title":"\ud83c\udfdb\ufe0f 1. Introducci\u00f3n al Laboratorio","text":"<p>En el mundo del Big Data, la mayor parte de la informaci\u00f3n es no estructurada (textos, correos, noticias). El reto fundamental es: \u00bfC\u00f3mo puede una computadora entender que dos documentos hablan de lo mismo sin leerlos?</p> <p>Este ejercicio implementa un pipeline completo de Ciencia de Datos para agrupar autom\u00e1ticamente 1,200 documentos en espa\u00f1ol, utilizando una combinaci\u00f3n de t\u00e9cnicas matem\u00e1ticas avanzadas para transformar el lenguaje humano en estructuras que las m\u00e1quinas pueden procesar.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#2-fundamentos-teoricos-que-usamos-y-por-que","title":"\ud83e\udde0 2. Fundamentos Te\u00f3ricos (\u00bfQu\u00e9 usamos y por qu\u00e9?)","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#a-vectorizacion-el-puente-entre-texto-y-matematicas","title":"A. Vectorizaci\u00f3n: El Puente entre Texto y Matem\u00e1ticas","text":"<p>Las computadoras no entienden palabras, solo n\u00fameros. Usamos TF-IDF (Term Frequency - Inverse Document Frequency) por su capacidad de discernir la relevancia.</p> <ul> <li>\u00bfQu\u00e9 es?: Un valor estad\u00edstico que busca medir qu\u00e9 tan importante es una palabra para un documento dentro de una colecci\u00f3n (corpus).</li> <li>\u00bfC\u00f3mo funciona?:     $\\(TF(t, d) = \\frac{\\text{Conteo de la palabra } t \\text{ en documento } d}{\\text{Total de palabras en } d}\\)$     $\\(IDF(t) = \\log\\left(\\frac{\\text{Total de documentos}}{\\text{Documentos que contienen la palabra } t}\\right)\\)$</li> <li>\u00bfPor qu\u00e9 lo usamos?: A diferencia del simple conteo (Bak-of-Words), el TF-IDF penaliza palabras que aparecen en todos lados (como \"el\", \"que\", \"es\") y premia palabras tem\u00e1ticas (\"procesador\", \"inversi\u00f3n\", \"vacuna\").</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#b-k-means-el-cerebro-del-agrupamiento","title":"B. K-Means: El Cerebro del Agrupamiento","text":"<p>Para el Aprendizaje No Supervisado, el algoritmo de K-Means es el est\u00e1ndar de oro para encontrar patrones sin etiquetas previas.</p> <ul> <li>El Proceso:<ol> <li>Define \\(k\\) puntos aleatorios (centroides).</li> <li>Asigna cada documento al centroide m\u00e1s cercano (usando distancia euclidiana en el espacio vectorial).</li> <li>Recalcula el centro del grupo y repite hasta que los grupos se estabilizan.</li> </ol> </li> <li>Por qu\u00e9 lo usamos: Es extremadamente eficiente para grandes vol\u00famenes de datos y nos permite segmentar el mercado, noticias o documentos legales de forma autom\u00e1tica.</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#c-pca-visualizando-el-hiperespacio","title":"C. PCA: Visualizando el Hiperespacio","text":"<p>Nuestra matriz TF-IDF tiene cientos de dimensiones (una por cada palabra \u00fanica). El ser humano solo puede ver en 2D o 3D.</p> <ul> <li>\u00bfQu\u00e9 significa?: Principal Component Analysis \"comprime\" la informaci\u00f3n. Busca las direcciones (componentes) donde los datos var\u00edan m\u00e1s y proyecta todo sobre ellas.</li> <li>Utilidad Did\u00e1ctica: Sin PCA, el clustering ser\u00eda una lista de n\u00fameros abstractos. Con PCA, podemos \"ver\" la separaci\u00f3n de conceptos en una gr\u00e1fica.</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#3-interpretacion-de-lo-que-estamos-viendo","title":"\ud83d\udcca 3. Interpretaci\u00f3n de lo que estamos viendo","text":"<p>Al ejecutar el c\u00f3digo, se genera la visualizaci\u00f3n <code>05_visualizacion_clustering.png</code>.</p> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#como-leer-este-grafico","title":"\u00bfC\u00f3mo leer este gr\u00e1fico?","text":"<ol> <li>Cercan\u00eda es Similitud: Dos puntos que est\u00e1n pegados significan documentos que comparten palabras clave y, por lo tanto, temas.</li> <li>Dispersi\u00f3n de Clusters:<ul> <li>Si los grupos est\u00e1n muy separados, el algoritmo ha tenido \u00e9xito total identificando temas \u00fanicos.</li> <li>Si hay solapamiento, indica que hay documentos que comparten vocabulario de varios temas (ej. un art\u00edculo sobre \"Tecnolog\u00eda en la Salud\").</li> </ul> </li> <li>Los Ejes (PCA): El Eje X (Componente 1) suele capturar la diferencia m\u00e1s grande entre los temas (ej. t\u00e9rminos m\u00e9dicos vs t\u00e9rminos financieros).</li> </ol>"},{"location":"dashboards/05_vectorizacion_y_clustering/#4-guia-didactica-paso-a-paso","title":"\ud83d\udc68\u200d\ud83c\udfeb 4. Gu\u00eda Did\u00e1ctica: Paso a Paso","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#paso-1-generacion-del-corpus","title":"Paso 1: Generaci\u00f3n del Corpus","text":"<p>Creamos 1,200 documentos sint\u00e9ticos. En una formaci\u00f3n real, esto simula la ingesta de datos de una API o una base de datos SQL.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#paso-2-limpieza-y-tokenizacion","title":"Paso 2: Limpieza y Tokenizaci\u00f3n","text":"<p>Aunque el script es directo, en NLP real eliminar\u00edamos puntuaci\u00f3n, convertir\u00edamos a min\u00fasculas y quitar\u00edamos Stop Words (palabras vac\u00edas) para que el modelo no se distraiga con ruido.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#paso-3-entrenamiento-del-modelo","title":"Paso 3: Entrenamiento del Modelo","text":"<p>El comando <code>kmeans.fit(tfidf_matrix)</code> es donde ocurre la \"magia\". El modelo \"aprende\" la estructura latente de los datos sin ayuda humana.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#referencias-y-citas-academicas","title":"\ud83d\udcda Referencias y Citas Acad\u00e9micas","text":"<p>Para profundizar en la metodolog\u00eda, se recomienda la consulta de las siguientes fuentes fundamentales:</p> <ul> <li>McKinney, W. (2012). Python for Data Analysis. O'Reilly Media. (Referencia para manipulaci\u00f3n de matrices).</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR. (Documentaci\u00f3n oficial del framework utilizado).</li> <li>Manning, C. D., et al. (2008). Introduction to Information Retrieval. Cambridge University Press. (Teor\u00eda base de TF-IDF).</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#informacion-institucional","title":"\ud83c\udf93 Informaci\u00f3n Institucional","text":"<p>Autor/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda \u00c1rea: Big Data, Ciencia de Datos &amp; Econometr\u00eda Aplicada.  </p> <p>Hash ID de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code></p>"},{"location":"dashboards/06_analisis_panel_qog/","title":"Modulo 06: Analisis de Datos de Panel (QoG)","text":"<p>Dashboard interactivo con los resultados del pipeline Big Data sobre el Quality of Government Standard Dataset (University of Gothenburg, Enero 2024).</p> <p>Abrir Dashboard Interactivo</p>"},{"location":"dashboards/06_analisis_panel_qog/#contenido","title":"Contenido","text":"<p>El dashboard presenta 5 pestanas interactivas:</p> Pestana Linea de investigacion Paises Asia Central Evolucion institucional post-sovietica KAZ, UZB, TKM, KGZ, TJK Seguridad Hidrica Crisis del Mar de Aral KAZ, UZB, TKM, AFG, IRN Terrorismo Estabilidad politica y fragilidad estatal 13 paises (Europa + Asia Central + Oriente Medio) Maghreb Autoritarismo vs democracia, Primaveras Arabes DZA, MAR, TUN, LBY, EGY, MRT ML: Clusters PCA Clasificacion de 28 paises (K-Means + PCA) 28 paises de todas las lineas"},{"location":"dashboards/06_analisis_panel_qog/#infraestructura","title":"Infraestructura","text":"<ul> <li>Procesamiento: Apache Spark 3.5.4 (cluster Docker: 1 master + 2 workers)</li> <li>Almacenamiento: PostgreSQL 15 + Parquet</li> <li>Clustering: scikit-learn (K-Means k=5 + PCA 2 componentes)</li> <li>Visualizacion: Plotly (graficos interactivos)</li> <li>Dataset: 924 observaciones (28 paises x 33 anios), 40 variables</li> </ul>"},{"location":"dashboards/06_analisis_panel_qog/#variables-principales","title":"Variables principales","text":"Variable Fuente Que mide <code>vdem_polyarchy</code> V-Dem Indice de democracia electoral <code>ti_cpi</code> Transparency International Percepcion de corrupcion <code>wbgi_cce</code> World Bank Control de corrupcion <code>wbgi_rle</code> World Bank Estado de derecho <code>wbgi_pse</code> World Bank Estabilidad politica <code>wdi_gdppc</code> World Bank PIB per capita (USD 2015) <code>undp_hdi</code> UNDP Indice de Desarrollo Humano <code>ffp_fsi</code> Fund for Peace Indice de fragilidad estatal"},{"location":"dashboards/06_analisis_panel_qog/#codigo-fuente","title":"Codigo fuente","text":"<ul> <li>Pipeline ETL + ML: <code>ejercicios/06_an\u00e1lisis_datos_de_panel/</code></li> <li>Material teorico: <code>ejercicios/07_infraestructura_bigdata/</code></li> </ul>"},{"location":"dashboards/EJERCICIO_01_CONTEO/","title":"\ud83e\uddf6 Ejercicio 1: Anatom\u00eda del Texto y el Ritmo de las Palabras \ud83c\udfb8","text":"<p>\"Check out Guitar George, he knows-all the chords... \ud83e\udd18\" Envolvi\u00e9ndote en el ritmo de los Sultans of Swing, aprenderemos a diseccionar el lenguaje para encontrar su melod\u00eda oculta.</p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#el-ritual-de-inicio-objetivos","title":"\ud83c\udfaf El Ritual de Inicio (Objetivos)","text":"<p>Convertir un murmullo de palabras en una sinfon\u00eda de datos. En este primer paso del NLP (Procesamiento de Lenguaje Natural), aprender\u00e1s a:</p> <ol> <li>Exponer la Estructura: Entender que el texto no es solo letras, sino una arquitectura que debe ser unificada (Merge). \u26d3\ufe0f</li> <li>Normalizar la Frecuencia: Doblegar el texto a min\u00fasculas para que el algoritmo no se confunda entre \"\ud83c\udfb8 Solo\" y \"solo\". \u26a1</li> <li>Tokenizar el Caos: Usar expresiones regulares (<code>re.findall</code>) como un bistur\u00ed para separar cada t\u00e9rmino (token) del ruido ambiental. \u2694\ufe0f</li> <li>Contar el Pulso: Usar <code>Counter</code> para medir qu\u00e9 tan fuerte late cada palabra en el corpus. \ud83c\udf0c</li> </ol>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#la-maquina-de-diseccion","title":"\ud83c\udfd7\ufe0f La M\u00e1quina de Disecci\u00f3n","text":"<p>Imagina una trituradora de alta precisi\u00f3n que toma un pergamino antiguo y lo convierte en bloques de datos puros.</p> <p></p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#el-purgatorio-del-procesamiento","title":"El Purgatorio del Procesamiento","text":"<ol> <li>Unificaci\u00f3n: Juntamos todas las frases en un solo bloque de acero textual.</li> <li>Normalizaci\u00f3n: Aplicamos <code>lower()</code> para estandarizar la se\u00f1al. \u26a1</li> <li>Tokenizaci\u00f3n: Extraemos los tokens, eliminando la puntuaci\u00f3n que no aporta al \"riff\" principal.</li> </ol>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#resultados-del-directo-python-output","title":"\ud83c\udfb8 Resultados del Directo (Python Output)","text":"<p>Al ejecutar <code>01_conteo_palabras.py</code>, ver\u00e1s surgir el Top 10 de palabras que dominan el escenario.</p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#el-grafico-de-la-verdad","title":"El Gr\u00e1fico de la Verdad","text":"<p>Aqu\u00ed es donde visualizamos el espectro de frecuencias. \u00bfVes esas barras gigantes? Son las palabras que m\u00e1s se repiten.</p> <p></p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#reflexion-de-backstage","title":"\u2694\ufe0f Reflexi\u00f3n de Backstage","text":"<ul> <li>\u00bfVes el ruido? Art\u00edculos como \"el\", \"la\" o \"de\" suelen dominar el gr\u00e1fico. Son como los acoples de un amplificador: est\u00e1n ah\u00ed, pero no son la melod\u00eda. \ud83c\udfc1</li> <li>Sultans of Swing: Al igual que en un buen solo de Knopfler, cada palabra tiene su lugar, pero algunas (las stopwords) aparecen demasiado y tapan el verdadero mensaje. \ud83c\udfb8</li> </ul> <p>Hash de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Master of Ceremonies: Juan Marcelo Gutierrez Miranda (@TodoEconometria) Vibe: Rock &amp; Data \ud83e\udd18\u26a1</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/","title":"\u26a1 Ejercicio 2: El Filtro de Ruido (Anti-Stopwords) \ud83d\udc80","text":"<p>\"You hear the guitar, it's a-clean and it's a-pure...\" \ud83c\udfb8 Como pasar de una distorsi\u00f3n ca\u00f3tica a un solo cristalino, en este ejercicio filtraremos el ruido del lenguaje para que brille la esencia.</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-desafio-limpiar-la-mezcla","title":"\ud83c\udfaf El Desaf\u00edo: Limpiar la Mezcla","text":"<p>En el ejercicio anterior vimos que las palabras m\u00e1s comunes son \"basura sem\u00e1ntica\" (stopwords). Tu misi\u00f3n es eliminarlas para que las palabras con peso real salgan a la superficie.</p> <ol> <li>Stopword Filtering: Activar el pedal de filtro para ignorar palabras como \"el\", \"es\", \"y\". \u26d3\ufe0f</li> <li>Impacto Visual: Comparar el \"Antes\" y el \"Despu\u00e9s\" para ver c\u00f3mo emerge el verdadero significado. \u26a1</li> <li>An\u00e1lisis de Sentimiento Primitivo: Al limpiar el ruido, palabras como \"fant\u00e1stico\" o \"terrible\" toman el protagonismo. \ud83e\udd18</li> </ol>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-pedal-de-filtro-stopword-filter","title":"\ud83c\udfd7\ufe0f El Pedal de Filtro (Stopword Filter)","text":"<p>Imagina que cada palabra com\u00fan es un acopio de est\u00e1tica. Nuestro algoritmo act\u00faa como un pedal de noise gate que solo deja pasar las frecuencias de alto impacto.</p> <p></p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-ritual-de-limpieza","title":"El Ritual de Limpieza","text":"<ul> <li>Input: Un texto sucio lleno de art\u00edculos y preposiciones.</li> <li>Filtro: Una lista negra de palabras prohibidas (<code>stopwords_es</code>). \u2694\ufe0f</li> <li>Output: Una se\u00f1al pura donde cada palabra cuenta una historia.</li> </ul>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#comparativa-del-escenario-python-data","title":"\ud83c\udfb8 Comparativa del Escenario (Python Data)","text":"<p>Al ejecutar <code>02_limpieza_texto.py</code>, ver\u00e1s el contraste brutal entre los dos mundos.</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-antes-vs-el-despues","title":"El Antes vs El Despu\u00e9s","text":"<p>Observa c\u00f3mo en el gr\u00e1fico izquierdo predominan las palabras vac\u00edas, mientras que en el derecho aparece el sentimiento puro.</p> <p></p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#profecia-para-el-analista","title":"\ud83c\udf11 Profec\u00eda para el Analista","text":"<ul> <li>El Silencio es Poder: Al eliminar el 70% de las palabras que no sirven, el an\u00e1lisis se vuelve 100% m\u00e1s preciso. \ud83c\udf0c</li> <li>Sultans of Data: Ahora que has limpiado la pista, estamos listos para el siguiente nivel: el an\u00e1lisis de sentimiento y la similitud. \ud83e\udd18</li> </ul> <p>Hash de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Master of Ceremonies: Juan Marcelo Gutierrez Miranda (@TodoEconometria) Vibe: Heavy Clean Sound \ud83c\udfb8\u26a1</p>"},{"location":"dashboards/arima-sarima-pro/","title":"Dashboard PRO: Series Temporales ARIMA/SARIMA","text":"<p>Dashboard interactivo con dise\u00f1o profesional inspirado en terminales financieras tipo Bloomberg, para el analisis completo de series temporales siguiendo la metodologia Box-Jenkins.</p> <p>Abrir Dashboard PRO</p>"},{"location":"dashboards/arima-sarima-pro/#caracteristicas-del-dashboard","title":"Caracteristicas del Dashboard","text":"Elemento Descripcion Tema Oscuro tipo terminal financiera (inspirado Bloomberg/OECD Explorer) KPIs Cards con metricas en tiempo real: RMSE, MAE, MAPE, R2, AIC Pestanas 7 secciones interactivas con transiciones suaves Responsivo Adaptable a diferentes tamanos de pantalla"},{"location":"dashboards/arima-sarima-pro/#contenido-por-pestana","title":"Contenido por Pestana","text":"# Pestana Descripcion 1 Serie Original Pasajeros aereos 1949-1960 con media movil 12M 2 Descomposicion Tendencia + Estacionalidad + Residuo (multiplicativa) 3 ACF / PACF Autocorrelacion de la serie diferenciada (d=1, D=1, s=12) 4 Diagnostico Residuos, histograma, ACF residual, Q-Q plot 5 Pronostico Serie + ajuste SARIMA + forecast 24 meses + IC 95% 6 Metricas Radar Visualizacion polar de las metricas normalizadas 7 Comparativa Comparacion de diferentes ordenes SARIMA"},{"location":"dashboards/arima-sarima-pro/#inspiracion-de-diseno","title":"Inspiracion de Diseno","text":"<p>Este dashboard fue creado siguiendo las mejores practicas de visualizacion financiera:</p> <ul> <li>OECD Pension Explorer - Plotly App oficial de la OECD</li> <li>Portfolio Optimizer - Panel/HoloViz Gallery</li> <li>Dash Bootstrap Templates - Temas profesionales para Dash</li> </ul>"},{"location":"dashboards/arima-sarima-pro/#metodologia-box-jenkins","title":"Metodologia Box-Jenkins","text":"<ol> <li>Identificacion: Analisis ACF/PACF para determinar ordenes (p, d, q)(P, D, Q)[s]</li> <li>Estimacion: Ajuste por maxima verosimilitud con SARIMAX</li> <li>Diagnostico: Tests de Ljung-Box, Jarque-Bera, Q-Q plot</li> <li>Pronostico: Forecast con intervalos de confianza al 95%</li> </ol>"},{"location":"dashboards/arima-sarima-pro/#codigo-fuente","title":"Codigo Fuente","text":"<ul> <li>Script de ejercicio: <code>ejercicios/04_machine_learning/07_series_temporales_arima/</code></li> <li>Exportador dashboard: <code>.profesor/soluciones/TRABAJO_FINAL/export_arima_pro.py</code></li> <li>Guia teorica: Series Temporales ARIMA</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Referencias academicas:</p> <ul> <li>Box, G.E.P. &amp; Jenkins, G.M. (1976). Time Series Analysis: Forecasting and Control. Holden-Day.</li> <li>Hyndman, R.J. &amp; Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3<sup>rd</sup> ed.). OTexts.</li> <li>Hamilton, J.D. (1994). Time Series Analysis. Princeton University Press.</li> </ul>"},{"location":"dashboards/bigdata-ventas/","title":"Big Data y Analisis de Ventas","text":"<p>Dashboards construidos con D3.js que demuestran visualizacion de datos de ventas a gran escala. Estos ejercicios conectan el procesamiento de Big Data con la presentacion ejecutiva de resultados.</p>"},{"location":"dashboards/bigdata-ventas/#dashboards-interactivos","title":"Dashboards interactivos","text":""},{"location":"dashboards/bigdata-ventas/#centro-de-comando-big-data-sales","title":"Centro de Comando: Big Data Sales","text":"<p>Dashboard ejecutivo tipo \"command center\" con KPIs de ventas, tendencias temporales, distribucion por categorias y metricas de rendimiento. Dise\u00f1ado para escenarios de monitoreo en tiempo real sobre datos particionados.</p> <p>Abrir Dashboard Big Data (interactivo)</p> <p>Tecnologias: D3.js, JavaScript, CSS Grid</p>"},{"location":"dashboards/bigdata-ventas/#visualizacion-d3js-scatter-plot-interactivo","title":"Visualizacion D3.js: Scatter Plot Interactivo","text":"<p>Grafico de dispersion interactivo construido desde cero con D3.js. Demuestra bindeo de datos, escalas, ejes, tooltips y transiciones animadas sin depender de librerias de alto nivel.</p> <p>Abrir Visualizacion D3.js (interactivo)</p> <p>Tecnologias: D3.js puro, SVG, JavaScript</p>"},{"location":"dashboards/bigdata-ventas/#contexto-academico","title":"Contexto academico","text":"<p>El analisis de datos de ventas es uno de los casos de uso mas frecuentes en Big Data:</p> <ul> <li>Volumen: millones de transacciones diarias requieren procesamiento distribuido</li> <li>Velocidad: dashboards en tiempo real para toma de decisiones operativas</li> <li>Variedad: datos estructurados (transacciones) combinados con semi-estructurados (logs, clickstream)</li> <li>Valor: traduccion de patrones en acciones comerciales concretas</li> </ul>"},{"location":"dashboards/bigdata-ventas/#por-que-d3js","title":"Por que D3.js?","text":"<p>D3.js (Data-Driven Documents) es la libreria de referencia para visualizaciones web personalizadas:</p> <ul> <li>Control total sobre cada elemento visual (SVG, Canvas)</li> <li>Bindeo directo entre datos y elementos del DOM</li> <li>Transiciones y animaciones declarativas</li> <li>Escalabilidad para grandes volumenes de puntos de datos</li> <li>Estandar de la industria en periodismo de datos y dashboards ejecutivos</li> </ul> <p>Estos dashboards ilustran como presentar resultados de pipelines de Big Data en formatos comprensibles para audiencias no tecnicas.</p>"},{"location":"dashboards/bigdata-ventas/#codigo-fuente","title":"Codigo fuente","text":"<p>Los scripts y archivos HTML fueron generados como parte del modulo de procesamiento distribuido. Utilizan datos sinteticos particionados (formato Parquet/CSV) procesados con Python y visualizados con D3.js.</p>"},{"location":"dashboards/ejercicios-python/","title":"Ejercicios Python: Limpieza y Analisis","text":"<p>Notebooks exportados como HTML interactivo que cubren el flujo completo de trabajo con datos: desde la verificacion del entorno hasta el analisis descriptivo con tablas y graficos.</p>"},{"location":"dashboards/ejercicios-python/#ejercicios-interactivos","title":"Ejercicios interactivos","text":""},{"location":"dashboards/ejercicios-python/#00-test-de-visualizacion","title":"00 - Test de Visualizacion","text":"<p>Verificacion del entorno de trabajo: instalacion correcta de librerias, generacion de graficos basicos con Matplotlib y Plotly, y validacion de la cadena completa desde datos hasta visualizacion.</p> <p>Abrir Test de Visualizacion (interactivo)</p> <p>Contenido: Setup del entorno, Matplotlib basico, Plotly basico, verificacion de dependencias</p>"},{"location":"dashboards/ejercicios-python/#02-limpieza-y-preparacion-de-datos","title":"02 - Limpieza y Preparacion de Datos","text":"<p>Ejercicio guiado sobre las tecnicas fundamentales de limpieza de datos: deteccion y tratamiento de valores nulos, eliminacion de duplicados, correccion de tipos de datos, manejo de outliers y normalizacion de columnas.</p> <p>Abrir Limpieza y Preparacion (interactivo)</p> <p>Contenido: Valores nulos, duplicados, tipos de datos, outliers, transformaciones</p>"},{"location":"dashboards/ejercicios-python/#03-tablas-y-analisis-descriptivo","title":"03 - Tablas y Analisis Descriptivo","text":"<p>Construccion de tablas resumen y analisis estadistico descriptivo: medidas de tendencia central, dispersion, tablas de frecuencia, tablas cruzadas (crosstabs) y agregaciones con groupby.</p> <p>Abrir Tablas y Analisis Descriptivo (interactivo)</p> <p>Contenido: Estadisticas descriptivas, groupby, crosstabs, tablas de frecuencia, pivot tables</p>"},{"location":"dashboards/ejercicios-python/#contexto-academico","title":"Contexto academico","text":"<p>Estos ejercicios cubren las etapas iniciales del ciclo de vida de los datos, que en la practica profesional consumen entre el 60% y el 80% del tiempo de un proyecto:</p> Etapa Descripcion Ejercicio Setup Verificar que el entorno funciona correctamente 00 - Test Limpieza Datos crudos a datos utilizables 02 - Limpieza Exploracion Entender la estructura y distribucion de los datos 03 - Descriptivo"},{"location":"dashboards/ejercicios-python/#flujo-de-trabajo-recomendado","title":"Flujo de trabajo recomendado","text":"<ol> <li>Verificar el entorno (Exercise 00)</li> <li>Cargar datos crudos y realizar limpieza sistematica (Exercise 02)</li> <li>Explorar con estadisticas descriptivas y tablas (Exercise 03)</li> <li>Visualizar patrones y anomalias (conecta con modulos de ML y NLP)</li> </ol>"},{"location":"dashboards/ejercicios-python/#tecnologias-utilizadas","title":"Tecnologias utilizadas","text":"<ul> <li>Pandas: manipulacion y analisis de datos tabulares</li> <li>NumPy: operaciones numericas y manejo de arrays</li> <li>Matplotlib / Plotly: visualizacion estatica e interactiva</li> <li>Jupyter Notebook: entorno interactivo exportado a HTML</li> </ul>"},{"location":"dashboards/ejercicios-python/#codigo-fuente","title":"Codigo fuente","text":"<p>Estos notebooks fueron desarrollados como ejercicios guiados del curso. Los archivos originales (.ipynb) estan disponibles en la estructura de ejercicios del repositorio.</p>"},{"location":"dashboards/epidemiologia/","title":"Epidemiologia y Salud Publica","text":"<p>Dashboards interactivos que aplican tecnicas de visualizacion y analisis de datos al dominio de la epidemiologia. Estos ejercicios demuestran como transformar datos de salud publica en herramientas de decision visual.</p>"},{"location":"dashboards/epidemiologia/#dashboards-interactivos","title":"Dashboards interactivos","text":""},{"location":"dashboards/epidemiologia/#dashboard-epidemiologico-completo","title":"Dashboard Epidemiologico Completo","text":"<p>Tablero de mando con multiples vistas sobre indicadores epidemiologicos: tasas de incidencia, distribucion geografica, tendencias temporales y correlaciones entre variables de salud.</p> <p>Abrir Dashboard Epidemiologico (interactivo)</p> <p>Tecnologias: Plotly, D3.js, HTML/CSS/JS</p>"},{"location":"dashboards/epidemiologia/#matriz-de-correlacion-epidemiologica","title":"Matriz de Correlacion Epidemiologica","text":"<p>Heatmap interactivo que muestra las correlaciones entre variables epidemiologicas. Permite identificar patrones de asociacion entre indicadores de salud a simple vista.</p> <p>Abrir Matriz de Correlacion (interactivo)</p> <p>Tecnologias: D3.js, HTML/CSS/JS</p>"},{"location":"dashboards/epidemiologia/#visualizacion-de-datos-epidemiologicos","title":"Visualizacion de Datos Epidemiologicos","text":"<p>Ejercicio completo de visualizacion aplicada a datos de salud: graficos de barras, lineas temporales, diagramas de dispersion y distribuciones. Incluye analisis exploratorio visual con anotaciones.</p> <p>Abrir Visualizacion Epidemiologica (interactivo)</p> <p>Tecnologias: Plotly, Pandas, HTML exportado</p>"},{"location":"dashboards/epidemiologia/#contexto-academico","title":"Contexto academico","text":"<p>La epidemiologia cuantitativa requiere herramientas de visualizacion que permitan:</p> <ul> <li>Vigilancia epidemiologica: monitoreo en tiempo real de indicadores clave</li> <li>Deteccion de brotes: identificacion visual de anomalias en series temporales</li> <li>Analisis espacial: distribucion geografica de enfermedades y factores de riesgo</li> <li>Comunicacion de riesgo: traduccion de datos complejos en visualizaciones comprensibles para tomadores de decisiones</li> </ul> <p>Estos dashboards ilustran como Python y las librerias de visualizacion modernas (Plotly, D3.js) pueden servir como puente entre el analisis estadistico y la toma de decisiones en salud publica.</p>"},{"location":"dashboards/epidemiologia/#codigo-fuente","title":"Codigo fuente","text":"<p>Los scripts que generan estas visualizaciones se encuentran en el repositorio del proyecto. Cada dashboard fue construido con datos sinteticos o abiertos para fines educativos.</p>"},{"location":"dashboards/flores-transfer-learning/","title":"Dashboard: Clasificaci\u00f3n de Flores con Transfer Learning","text":""},{"location":"dashboards/flores-transfer-learning/#descripcion","title":"Descripci\u00f3n","text":"<p>Pipeline de Computer Vision que clasifica im\u00e1genes de flores usando Transfer Learning con MobileNetV2.</p> <p>\u00bfQu\u00e9 es Transfer Learning? En lugar de entrenar una red neuronal desde cero (necesitar\u00edamos millones de im\u00e1genes), usamos una red ya entrenada en ImageNet y la adaptamos a nuestro problema.</p>"},{"location":"dashboards/flores-transfer-learning/#pipeline","title":"Pipeline","text":"<pre><code>1. DESCARGA          2. EMBEDDINGS         3. CLASIFICACI\u00d3N      4. VISUALIZACI\u00d3N\n   3,670 flores         MobileNetV2           ML tradicional        Dashboard\n   5 clases             1280 features         KNN/SVM/RF            Plotly\n</code></pre>"},{"location":"dashboards/flores-transfer-learning/#resultados","title":"Resultados","text":"Modelo Accuracy SVM 89.9% Random Forest 86.5% KNN 86.2%"},{"location":"dashboards/flores-transfer-learning/#visualizaciones","title":"Visualizaciones","text":"<p>El dashboard incluye 4 pesta\u00f1as interactivas:</p> <ol> <li>t-SNE: Proyecci\u00f3n 2D de los embeddings - flores similares aparecen juntas</li> <li>Comparativa: Barras con accuracy de cada modelo</li> <li>Confusion Matrix: Aciertos/errores por clase (porcentajes)</li> <li>Distribuci\u00f3n: Radar chart del dataset</li> </ol>"},{"location":"dashboards/flores-transfer-learning/#ver-dashboard","title":"Ver Dashboard","text":"Abrir Dashboard Interactivo"},{"location":"dashboards/flores-transfer-learning/#ejecutar-el-ejercicio","title":"Ejecutar el Ejercicio","text":"<pre><code>cd ejercicios/04_machine_learning/flores_transfer_learning/\npip install -r requirements.txt\npython 01_flores_transfer_learning.py\n</code></pre> <p>Requisitos: TensorFlow (GPU recomendado pero funciona en CPU)</p> <p>Curso: Big Data con Python - De Cero a Producci\u00f3n Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Referencias acad\u00e9micas:</p> <ul> <li>Sandler, M., et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR.</li> <li>Yosinski, J., et al. (2014). How transferable are features in deep neural networks? NeurIPS.</li> <li>van der Maaten, L. &amp; Hinton, G. (2008). Visualizing Data using t-SNE. JMLR.</li> </ul>"},{"location":"ejercicios/","title":"Ejercicios","text":"<p>Lista completa de todos los ejercicios disponibles en el curso.</p>"},{"location":"ejercicios/#roadmap-de-ejercicios","title":"Roadmap de Ejercicios","text":""},{"location":"ejercicios/#modulo-1-bases-de-datos","title":"Modulo 1: Bases de Datos","text":"# Ejercicio Tecnologia Nivel Estado 1.1 Introduccion SQLite SQLite + Pandas Basico Disponible 2.1 PostgreSQL HR PostgreSQL Intermedio Disponible 2.2 PostgreSQL Jardineria PostgreSQL Intermedio Disponible 2.3 Migracion SQLite a PostgreSQL PostgreSQL + Python Intermedio Disponible 3.1 Oracle HR Oracle Database Avanzado Disponible 5.1 Analisis Excel/Python Pandas + Excel Basico Disponible"},{"location":"ejercicios/#modulo-2-big-data-y-etl","title":"Modulo 2: Big Data y ETL","text":"# Ejercicio Tecnologia Nivel Estado 02 Pipeline ETL QoG PostgreSQL + Pandas Avanzado Disponible 03 Procesamiento Distribuido Dask + Parquet Intermedio Disponible"},{"location":"ejercicios/#modulo-3-analitica-avanzada","title":"Modulo 3: Analitica Avanzada","text":"# Ejercicio Tecnologia Nivel Estado 04 Machine Learning Scikit-Learn, PCA, K-Means Avanzado Disponible 04.2 Transfer Learning Flores TensorFlow, MobileNetV2 Avanzado Disponible ARIMA Series Temporales ARIMA/SARIMA, Box-Jenkins Avanzado Disponible 05 NLP y Text Mining NLTK, TF-IDF, Jaccard Avanzado Disponible"},{"location":"ejercicios/#modulo-4-econometria-de-panel","title":"Modulo 4: Econometria de Panel","text":"# Ejercicio Tecnologia Nivel Estado 06 Analisis de Datos de Panel linearmodels, Panel OLS, Altair Avanzado Disponible"},{"location":"ejercicios/#trabajo-final","title":"Trabajo Final","text":"# Ejercicio Tecnologia Nivel Estado TF Proyecto Final Integrador Docker + Spark + PostgreSQL + QoG Avanzado Disponible"},{"location":"ejercicios/#modulo-1-bases-de-datos_1","title":"MODULO 1: Bases de Datos","text":""},{"location":"ejercicios/#ejercicio-11-introduccion-a-sqlite","title":"Ejercicio 1.1: Introduccion a SQLite","text":"<p>Detalles</p> <ul> <li>Nivel: Basico</li> <li>Dataset: NYC Taxi (muestra 10MB)</li> <li>Tecnologias: SQLite, Pandas</li> </ul> <p>Que aprenderas:</p> <ul> <li>Cargar datos CSV a base de datos SQLite</li> <li>Queries SQL basicas (SELECT, WHERE, GROUP BY)</li> <li>Optimizacion con indices</li> <li>Exportar resultados a CSV</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-21-postgresql-con-bd-hr","title":"Ejercicio 2.1: PostgreSQL con BD HR","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Base de Datos: HR (Human Resources) de Oracle</li> <li>Tecnologias: PostgreSQL, SQL</li> </ul> <p>Que aprenderas:</p> <ul> <li>Instalar y configurar PostgreSQL</li> <li>Cargar bases de datos desde scripts SQL</li> <li>Consultas complejas con multiples JOINs</li> <li>Funciones especificas de PostgreSQL</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-22-postgresql-jardineria","title":"Ejercicio 2.2: PostgreSQL Jardineria","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Base de Datos: Sistema de ventas de jardineria</li> <li>Tecnologias: PostgreSQL, Window Functions</li> </ul> <p>Que aprenderas:</p> <ul> <li>Analisis de ventas con SQL</li> <li>Agregaciones complejas (GROUP BY, HAVING)</li> <li>Window Functions para rankings</li> <li>Vistas materializadas</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-23-migracion-sqlite-a-postgresql","title":"Ejercicio 2.3: Migracion SQLite a PostgreSQL","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Tecnologias: SQLite, PostgreSQL, Python</li> </ul> <p>Que aprenderas:</p> <ul> <li>Diferencias entre motores de BD</li> <li>Migrar esquemas y datos</li> <li>Adaptar tipos de datos</li> <li>Validar integridad</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-31-oracle-con-bd-hr","title":"Ejercicio 3.1: Oracle con BD HR","text":"<p>Avanzado</p> <ul> <li>Nivel: Avanzado</li> <li>Base de Datos: HR en Oracle nativo</li> <li>Tecnologias: Oracle Database, PL/SQL</li> </ul> <p>Que aprenderas:</p> <ul> <li>Instalar Oracle Database XE</li> <li>Sintaxis especifica de Oracle</li> <li>PL/SQL (procedimientos, funciones)</li> <li>Secuencias y triggers</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-51-analisis-excelpython","title":"Ejercicio 5.1: Analisis Excel/Python","text":"<p>Detalles</p> <ul> <li>Nivel: Basico-Intermedio</li> <li>Tecnologias: Python, Pandas, Excel</li> </ul> <p>Que aprenderas:</p> <ul> <li>Leer archivos Excel con Python</li> <li>Analisis exploratorio de datos (EDA)</li> <li>Visualizaciones con matplotlib/seaborn</li> <li>Automatizar analisis</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-2-big-data-y-etl_1","title":"MODULO 2: Big Data y ETL","text":""},{"location":"ejercicios/#pipeline-etl-profesional-quality-of-government","title":"Pipeline ETL Profesional - Quality of Government","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Dataset: QoG (1289 variables, 194+ paises)</li> <li>Tecnologias: PostgreSQL, Pandas, psycopg2</li> </ul> <p>Que aprenderas:</p> <ul> <li>Disenar arquitectura ETL modular</li> <li>Trabajar con PostgreSQL para analisis longitudinal</li> <li>Limpiar datasets complejos (&gt;1000 variables)</li> <li>Preparar datos de panel para econometria</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#procesamiento-distribuido-con-dask","title":"Procesamiento Distribuido con Dask","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Tecnologias: Dask, Parquet, LocalCluster</li> </ul> <p>Que aprenderas:</p> <ul> <li>Configurar un Cluster Local con Dask</li> <li>Leer archivos Parquet de forma particionada</li> <li>Ejecutar agregaciones complejas en paralelo</li> <li>Comparar rendimiento vs Pandas</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-3-analitica-avanzada_1","title":"MODULO 3: Analitica Avanzada","text":""},{"location":"ejercicios/#machine-learning-en-big-data","title":"Machine Learning en Big Data","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: Scikit-Learn, PCA, K-Means</li> <li>Scripts: PCA Iris, FactoMineR, Breast Cancer, Wine, TF-IDF</li> </ul> <p>Que aprenderas:</p> <ul> <li>Reduccion de dimensionalidad con PCA</li> <li>Clustering con K-Means y Hierarchical Clustering</li> <li>Interpretacion de componentes principales</li> <li>Perfilado de clusters</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#transfer-learning-clasificacion-de-flores","title":"Transfer Learning: Clasificacion de Flores","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: TensorFlow, MobileNetV2, Scikit-Learn</li> <li>Dataset: TensorFlow Flowers (3,670 imagenes, 5 clases)</li> </ul> <p>Que aprenderas:</p> <ul> <li>Transfer Learning con redes pre-entrenadas (ImageNet)</li> <li>Extraccion de embeddings con CNNs</li> <li>Clasificacion de imagenes con ML tradicional (KNN, SVM, Random Forest)</li> <li>Visualizacion t-SNE de espacios de alta dimension</li> </ul> <p>Ver Dashboard Interactivo</p>"},{"location":"ejercicios/#series-temporales-arimasarima","title":"Series Temporales: ARIMA/SARIMA","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Dataset: AirPassengers (144 observaciones, 1949-1960)</li> <li>Tecnologias: statsmodels, Metodologia Box-Jenkins</li> </ul> <p>Que aprenderas:</p> <ul> <li>Metodologia Box-Jenkins completa (Identificacion, Estimacion, Diagnostico, Pronostico)</li> <li>Modelos ARIMA y SARIMA con estacionalidad</li> <li>ACF/PACF para identificacion de ordenes</li> <li>Diagnostico de residuos y pronosticos</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#nlp-y-text-mining","title":"NLP y Text Mining","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: NLTK, TF-IDF, Jaccard, Sentiment Analysis</li> <li>Scripts: Conteo, Limpieza, Sentimiento, Similitud</li> </ul> <p>Que aprenderas:</p> <ul> <li>Tokenizacion y limpieza de texto</li> <li>Eliminacion de stopwords</li> <li>Similitud de Jaccard entre documentos</li> <li>Analisis de sentimiento por lexicon</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-4-econometria-de-panel_1","title":"MODULO 4: Econometria de Panel","text":""},{"location":"ejercicios/#analisis-de-datos-de-panel","title":"Analisis de Datos de Panel","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Datasets: Guns (leyes de armas), Fatalities (mortalidad trafico)</li> <li>Tecnologias: linearmodels, Panel OLS, Altair</li> </ul> <p>Que aprenderas:</p> <ul> <li>Datos de panel: estructura pais x anio</li> <li>Efectos Fijos vs Efectos Aleatorios</li> <li>Two-Way Fixed Effects</li> <li>Test de Hausman para seleccion de modelo</li> <li>Odds Ratios y Efectos Marginales</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#trabajo-final_1","title":"TRABAJO FINAL","text":""},{"location":"ejercicios/#proyecto-final-pipeline-de-big-data-con-docker","title":"Proyecto Final: Pipeline de Big Data con Docker","text":"<p>Proyecto Integrador</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: Docker, Apache Spark, PostgreSQL, QoG</li> <li>Evaluacion: Infraestructura 30% + ETL 25% + Analisis 25% + Reflexion IA 20%</li> </ul> <p>Que haras:</p> <ul> <li>Construir infraestructura Docker (Spark + PostgreSQL)</li> <li>Disenar y ejecutar un pipeline ETL con Apache Spark</li> <li>Analizar datos QoG con pregunta de investigacion propia</li> <li>Documentar tu proceso de aprendizaje con IA</li> </ul> <p>Ver Enunciado Completo</p>"},{"location":"ejercicios/#datasets-utilizados","title":"Datasets Utilizados","text":""},{"location":"ejercicios/#nyc-taxi-limousine-commission-tlc","title":"NYC Taxi &amp; Limousine Commission (TLC)","text":"<ul> <li>Fuente: NYC Open Data</li> <li>Periodo: 2021</li> <li>Registros: 10M+ viajes</li> </ul>"},{"location":"ejercicios/#quality-of-government-qog","title":"Quality of Government (QoG)","text":"<ul> <li>Fuente: Universidad de Gotemburgo</li> <li>Variables: 1289 indicadores de calidad institucional</li> <li>Paises: 194+ con datos desde 1946</li> </ul>"},{"location":"ejercicios/#airpassengers","title":"AirPassengers","text":"<ul> <li>Fuente: Box &amp; Jenkins (1976)</li> <li>Periodo: 1949-1960 (144 observaciones mensuales)</li> <li>Uso: Series temporales ARIMA/SARIMA</li> </ul>"},{"location":"ejercicios/#como-trabajar-los-ejercicios","title":"Como Trabajar los Ejercicios","text":""},{"location":"ejercicios/#flujo-recomendado","title":"Flujo Recomendado","text":"<ol> <li>Leer el enunciado completo - No empieces a codear sin leer todo</li> <li>Entender los objetivos - Que se espera que logres?</li> <li>Crear rama de trabajo - <code>git checkout -b tu-apellido-ejercicio-XX</code></li> <li>Trabajar en pasos pequenos - No intentes hacerlo todo de una vez</li> <li>Probar frecuentemente - Ejecuta tu codigo cada vez que completes una parte</li> <li>Hacer commits regulares - Guarda tu progreso frecuentemente</li> <li>Subir con git push - Cuando completes, el sistema evalua tu PROMPTS.md</li> </ol>"},{"location":"ejercicios/#proximos-pasos","title":"Proximos Pasos","text":"<p>Empieza con el primer ejercicio:</p> <p>Ejercicio 01: Introduccion SQLite</p> <p>O salta al proyecto final:</p> <p>Trabajo Final: Pipeline Big Data</p>"},{"location":"ejercicios/01-introduccion-sqlite/","title":"Ejercicio 01: Introduccion a SQLite","text":"<p>Aprende a cargar y consultar datos usando SQLite y Pandas.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#informacion-general","title":"Informacion General","text":"Campo Valor Nivel \ud83d\udfe2 Basico Tiempo estimado 2-3 horas Tecnologias Python, SQLite, Pandas Dataset NYC Taxi (muestra 10MB) Prerequisitos Python basico, conocimientos de SQL basico"},{"location":"ejercicios/01-introduccion-sqlite/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio, seras capaz de:</p> <ul> <li> Cargar archivos CSV grandes a SQLite usando chunks</li> <li> Crear y gestionar bases de datos SQLite</li> <li> Ejecutar queries SQL basicas y avanzadas</li> <li> Optimizar rendimiento con indices</li> <li> Exportar resultados de queries a CSV</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#el-problema","title":"El Problema","text":"<p>Tienes un archivo CSV con 100,000 registros de viajes de taxi de NYC. Si intentas cargarlo todo en memoria con Pandas, tu computadora puede quedarse sin memoria.</p> <p>Tu mision: Cargar estos datos a una base de datos SQLite de forma eficiente y realizar analisis.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#dataset","title":"Dataset","text":""},{"location":"ejercicios/01-introduccion-sqlite/#nyc-taxi-trip-records","title":"NYC Taxi Trip Records","text":"<ul> <li>Archivo: <code>datos/muestra_taxi.csv</code></li> <li>Tamano: ~10 MB</li> <li>Registros: ~100,000 viajes</li> <li>Periodo: Enero 2021</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#estructura-de-datos","title":"Estructura de Datos","text":"<pre><code>Columnas:\n- tpep_pickup_datetime    # Fecha/hora de inicio del viaje\n- tpep_dropoff_datetime   # Fecha/hora de fin del viaje\n- passenger_count         # Numero de pasajeros\n- trip_distance           # Distancia en millas\n- pickup_longitude        # Longitud de origen\n- pickup_latitude         # Latitud de origen\n- dropoff_longitude       # Longitud de destino\n- dropoff_latitude        # Latitud de destino\n- payment_type            # Tipo de pago (1=Credit, 2=Cash, ...)\n- fare_amount             # Tarifa base\n- tip_amount              # Propina\n- total_amount            # Total pagado\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#tareas","title":"Tareas","text":""},{"location":"ejercicios/01-introduccion-sqlite/#tarea-1-cargar-csv-a-sqlite-en-chunks","title":"Tarea 1: Cargar CSV a SQLite en Chunks","text":"<p>Objetivo: Cargar el CSV completo a SQLite sin quedarte sin memoria.</p> <p>Requisitos:</p> <ul> <li>Usar <code>pandas.read_csv()</code> con parametro <code>chunksize</code></li> <li>Procesar el CSV en chunks de 10,000 registros</li> <li>Insertar cada chunk en la tabla <code>trips</code></li> <li>Mostrar progreso de carga</li> </ul> <p>Ejemplo de codigo inicial:</p> <pre><code>import sqlite3\nimport pandas as pd\n\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"\n    Carga un CSV grande a SQLite en chunks\n\n    Args:\n        csv_path: Ruta al archivo CSV\n        db_path: Ruta a la base de datos SQLite\n        chunksize: Numero de registros por chunk\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # TODO: Implementar carga por chunks\n    # Pista: usa pd.read_csv con chunksize\n    # y itera sobre los chunks\n\n    conn.close()\n</code></pre> <p>Pista</p> <pre><code>chunks = pd.read_csv(csv_path, chunksize=chunksize)\nfor i, chunk in enumerate(chunks):\n    # Insertar chunk en SQLite\n    chunk.to_sql('trips', conn, if_exists='append', index=False)\n    print(f\"Chunk {i+1} cargado\")\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#tarea-2-crear-indices","title":"Tarea 2: Crear Indices","text":"<p>Objetivo: Optimizar queries agregando indices a columnas frecuentemente consultadas.</p> <p>Requisitos:</p> <ul> <li>Crear indice en <code>tpep_pickup_datetime</code></li> <li>Crear indice en <code>payment_type</code></li> <li>Medir tiempo de query antes y despues de indices</li> </ul> <p>Ejemplo:</p> <pre><code>def crear_indices(db_path):\n    \"\"\"\n    Crea indices para optimizar queries\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # TODO: Crear indices\n    # Ejemplo:\n    # cursor.execute(\"CREATE INDEX idx_pickup ON trips(tpep_pickup_datetime)\")\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#tarea-3-queries-de-analisis","title":"Tarea 3: Queries de Analisis","text":"<p>Objetivo: Extraer insights de los datos usando SQL.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#query-1-ingresos-promedio-por-hora-del-dia","title":"Query 1: Ingresos Promedio por Hora del Dia","text":"<p>Calcular el ingreso promedio para cada hora del dia.</p> <p>SQL esperado:</p> <pre><code>SELECT\n    strftime('%H', tpep_pickup_datetime) as hora,\n    AVG(total_amount) as promedio_ingreso,\n    COUNT(*) as num_viajes\nFROM trips\nGROUP BY hora\nORDER BY hora\n</code></pre> <p>Resultado esperado:</p> <pre><code>hora  promedio_ingreso  num_viajes\n00    15.23             2340\n01    14.89             1982\n02    16.45             1657\n...\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#query-2-distribucion-de-metodos-de-pago","title":"Query 2: Distribucion de Metodos de Pago","text":"<p>Calcular el porcentaje de cada metodo de pago.</p> <pre><code>SELECT\n    payment_type,\n    COUNT(*) as total,\n    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM trips), 2) as porcentaje\nFROM trips\nGROUP BY payment_type\nORDER BY total DESC\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#query-3-top-10-rutas-mas-rentables","title":"Query 3: Top 10 Rutas Mas Rentables","text":"<p>Encontrar las rutas (pickup \u2192 dropoff) con mayor ingreso promedio.</p> <p>Desafio</p> <p>Esta query requiere agrupar por coordenadas redondeadas. Piensa como hacerlo.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#tarea-4-exportar-resultados","title":"Tarea 4: Exportar Resultados","text":"<p>Objetivo: Guardar los resultados de tus analisis en archivos CSV.</p> <pre><code>def exportar_resultados(db_path, query, output_path):\n    \"\"\"\n    Ejecuta una query y exporta a CSV\n\n    Args:\n        db_path: Ruta a la BD SQLite\n        query: Query SQL a ejecutar\n        output_path: Ruta del CSV de salida\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # TODO: Ejecutar query y exportar\n    # Pista: usa pd.read_sql_query() y df.to_csv()\n\n    conn.close()\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#criterios-de-evaluacion","title":"Criterios de Evaluacion","text":""},{"location":"ejercicios/01-introduccion-sqlite/#funcionalidad-40-puntos","title":"Funcionalidad (40 puntos)","text":"<ul> <li> Carga completa de datos sin errores (10 pts)</li> <li> Indices creados correctamente (10 pts)</li> <li> Queries ejecutan y devuelven resultados correctos (15 pts)</li> <li> Exportacion funciona (5 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#codigo-limpio-30-puntos","title":"Codigo Limpio (30 puntos)","text":"<ul> <li> Funciones bien documentadas (10 pts)</li> <li> Codigo legible y organizado (10 pts)</li> <li> Manejo de errores (10 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#rendimiento-20-puntos","title":"Rendimiento (20 puntos)","text":"<ul> <li> Carga eficiente con chunks (10 pts)</li> <li> Indices mejoran rendimiento medible (10 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#analisis-10-puntos","title":"Analisis (10 puntos)","text":"<ul> <li> Interpretacion de resultados (5 pts)</li> <li> Insights adicionales (5 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#entregables","title":"Entregables","text":"<p>Debes entregar subiendo a tu fork (git push):</p> <ol> <li>Codigo Python: <code>01_cargar_sqlite.py</code></li> <li>Base de datos: <code>datos/taxi.db</code> (NO subir a GitHub, muy grande)</li> <li>Resultados: Carpeta <code>resultados/</code> con CSVs exportados</li> <li>Analisis: <code>ANALISIS.md</code> con tus hallazgos</li> </ol>"},{"location":"ejercicios/01-introduccion-sqlite/#estructura-de-carpeta","title":"Estructura de carpeta","text":"<pre><code>entregas/01_sqlite/tu_apellido_nombre/\n\u251c\u2500\u2500 01_cargar_sqlite.py\n\u251c\u2500\u2500 resultados/\n\u2502   \u251c\u2500\u2500 ingresos_por_hora.csv\n\u2502   \u251c\u2500\u2500 distribucion_pagos.csv\n\u2502   \u2514\u2500\u2500 top_rutas.csv\n\u2514\u2500\u2500 ANALISIS.md\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#ejemplo-de-solucion-parcial","title":"Ejemplo de Solucion (Parcial)","text":"<p>Codigo de Ejemplo</p> <pre><code>import sqlite3\nimport pandas as pd\nimport time\n\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"Carga CSV a SQLite en chunks\"\"\"\n    conn = sqlite3.connect(db_path)\n\n    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n\n    for i, chunk in enumerate(chunks):\n        chunk.to_sql('trips', conn, if_exists='append', index=False)\n        print(f\"Chunk {i+1} cargado ({len(chunk)} registros)\")\n\n    conn.close()\n    print(\"Carga completa!\")\n\ndef crear_indices(db_path):\n    \"\"\"Crea indices para optimizacion\"\"\"\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    print(\"Creando indices...\")\n    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_pickup ON trips(tpep_pickup_datetime)\")\n    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_payment ON trips(payment_type)\")\n\n    conn.commit()\n    conn.close()\n    print(\"Indices creados!\")\n\ndef analizar_ingresos_por_hora(db_path):\n    \"\"\"Query: Ingresos promedio por hora\"\"\"\n    conn = sqlite3.connect(db_path)\n\n    query = \"\"\"\n        SELECT\n            strftime('%H', tpep_pickup_datetime) as hora,\n            ROUND(AVG(total_amount), 2) as promedio_ingreso,\n            COUNT(*) as num_viajes\n        FROM trips\n        GROUP BY hora\n        ORDER BY hora\n    \"\"\"\n\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return df\n\nif __name__ == \"__main__\":\n    # Rutas\n    csv_path = \"datos/muestra_taxi.csv\"\n    db_path = \"datos/taxi.db\"\n\n    # 1. Cargar datos\n    print(\"=== CARGANDO DATOS ===\")\n    start = time.time()\n    cargar_datos_sqlite(csv_path, db_path)\n    print(f\"Tiempo: {time.time() - start:.2f} segundos\\n\")\n\n    # 2. Crear indices\n    print(\"=== CREANDO INDICES ===\")\n    crear_indices(db_path)\n    print()\n\n    # 3. Analisis\n    print(\"=== ANALISIS: INGRESOS POR HORA ===\")\n    df = analizar_ingresos_por_hora(db_path)\n    print(df)\n\n    # Exportar\n    df.to_csv(\"resultados/ingresos_por_hora.csv\", index=False)\n    print(\"\\nResultados exportados!\")\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#preguntas-para-reflexion","title":"Preguntas para Reflexion","text":"<p>Responde en tu <code>ANALISIS.md</code>:</p> <ol> <li> <p>Rendimiento:</p> <ul> <li>Cuanto tiempo tomo cargar 100,000 registros?</li> <li>Cuanto mejoraron los indices el tiempo de query?</li> </ul> </li> <li> <p>Insights:</p> <ul> <li>A que hora del dia los taxis ganan mas?</li> <li>Cual es el metodo de pago mas comun?</li> <li>Que patron observas en los datos?</li> </ul> </li> <li> <p>Mejoras:</p> <ul> <li>Como optimizarias aun mas la carga?</li> <li>Que otros analisis se podrian hacer?</li> <li>Que limitaciones tiene SQLite para este dataset?</li> </ul> </li> </ol>"},{"location":"ejercicios/01-introduccion-sqlite/#recursos-adicionales","title":"Recursos Adicionales","text":""},{"location":"ejercicios/01-introduccion-sqlite/#documentacion","title":"Documentacion","text":"<ul> <li>SQLite Python Tutorial</li> <li>Pandas to_sql Documentation</li> <li>SQL Tutorial</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#tutoriales","title":"Tutoriales","text":"<ul> <li>Working with Large CSV Files</li> <li>SQLite Indexes</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#problemas-comunes","title":"Problemas Comunes","text":"Error: MemoryError al cargar CSV <p>Causa: Intentando cargar todo el archivo de una vez.</p> <p>Solucion: Usa chunks: <pre><code>chunks = pd.read_csv(csv_path, chunksize=10000)\n</code></pre></p> Query muy lenta <p>Causa: Falta indice en columnas consultadas.</p> <p>Solucion: Crea indices: <pre><code>cursor.execute(\"CREATE INDEX idx_nombre ON tabla(columna)\")\n</code></pre></p> Error: database is locked <p>Causa: Otro proceso tiene la BD abierta.</p> <p>Solucion: - Cierra todas las conexiones: <code>conn.close()</code> - Asegurate de no tener la BD abierta en otro programa</p>"},{"location":"ejercicios/01-introduccion-sqlite/#proximos-pasos","title":"Proximos Pasos","text":"<p>Una vez completado este ejercicio:</p> <ul> <li>Ejercicio 02: Limpieza de Datos - Siguiente ejercicio</li> <li>Guia de Entregas - Como entregar tu trabajo</li> <li>Roadmap - Ver todos los ejercicios</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/","title":"Ejercicio 02: Pipeline ETL Profesional - Quality of Government","text":"<p>Nivel: Avanzado | Duraci\u00f3n: 15-20 horas | Modalidad: Grupal o Individual</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Construir\u00e1s un pipeline ETL profesional trabajando con el dataset Quality of Government, una base de datos longitudinal con m\u00e1s de 1000 variables sobre calidad institucional, democracia y desarrollo econ\u00f3mico.</p> <p>Objetivo: Aplicar t\u00e9cnicas de ingenier\u00eda de software y ciencia de datos para limpiar, transformar y analizar datos reales de investigaci\u00f3n acad\u00e9mica.</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#dataset-quality-of-government-qog","title":"Dataset: Quality of Government (QoG)","text":"<p>\u00bfQu\u00e9 es?</p> <p>Base de datos mantenida por la Universidad de Gotemburgo que agrega variables de m\u00faltiples fuentes internacionales.</p> <p>Caracter\u00edsticas: - 1289 variables de calidad institucional, econom\u00eda, sociedad - 194+ pa\u00edses con datos desde 1946 - Fuentes: World Bank, V-Dem, Transparency International, Freedom House, UNDP</p> <p>Fuente: https://www.qog.pol.gu.se/</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>\u2705 Dise\u00f1ar arquitectura ETL modular</li> <li>\u2705 Trabajar con PostgreSQL para an\u00e1lisis longitudinal</li> <li>\u2705 Limpiar datasets complejos (&gt;1000 variables)</li> <li>\u2705 Preparar datos de panel para econometr\u00eda</li> <li>\u2705 Aplicar buenas pr\u00e1cticas de software engineering</li> <li>\u2705 Escribir c\u00f3digo production-ready</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#temas-de-analisis","title":"Temas de An\u00e1lisis","text":"<p>Elige UNO:</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#tema-1-evolucion-institucional-post-autoritaria","title":"Tema 1: Evoluci\u00f3n Institucional Post-Autoritaria","text":"<p>Pregunta: \u00bfC\u00f3mo evoluciona la calidad institucional en transiciones democr\u00e1ticas?</p> <p>Variables clave: - \u00cdndices de democracia (V-Dem, Polity) - Calidad institucional (Transparency International) - Desarrollo econ\u00f3mico (PIB, HDI)</p> <p>Casos: Europa del Este, Am\u00e9rica Latina, Asia Central</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#tema-2-recursos-naturales-y-desarrollo","title":"Tema 2: Recursos Naturales y Desarrollo","text":"<p>Pregunta: \u00bfLa dependencia de recursos naturales afecta el desarrollo?</p> <p>Variables clave: - Producci\u00f3n petr\u00f3leo/gas (Ross dataset) - Rentas recursos naturales (World Bank) - Acceso servicios b\u00e1sicos (agua, saneamiento) - Calidad institucional</p> <p>Casos: Pa\u00edses petroleros, resource curse, seguridad h\u00eddrica</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#arquitectura-del-proyecto","title":"Arquitectura del Proyecto","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#estructura-esperada","title":"Estructura Esperada","text":"<pre><code>tu_apellido_nombre/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 database/          # Conexi\u00f3n PostgreSQL\n\u2502   \u251c\u2500\u2500 etl/               # Extract, Transform, Load\n\u2502   \u251c\u2500\u2500 analysis/          # An\u00e1lisis de datos\n\u2502   \u2514\u2500\u2500 utils/             # Logging, helpers\n\u251c\u2500\u2500 scripts/               # CLI ejecutables\n\u251c\u2500\u2500 sql/                   # Queries complejas\n\u251c\u2500\u2500 tests/                 # Tests (opcional)\n\u2514\u2500\u2500 docs/                  # Documentaci\u00f3n\n</code></pre> <p>Ver detalles: Arquitectura completa</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#tecnologias","title":"Tecnolog\u00edas","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#obligatorias","title":"Obligatorias","text":"<ul> <li>Python 3.11+</li> <li>PostgreSQL 14+</li> <li>pandas - Manipulaci\u00f3n de datos</li> <li>psycopg2 - Conexi\u00f3n PostgreSQL</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#recomendadas","title":"Recomendadas","text":"<ul> <li>SQLAlchemy - ORM</li> <li>pytest - Testing</li> <li>Black - Code formatting</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#fases-del-proyecto","title":"Fases del Proyecto","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#1-extract-e","title":"1. Extract (E)","text":"<ul> <li>Descargar dataset QoG</li> <li>Filtrar por tema y per\u00edodo</li> <li>Validar integridad</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#2-transform-t","title":"2. Transform (T)","text":"<ul> <li>Renombrar columnas</li> <li>Crear variables derivadas</li> <li>Manejar valores faltantes</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#3-load-l","title":"3. Load (L)","text":"<ul> <li>Cargar a PostgreSQL</li> <li>Validar integridad referencial</li> <li>Optimizar con \u00edndices</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#4-analysis","title":"4. Analysis","text":"<ul> <li>Estad\u00edsticas descriptivas</li> <li>Preparar panel balanceado</li> <li>Exportar para econometr\u00eda (.dta, .csv)</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#datos-de-panel","title":"Datos de Panel","text":"<p>Este ejercicio te prepara para an\u00e1lisis econom\u00e9trico.</p> <p>Panel data = Cross-section \u00d7 Time-series</p> <pre><code>| country | year | democracy | gdp_pc |\n|---------|------|-----------|--------|\n| ESP     | 2000 | 0.85      | 24000  |\n| ESP     | 2001 | 0.86      | 24500  |\n</code></pre> <p>Permite: - Fixed Effects (controlar heterogeneidad) - Random Effects - Difference-in-Differences - Modelos din\u00e1micos</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#especificaciones-tecnicas","title":"Especificaciones T\u00e9cnicas","text":"<p>Toda la documentaci\u00f3n t\u00e9cnica detallada est\u00e1 en:</p> <pre><code>ejercicios/02_limpieza_datos/especificaciones/\n\u251c\u2500\u2500 ARQUITECTURA.md           # Estructura de proyecto\n\u251c\u2500\u2500 ESQUEMA_DB.sql            # Schema PostgreSQL completo\n\u251c\u2500\u2500 FUNCIONES_REQUERIDAS.md   # Firmas de funciones\n\u251c\u2500\u2500 VARIABLES_TEMA1.md        # Variables + prompts AI\n\u251c\u2500\u2500 VARIABLES_TEMA2.md        # Variables + prompts AI\n\u2514\u2500\u2500 VALIDACIONES.md           # Checks obligatorios\n</code></pre> <p>Especialmente \u00fatil: VARIABLES_TEMA*.md incluyen prompts para Claude/ChatGPT para investigar variables adicionales.</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#criterios-de-evaluacion","title":"Criterios de Evaluaci\u00f3n","text":"Criterio Peso Qu\u00e9 evaluamos Funcionalidad 40% Pipeline ejecuta sin errores, datos correctos Arquitectura 25% C\u00f3digo modular, separaci\u00f3n responsabilidades Calidad C\u00f3digo 20% PEP 8, type hints, docstrings Documentaci\u00f3n 10% README, metodolog\u00eda, comentarios Innovaci\u00f3n 5% Tests, visualizaciones, an\u00e1lisis extra"},{"location":"ejercicios/02-pipeline-etl-qog/#entregables","title":"Entregables","text":"<p>Ubicaci\u00f3n: <code>entregas/02_limpieza_datos/tu_apellido_nombre/</code></p> <p>M\u00ednimo requerido: - C\u00f3digo fuente modular (src/) - Scripts ejecutables (scripts/) - Queries SQL (sql/) - README completo - METODOLOGIA.md (decisiones de dise\u00f1o) - requirements.txt</p> <p>NO incluir: Datos, logs, venv/, .env</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#dataset","title":"Dataset","text":"<ul> <li>QoG Website</li> <li>Codebook PDF</li> <li>Download CSV</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#guias-del-proyecto","title":"Gu\u00edas del Proyecto","text":"<ul> <li>Setup PostgreSQL</li> <li>Instrucciones de Entrega</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#documentacion-tecnica","title":"Documentaci\u00f3n T\u00e9cnica","text":"<ul> <li>PostgreSQL Docs</li> <li>pandas Docs</li> <li>psycopg2 Docs</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#preparacion-para-docker","title":"Preparaci\u00f3n para Docker","text":"<p>Este proyecto est\u00e1 dise\u00f1ado para ser dockerizado en ejercicios futuros.</p> <p>Tu arquitectura modular facilitar\u00e1: - Contenedor PostgreSQL - Contenedor aplicaci\u00f3n Python - docker-compose orchestration</p> <p>Por ahora: PostgreSQL instalaci\u00f3n local.</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#como-empezar","title":"C\u00f3mo Empezar","text":"<ol> <li>Lee toda la documentaci\u00f3n en <code>ejercicios/02_limpieza_datos/</code></li> <li>Instala PostgreSQL (ver POSTGRESQL_SETUP.md)</li> <li>Elige un tema (1 o 2)</li> <li>Investiga variables usando prompts en VARIABLES_TEMA*.md</li> <li>Implementa paso a paso: Extract \u2192 Transform \u2192 Load \u2192 Analysis</li> <li>Testea frecuentemente</li> <li>Documenta mientras codeas</li> </ol>"},{"location":"ejercicios/02-pipeline-etl-qog/#consejos","title":"Consejos","text":"<ul> <li>Empieza con pipeline b\u00e1sico, optimiza despu\u00e9s</li> <li>Logging en todo (tu mejor amigo para debugging)</li> <li>Git commits frecuentes</li> <li>Testea con subset peque\u00f1o primero (no 1M filas de golpe)</li> <li>Lee el codebook QoG - es tu biblia</li> <li>Pregunta temprano si algo no est\u00e1 claro</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#faq","title":"FAQ","text":"<p>P: \u00bfIndividual o grupal? R: Eliges. Grupos 2-5 personas.</p> <p>P: \u00bfTengo que usar TODAS las variables sugeridas? R: No, son sugerencias. Investiga y elige las relevantes.</p> <p>P: \u00bfPuedo usar Docker? R: No para este ejercicio. PostgreSQL local. Docker ser\u00e1 futuro.</p> <p>P: \u00bfQu\u00e9 hago si no encuentro una variable? R: Usa los prompts AI en VARIABLES_TEMA*.md para buscar alternativas.</p> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/02-postgresql-hr/","title":"Ejercicio 2.1: PostgreSQL con Base de Datos HR","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/02-postgresql-hr/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Aprender\u00e1s a trabajar con PostgreSQL usando la base de datos HR (Human Resources) de Oracle adaptada.</p> <p>Duraci\u00f3n estimada: 4-6 horas Nivel: Intermedio Prerequisitos: SQL b\u00e1sico, haber completado Ejercicio 1.1</p>"},{"location":"ejercicios/02-postgresql-hr/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Instalar y configurar PostgreSQL en tu sistema</li> <li>\u2705 Crear bases de datos y usuarios en PostgreSQL</li> <li>\u2705 Cargar esquemas y datos desde scripts SQL</li> <li>\u2705 Realizar consultas complejas con m\u00faltiples JOINs</li> <li>\u2705 Usar funciones espec\u00edficas de PostgreSQL</li> <li>\u2705 Comparar sintaxis SQL: Oracle vs PostgreSQL</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#base-de-datos-hr","title":"Base de Datos HR","text":"<p>La BD HR es una base de datos de ejemplo oficial de Oracle que modela un sistema de gesti\u00f3n de recursos humanos.</p>"},{"location":"ejercicios/02-postgresql-hr/#entidades-principales","title":"Entidades Principales","text":"<ul> <li>Employees - Informaci\u00f3n de empleados</li> <li>Departments - Departamentos de la empresa</li> <li>Jobs - Puestos de trabajo</li> <li>Locations - Ubicaciones geogr\u00e1ficas</li> <li>Countries - Pa\u00edses</li> <li>Regions - Regiones</li> <li>Job_History - Historial laboral de empleados</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#diagrama-er-simplificado","title":"Diagrama ER Simplificado","text":"<pre><code>erDiagram\n    REGIONS ||--o{ COUNTRIES : contains\n    COUNTRIES ||--o{ LOCATIONS : has\n    LOCATIONS ||--o{ DEPARTMENTS : located_in\n    DEPARTMENTS ||--o{ EMPLOYEES : employs\n    EMPLOYEES ||--o| EMPLOYEES : manages\n    JOBS ||--o{ EMPLOYEES : has_role</code></pre>"},{"location":"ejercicios/02-postgresql-hr/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/02-postgresql-hr/#software-necesario","title":"Software Necesario","text":"<ol> <li>PostgreSQL 14+</li> <li>Descargar para Windows</li> <li>Descargar para Mac</li> <li> <p>Descargar para Linux</p> </li> <li> <p>Cliente SQL (elige uno):</p> </li> <li>pgAdmin (incluido con PostgreSQL)</li> <li>DBeaver (recomendado para principiantes)</li> <li> <p>VS Code con extensi\u00f3n PostgreSQL</p> </li> <li> <p>Python (opcional):</p> </li> <li>psycopg2 para conectar desde Python    <pre><code>pip install psycopg2-binary\n</code></pre></li> </ol>"},{"location":"ejercicios/02-postgresql-hr/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/2.1_postgresql_hr/\n</code></pre>"},{"location":"ejercicios/02-postgresql-hr/#estructura","title":"Estructura","text":"<ul> <li><code>README.md</code> - Instrucciones detalladas</li> <li><code>scripts/</code> - Scripts SQL (a completar por alumnos)</li> <li><code>soluciones/</code> - Soluciones de referencia</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#datos","title":"Datos","text":"<p>Scripts de la BD HR est\u00e1n en: <code>datos/oracle_hr/</code></p>"},{"location":"ejercicios/02-postgresql-hr/#temas-cubiertos","title":"Temas Cubiertos","text":""},{"location":"ejercicios/02-postgresql-hr/#1-instalacion-y-configuracion","title":"1. Instalaci\u00f3n y Configuraci\u00f3n","text":"<ul> <li>Instalar PostgreSQL</li> <li>Crear usuario y base de datos</li> <li>Configurar conexi\u00f3n</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#2-carga-de-datos","title":"2. Carga de Datos","text":"<ul> <li>Ejecutar scripts DDL (estructura)</li> <li>Ejecutar scripts DML (datos)</li> <li>Verificar integridad</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#3-consultas-basicas","title":"3. Consultas B\u00e1sicas","text":"<ul> <li>SELECT con filtros</li> <li>Ordenamiento y l\u00edmites</li> <li>Funciones de agregaci\u00f3n</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#4-consultas-avanzadas","title":"4. Consultas Avanzadas","text":"<ul> <li>JOINs m\u00faltiples</li> <li>Subconsultas</li> <li>CTEs (Common Table Expressions)</li> <li>Window Functions</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#5-analisis-de-negocio","title":"5. An\u00e1lisis de Negocio","text":"<ul> <li>Salarios por departamento</li> <li>Jerarqu\u00edas de empleados</li> <li>Historial laboral</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.1_postgresql_hr/\n</code></pre></p>"},{"location":"ejercicios/02-postgresql-hr/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/02-postgresql-hr/#documentacion-oficial","title":"Documentaci\u00f3n Oficial","text":"<ul> <li>PostgreSQL Documentation</li> <li>PostgreSQL Tutorial</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#tutoriales","title":"Tutoriales","text":"<ul> <li>Gu\u00eda de Instalaci\u00f3n PostgreSQL</li> <li>SQL Avanzado en PostgreSQL</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#comparativas","title":"Comparativas","text":"<ul> <li>Oracle vs PostgreSQL - Diferencias de Sintaxis</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 2.2 - PostgreSQL Jardiner\u00eda (m\u00e1s consultas complejas)</li> <li>Ejercicio 2.3 - Migraci\u00f3n de SQLite a PostgreSQL</li> <li>Ejercicio 3.1 - Oracle con BD HR (comparar con PostgreSQL)</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/03-postgresql-jardineria/","title":"Ejercicio 2.2: PostgreSQL con Base de Datos Jardiner\u00eda","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/03-postgresql-jardineria/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Practicar\u00e1s consultas SQL avanzadas con una base de datos de gesti\u00f3n de ventas de jardiner\u00eda.</p> <p>Duraci\u00f3n estimada: 4-6 horas Nivel: Intermedio Prerequisitos: SQL b\u00e1sico, Ejercicio 2.1 (PostgreSQL HR)</p>"},{"location":"ejercicios/03-postgresql-jardineria/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Analizar datos de ventas con SQL</li> <li>\u2705 Generar reportes de negocio</li> <li>\u2705 Usar agregaciones complejas (GROUP BY, HAVING)</li> <li>\u2705 Aplicar Window Functions para rankings y an\u00e1lisis temporal</li> <li>\u2705 Optimizar consultas con \u00edndices</li> <li>\u2705 Crear vistas materializadas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#base-de-datos-jardineria","title":"Base de Datos Jardiner\u00eda","text":"<p>Base de datos de una empresa que vende productos de jardiner\u00eda y plantas.</p>"},{"location":"ejercicios/03-postgresql-jardineria/#entidades-principales","title":"Entidades Principales","text":"<ul> <li>Clientes - Clientes de la empresa</li> <li>Empleados - Organizaci\u00f3n y ventas</li> <li>Oficinas - Ubicaciones de la empresa</li> <li>Pedidos - \u00d3rdenes de compra</li> <li>Detalle_Pedidos - L\u00edneas de cada pedido</li> <li>Productos - Cat\u00e1logo de productos</li> <li>Gamas_Producto - Categor\u00edas</li> <li>Pagos - Transacciones</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    OFICINAS ||--o{ EMPLEADOS : trabaja_en\n    EMPLEADOS ||--o{ CLIENTES : atiende\n    CLIENTES ||--o{ PEDIDOS : realiza\n    CLIENTES ||--o{ PAGOS : hace\n    GAMAS_PRODUCTO ||--o{ PRODUCTOS : tiene\n    PEDIDOS ||--o{ DETALLE_PEDIDOS : contiene\n    PRODUCTOS ||--o{ DETALLE_PEDIDOS : incluido_en</code></pre>"},{"location":"ejercicios/03-postgresql-jardineria/#casos-de-uso-reales","title":"Casos de Uso Reales","text":"<p>Este ejercicio simula an\u00e1lisis que har\u00edas en una empresa real:</p>"},{"location":"ejercicios/03-postgresql-jardineria/#1-analisis-de-ventas","title":"1. An\u00e1lisis de Ventas","text":"<ul> <li>Total de ventas por cliente</li> <li>Productos m\u00e1s vendidos</li> <li>Evoluci\u00f3n de ventas por mes/a\u00f1o</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#2-gestion-de-clientes","title":"2. Gesti\u00f3n de Clientes","text":"<ul> <li>Clientes con mayor volumen de compra</li> <li>Clientes inactivos (sin pedidos recientes)</li> <li>Distribuci\u00f3n geogr\u00e1fica</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#3-rendimiento-de-empleados","title":"3. Rendimiento de Empleados","text":"<ul> <li>Ventas por empleado</li> <li>Clientes asignados por empleado</li> <li>Performance por oficina</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#4-inventario","title":"4. Inventario","text":"<ul> <li>Productos con bajo stock</li> <li>An\u00e1lisis de rotaci\u00f3n</li> <li>Productos sin ventas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":"<ul> <li>PostgreSQL 14+ instalado (del Ejercicio 2.1)</li> <li>Cliente SQL (pgAdmin, DBeaver)</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/2.2_postgresql_jardineria/\n</code></pre>"},{"location":"ejercicios/03-postgresql-jardineria/#datos","title":"Datos","text":"<p>Scripts SQL est\u00e1n en: <code>datos/jardineria/</code></p>"},{"location":"ejercicios/03-postgresql-jardineria/#temas-cubiertos","title":"Temas Cubiertos","text":""},{"location":"ejercicios/03-postgresql-jardineria/#1-consultas-de-analisis","title":"1. Consultas de An\u00e1lisis","text":"<ul> <li>Agregaciones con GROUP BY</li> <li>Filtrado con HAVING</li> <li>M\u00faltiples JOINs</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#2-window-functions","title":"2. Window Functions","text":"<ul> <li>ROW_NUMBER() para rankings</li> <li>LAG/LEAD para comparaciones temporales</li> <li>PARTITION BY para an\u00e1lisis por grupo</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#3-subconsultas","title":"3. Subconsultas","text":"<ul> <li>Subconsultas correlacionadas</li> <li>EXISTS / NOT EXISTS</li> <li>IN / NOT IN</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#4-optimizacion","title":"4. Optimizaci\u00f3n","text":"<ul> <li>EXPLAIN para analizar planes de ejecuci\u00f3n</li> <li>Creaci\u00f3n de \u00edndices</li> <li>An\u00e1lisis de rendimiento</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#5-vistas","title":"5. Vistas","text":"<ul> <li>Vistas simples</li> <li>Vistas materializadas</li> <li>Actualizaci\u00f3n de vistas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.2_postgresql_jardineria/\n</code></pre></p>"},{"location":"ejercicios/03-postgresql-jardineria/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/03-postgresql-jardineria/#documentacion","title":"Documentaci\u00f3n","text":"<ul> <li>PostgreSQL Window Functions</li> <li>Query Optimization</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#tutoriales","title":"Tutoriales","text":"<ul> <li>An\u00e1lisis de Ventas con SQL</li> <li>Window Functions Explicadas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 2.3 - Migraci\u00f3n SQLite \u2192 PostgreSQL</li> <li>Ejercicio 3.2 - Oracle Jardiner\u00eda (comparar implementaci\u00f3n)</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/03-procesamiento-distribuido/","title":"Ejercicio 03: Procesamiento Distribuido con Dask","text":"<p>En este m\u00f3dulo aprender\u00e1s a escalar tu capacidad de c\u00f3mputo m\u00e1s all\u00e1 de la memoria RAM de tu m\u00e1quina, utilizando clusters locales.</p>"},{"location":"ejercicios/03-procesamiento-distribuido/#objetivos","title":"Objetivos","text":"<ol> <li>Configurar un Cluster Local con Dask.</li> <li>Leer archivos Parquet de forma particionada.</li> <li>Ejecutar agregaciones complejas (GroupBy) en paralelo.</li> </ol>"},{"location":"ejercicios/03-procesamiento-distribuido/#instrucciones","title":"Instrucciones","text":"<p>El script principal se encuentra en <code>ejercicios/03_procesamiento_distribuido/esqueleto.py</code>. Tu tarea es completar las funciones marcadas con <code>TODO</code> para construir un pipeline funcional.</p>"},{"location":"ejercicios/03-procesamiento-distribuido/#tarea-de-programacion","title":"Tarea de Programaci\u00f3n","text":"<p>Debes implementar la funci\u00f3n <code>procesamiento_dask()</code> para que: 1. Inicie un cliente local (<code>LocalCluster</code>). 2. Lea el dataset de QoG procesado en el ejercicio anterior. 3. Calcule el promedio anual del \u00cdndice de Democracia. 4. Compare el tiempo de ejecuci\u00f3n vs Pandas tradicional.</p>"},{"location":"ejercicios/04-machine-learning/","title":"M\u00f3dulo 04: Machine Learning","text":"<p>T\u00e9cnicas de aprendizaje autom\u00e1tico aplicadas a Big Data: desde clustering tradicional hasta Computer Vision con Deep Learning.</p>"},{"location":"ejercicios/04-machine-learning/#ejercicio-41-pca-y-clustering","title":"Ejercicio 4.1: PCA y Clustering","text":"<p>Aplicamos t\u00e9cnicas de aprendizaje no supervisado para detectar patrones en datos complejos.</p>"},{"location":"ejercicios/04-machine-learning/#alcance","title":"Alcance","text":"<ul> <li>Reducci\u00f3n de Dimensionalidad: Principal Component Analysis (PCA)</li> <li>Clustering: K-Means y Hierarchical Clustering (HCA)</li> </ul>"},{"location":"ejercicios/04-machine-learning/#tareas","title":"Tareas","text":"<ol> <li>PCA: Reducir variables a 2 componentes principales para visualizaci\u00f3n</li> <li>Clustering: Implementar K-Means y determinar K \u00f3ptimo (m\u00e9todo del codo, Silhouette)</li> <li>Interpretaci\u00f3n: Generar perfil de cada cluster</li> </ol>"},{"location":"ejercicios/04-machine-learning/#recursos","title":"Recursos","text":"<ul> <li>Dashboard PCA + K-Means Iris</li> <li>Dashboard PCA estilo FactoMineR</li> </ul>"},{"location":"ejercicios/04-machine-learning/#ejercicio-42-transfer-learning-clasificacion-de-flores","title":"Ejercicio 4.2: Transfer Learning - Clasificaci\u00f3n de Flores","text":"<p>Pipeline de Computer Vision que clasifica im\u00e1genes de flores usando Transfer Learning con MobileNetV2.</p>"},{"location":"ejercicios/04-machine-learning/#que-es-transfer-learning","title":"\u00bfQu\u00e9 es Transfer Learning?","text":"<p>En lugar de entrenar una red neuronal desde cero (necesitar\u00edamos millones de im\u00e1genes), usamos una red ya entrenada en ImageNet y la adaptamos:</p> <pre><code>ImageNet (14M imgs) \u2192 MobileNetV2 \u2192 Embeddings (1280D) \u2192 Clasificador ML\n</code></pre> <p>Las primeras capas de la CNN ya aprendieron patrones universales (bordes, texturas, formas) que sirven para cualquier imagen.</p>"},{"location":"ejercicios/04-machine-learning/#pipeline","title":"Pipeline","text":"<pre><code>1. DESCARGA          2. EMBEDDINGS         3. CLASIFICACI\u00d3N      4. VISUALIZACI\u00d3N\n   3,670 flores         MobileNetV2           ML tradicional        Dashboard\n   5 clases             1280 features         KNN/SVM/RF            Plotly\n</code></pre>"},{"location":"ejercicios/04-machine-learning/#resultados","title":"Resultados","text":"Modelo Accuracy SVM 89.9% Random Forest 86.5% KNN 86.2%"},{"location":"ejercicios/04-machine-learning/#ejecutar","title":"Ejecutar","text":"<pre><code>cd ejercicios/04_machine_learning/flores_transfer_learning/\npip install -r requirements.txt\npython 01_flores_transfer_learning.py\n</code></pre> <p>Requisitos: TensorFlow (GPU recomendado pero funciona en CPU)</p>"},{"location":"ejercicios/04-machine-learning/#recursos_1","title":"Recursos","text":"<ul> <li>Dashboard Transfer Learning Flores - Galer\u00eda, t-SNE, Comparativa, Confusion Matrix</li> <li>Dashboard Interactivo</li> </ul>"},{"location":"ejercicios/04-machine-learning/#ejercicio-43-series-temporales-arimasarima","title":"Ejercicio 4.3: Series Temporales (ARIMA/SARIMA)","text":"<p>Metodolog\u00eda Box-Jenkins para an\u00e1lisis y pron\u00f3stico de series temporales.</p>"},{"location":"ejercicios/04-machine-learning/#recursos_2","title":"Recursos","text":"<ul> <li>ARIMA/SARIMA - Metodolog\u00eda Box-Jenkins</li> <li>Dashboard ARIMA Interactivo</li> </ul> <p>Curso: Big Data con Python - De Cero a Producci\u00f3n Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Referencias acad\u00e9micas:</p> <ul> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12.</li> <li>Sandler, M., et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR.</li> <li>van der Maaten, L. &amp; Hinton, G. (2008). Visualizing Data using t-SNE. JMLR.</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/","title":"Ejercicio 2.3: Migraci\u00f3n SQLite \u2192 PostgreSQL","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Aprender\u00e1s a migrar bases de datos desde SQLite a PostgreSQL, comprendiendo las diferencias entre ambos motores.</p> <p>Duraci\u00f3n estimada: 3-4 horas Nivel: Intermedio-Avanzado Prerequisitos: Ejercicio 1.1 (SQLite), Ejercicio 2.1 (PostgreSQL)</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Identificar diferencias entre SQLite y PostgreSQL</li> <li>\u2705 Adaptar tipos de datos entre motores</li> <li>\u2705 Migrar esquemas (DDL)</li> <li>\u2705 Migrar datos (DML)</li> <li>\u2705 Escribir scripts de migraci\u00f3n en Python</li> <li>\u2705 Validar integridad despu\u00e9s de la migraci\u00f3n</li> <li>\u2705 Comparar rendimiento entre motores</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#contexto-del-ejercicio","title":"Contexto del Ejercicio","text":"<p>Usar\u00e1s las bases de datos que creaste en el Ejercicio 1.1 (tienda inform\u00e1tica):</p> <ul> <li><code>tienda_modelo_a.db</code> - 26 tablas independientes</li> <li><code>tienda_modelo_b.db</code> - Modelo normalizado</li> <li><code>tienda_modelo_c.db</code> - E-commerce completo</li> </ul> <p>Objetivo: Migrarlas a PostgreSQL y comparar.</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#diferencias-principales","title":"Diferencias Principales","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#tipos-de-datos","title":"Tipos de Datos","text":"SQLite PostgreSQL <code>INTEGER</code> <code>INTEGER</code> o <code>BIGINT</code> <code>REAL</code> <code>NUMERIC(p,s)</code> o <code>DOUBLE PRECISION</code> <code>TEXT</code> <code>VARCHAR(n)</code> o <code>TEXT</code> <code>BLOB</code> <code>BYTEA</code> Sin tipo PostgreSQL es estricto"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#auto-increment","title":"Auto-increment","text":"SQLite PostgreSQL <code>AUTOINCREMENT</code> <code>SERIAL</code> o <code>IDENTITY</code>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#constraints","title":"Constraints","text":"<p>PostgreSQL tiene constraints m\u00e1s estrictos y soporta m\u00e1s tipos.</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#performance","title":"Performance","text":"<p>PostgreSQL es mucho m\u00e1s r\u00e1pido con grandes vol\u00famenes y consultas complejas.</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/2.3_postgresql_tienda/\n</code></pre>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#archivos","title":"Archivos","text":"<ul> <li><code>migracion_desde_sqlite.py</code> - Script de migraci\u00f3n (plantilla)</li> <li><code>comparativa_sqlite_vs_postgres.md</code> - An\u00e1lisis comparativo</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#proceso-de-migracion","title":"Proceso de Migraci\u00f3n","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#1-analisis-del-esquema-origen","title":"1. An\u00e1lisis del Esquema Origen","text":"<pre><code># Conectar a SQLite\nimport sqlite3\nconn = sqlite3.connect('tienda_modelo_b.db')\n\n# Obtener lista de tablas\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n</code></pre>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#2-adaptacion-de-tipos-de-datos","title":"2. Adaptaci\u00f3n de Tipos de Datos","text":"<p>Convertir tipos SQLite \u2192 PostgreSQL: - Detectar tipos din\u00e1micamente - Mapear a tipos PostgreSQL equivalentes - Ajustar precisi\u00f3n de NUMERIC</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#3-recreacion-del-esquema","title":"3. Recreaci\u00f3n del Esquema","text":"<p>Generar DDL para PostgreSQL: - CREATE TABLE con tipos adaptados - PRIMARY KEY - FOREIGN KEY - CONSTRAINTS</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#4-migracion-de-datos","title":"4. Migraci\u00f3n de Datos","text":"<pre><code># Leer datos de SQLite\ndf = pd.read_sql(\"SELECT * FROM productos\", sqlite_conn)\n\n# Insertar en PostgreSQL\ndf.to_sql('productos', postgres_engine, if_exists='append')\n</code></pre>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#5-validacion","title":"5. Validaci\u00f3n","text":"<ul> <li>Verificar conteos: SQLite vs PostgreSQL</li> <li>Validar integridad referencial</li> <li>Probar consultas complejas</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#tareas-a-realizar","title":"Tareas a Realizar","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#parte-1-script-de-migracion","title":"Parte 1: Script de Migraci\u00f3n","text":"<p>Crear <code>migracion_desde_sqlite.py</code> que:</p> <ol> <li>Lee esquema de SQLite</li> <li>Genera DDL para PostgreSQL</li> <li>Migra datos tabla por tabla</li> <li>Valida la migraci\u00f3n</li> <li>Genera reporte de \u00e9xito/errores</li> </ol>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#parte-2-analisis-comparativo","title":"Parte 2: An\u00e1lisis Comparativo","text":"<p>Crear <code>comparativa_sqlite_vs_postgres.md</code> con:</p> <ol> <li>Diferencias de Esquema</li> <li>Tipos de datos modificados</li> <li>Constraints a\u00f1adidos</li> <li> <p>\u00cdndices creados</p> </li> <li> <p>Pruebas de Rendimiento</p> </li> <li>Misma consulta en ambos motores</li> <li>Tiempos de ejecuci\u00f3n</li> <li> <p>Uso de memoria</p> </li> <li> <p>Conclusiones</p> </li> <li>\u00bfCu\u00e1ndo usar SQLite?</li> <li>\u00bfCu\u00e1ndo usar PostgreSQL?</li> <li>Recomendaciones</li> </ol>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#software","title":"Software","text":"<ul> <li>SQLite (ya instalado)</li> <li>PostgreSQL 14+</li> <li>Python con:   <pre><code>pip install pandas psycopg2-binary sqlite3\n</code></pre></li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#datos-origen","title":"Datos Origen","text":"<p>Tus bases de datos del Ejercicio 1.1: - <code>tienda_modelo_a.db</code> - <code>tienda_modelo_b.db</code> - <code>tienda_modelo_c.db</code></p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.3_postgresql_tienda/\n\u251c\u2500\u2500 migracion_desde_sqlite.py\n\u251c\u2500\u2500 comparativa_sqlite_vs_postgres.md\n\u2514\u2500\u2500 capturas/\n    \u251c\u2500\u2500 sqlite_query.png\n    \u2514\u2500\u2500 postgres_query.png\n</code></pre></p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#herramientas-de-migracion","title":"Herramientas de Migraci\u00f3n","text":"<ul> <li>pgloader - Migraci\u00f3n autom\u00e1tica</li> <li>SQLite to PostgreSQL Converter</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#documentacion","title":"Documentaci\u00f3n","text":"<ul> <li>PostgreSQL Data Types</li> <li>Migraci\u00f3n de Bases de Datos</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#tutoriales","title":"Tutoriales","text":"<ul> <li>Comparativa SQLite vs PostgreSQL</li> <li>Psycopg2 Tutorial</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 3.1 - Oracle con BD HR</li> <li>Ejercicio 4.1 - SQL Server con Tienda</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/05-nlp-mining/","title":"Ejercicio 05: Miner\u00eda de Texto y NLP","text":"<p>Introducci\u00f3n al procesamiento de datos no estructurados mediante t\u00e9cnicas de Natural Language Processing.</p>"},{"location":"ejercicios/05-nlp-mining/#conceptos-clave","title":"Conceptos Clave","text":"<ul> <li>Tokenizaci\u00f3n: Dividir texto en unidades procesables.</li> <li>Stopwords: Eliminaci\u00f3n de ruido (art\u00edculos, preposiciones).</li> <li>Similitud Jaccard: Medici\u00f3n matem\u00e1tica de qu\u00e9 tan parecidos son dos documentos.</li> <li>Sentiment Analysis: Clasificaci\u00f3n polar (Positivo/Negativo) basada en l\u00e9xicos.</li> </ul>"},{"location":"ejercicios/05-nlp-mining/#actividad-practica","title":"Actividad Pr\u00e1ctica","text":"<p>Encontrar\u00e1s los scripts en <code>ejercicios/05_nlp_text_mining/</code>.</p> <p>Reto Principal: Implementar un sistema que compare descripciones de pol\u00edticas p\u00fablicas de diferentes pa\u00edses y detecte cu\u00e1les son sem\u00e1nticamente similares utilizando la Distancia de Jaccard.</p>"},{"location":"ejercicios/05-oracle-hr/","title":"Ejercicio 3.1: Oracle con Base de Datos HR","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/05-oracle-hr/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Trabajar\u00e1s con Oracle Database usando la base de datos HR en su entorno nativo original.</p> <p>Duraci\u00f3n estimada: 5-7 horas Nivel: Avanzado Prerequisitos: Ejercicio 2.1 (PostgreSQL HR)</p>"},{"location":"ejercicios/05-oracle-hr/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Instalar y configurar Oracle Database Express Edition (XE)</li> <li>\u2705 Usar SQL Developer o SQL*Plus</li> <li>\u2705 Trabajar con sintaxis espec\u00edfica de Oracle</li> <li>\u2705 Crear secuencias y triggers</li> <li>\u2705 Escribir procedimientos almacenados en PL/SQL</li> <li>\u2705 Comparar Oracle con PostgreSQL</li> <li>\u2705 Entender caracter\u00edsticas enterprise de Oracle</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#oracle-database","title":"Oracle Database","text":"<p>Oracle es el motor de bases de datos relacional l\u00edder en el mercado enterprise.</p> <p>Caracter\u00edsticas: - PL/SQL (lenguaje procedural) - Particionamiento avanzado - Replicaci\u00f3n y alta disponibilidad - Seguridad enterprise - Optimizador de consultas muy potente</p>"},{"location":"ejercicios/05-oracle-hr/#diferencias-oracle-vs-postgresql","title":"Diferencias Oracle vs PostgreSQL","text":""},{"location":"ejercicios/05-oracle-hr/#sintaxis","title":"Sintaxis","text":"Aspecto Oracle PostgreSQL Auto-increment SEQUENCE SERIAL String concat <code>\\|\\|</code> o <code>CONCAT()</code> <code>\\|\\|</code> Tipos VARCHAR <code>VARCHAR2</code> <code>VARCHAR</code> LIMIT <code>ROWNUM</code> o <code>FETCH FIRST</code> <code>LIMIT</code> Outer Join <code>(+)</code> (legacy) <code>LEFT/RIGHT JOIN</code>"},{"location":"ejercicios/05-oracle-hr/#funcionalidad","title":"Funcionalidad","text":"<ul> <li>PL/SQL vs PL/pgSQL: Oracle tiene PL/SQL m\u00e1s maduro</li> <li>Packages: Oracle soporta packages (agrupaci\u00f3n de procedimientos)</li> <li>Triggers: Sintaxis diferente</li> <li>Performance: Oracle optimizado para cargas enterprise</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/3.1_oracle_hr/\n</code></pre>"},{"location":"ejercicios/05-oracle-hr/#datos","title":"Datos","text":"<p>Scripts SQL originales de Oracle est\u00e1n en: <code>datos/oracle_hr/</code></p>"},{"location":"ejercicios/05-oracle-hr/#temas-cubiertos","title":"Temas Cubiertos","text":""},{"location":"ejercicios/05-oracle-hr/#1-instalacion-y-configuracion","title":"1. Instalaci\u00f3n y Configuraci\u00f3n","text":"<ul> <li>Instalar Oracle XE 21c</li> <li>Configurar listener</li> <li>Crear usuarios y permisos</li> <li>Conectar con SQL Developer</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#2-sintaxis-oracle","title":"2. Sintaxis Oracle","text":"<ul> <li>Tipos de datos espec\u00edficos</li> <li>Funciones built-in de Oracle</li> <li>ROWNUM y paginaci\u00f3n</li> <li>Hints del optimizador</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#3-plsql-basico","title":"3. PL/SQL B\u00e1sico","text":"<ul> <li>Bloques an\u00f3nimos</li> <li>Variables y tipos</li> <li>Estructuras de control</li> <li>Manejo de excepciones</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#4-objetos-de-base-de-datos","title":"4. Objetos de Base de Datos","text":"<ul> <li>Secuencias</li> <li>Triggers</li> <li>Vistas</li> <li>Sin\u00f3nimos</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#5-procedimientos-y-funciones","title":"5. Procedimientos y Funciones","text":"<ul> <li>Crear procedimientos almacenados</li> <li>Par\u00e1metros IN/OUT/IN OUT</li> <li>Funciones que retornan valores</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/05-oracle-hr/#software-necesario","title":"Software Necesario","text":"<ol> <li>Oracle Database 21c Express Edition (XE) - Gratuito</li> <li>Descargar Oracle XE</li> <li>Requiere cuenta Oracle (gratuita)</li> <li> <p>Limitaciones XE: 12GB RAM, 2 CPUs, 12GB datos de usuario</p> </li> <li> <p>SQL Developer - Cliente gr\u00e1fico oficial de Oracle</p> </li> <li>Descargar SQL Developer</li> <li> <p>Alternativa: DBeaver con driver Oracle</p> </li> <li> <p>Oracle Instant Client (opcional para conexiones remotas)</p> </li> </ol>"},{"location":"ejercicios/05-oracle-hr/#sistema-operativo","title":"Sistema Operativo","text":"<ul> <li>Windows: Instalaci\u00f3n directa</li> <li>Mac/Linux: Usar Docker   <pre><code>docker pull container-registry.oracle.com/database/express:21.3.0-xe\n</code></pre></li> </ul>"},{"location":"ejercicios/05-oracle-hr/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/3.1_oracle_hr/\n</code></pre></p>"},{"location":"ejercicios/05-oracle-hr/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/05-oracle-hr/#documentacion-oficial","title":"Documentaci\u00f3n Oficial","text":"<ul> <li>Oracle Database Documentation</li> <li>PL/SQL Language Reference</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#tutoriales","title":"Tutoriales","text":"<ul> <li>Oracle Live SQL - Pr\u00e1ctica online gratuita</li> <li>PL/SQL Tutorial</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#comparativas","title":"Comparativas","text":"<ul> <li>Oracle vs PostgreSQL</li> <li>Migraci\u00f3n Oracle \u2192 PostgreSQL</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 3.2 - Oracle Jardiner\u00eda (m\u00e1s pr\u00e1ctica con Oracle)</li> <li>Ejercicio 4.1 - SQL Server (otro motor enterprise)</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/06-analisis-excel-python/","title":"Ejercicio 5.1: An\u00e1lisis de Datos con Excel y Python","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/06-analisis-excel-python/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Aprender\u00e1s a analizar datos de Excel usando Python, comparando el an\u00e1lisis manual vs automatizado.</p> <p>Duraci\u00f3n estimada: 3-4 horas Nivel: B\u00e1sico-Intermedio Prerequisitos: Python b\u00e1sico, pandas</p>"},{"location":"ejercicios/06-analisis-excel-python/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Leer archivos Excel con pandas y openpyxl</li> <li>\u2705 Realizar an\u00e1lisis exploratorio de datos (EDA)</li> <li>\u2705 Generar estad\u00edsticas descriptivas</li> <li>\u2705 Crear visualizaciones (gr\u00e1ficos)</li> <li>\u2705 Automatizar an\u00e1lisis que har\u00edas manualmente en Excel</li> <li>\u2705 Exportar resultados a Excel formateado</li> <li>\u2705 Comparar an\u00e1lisis manual vs program\u00e1tico</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#archivo-de-datos","title":"Archivo de Datos","text":"<p>Trabajar\u00e1s con: <code>datos/Ejercicio-de-Excel-resuelto-nivel-medio.xlsx</code></p> <p>Este archivo contiene datos reales que normalmente analizar\u00edas en Excel.</p>"},{"location":"ejercicios/06-analisis-excel-python/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/5.1_analisis_excel/\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#archivos","title":"Archivos","text":"<ul> <li><code>analisis_exploratorio.py</code> - Plantilla de script</li> <li><code>INSTRUCCIONES.md</code> - Gu\u00eda paso a paso</li> <li><code>informe_analisis.md</code> - Plantilla para tu informe</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#proceso-de-analisis","title":"Proceso de An\u00e1lisis","text":""},{"location":"ejercicios/06-analisis-excel-python/#1-exploracion-inicial","title":"1. Exploraci\u00f3n Inicial","text":"<pre><code>import pandas as pd\n\n# Leer Excel\ndf = pd.read_excel('datos/Ejercicio-de-Excel-resuelto-nivel-medio.xlsx')\n\n# Ver estructura\nprint(df.info())\nprint(df.describe())\nprint(df.head())\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#2-limpieza-de-datos","title":"2. Limpieza de Datos","text":"<ul> <li>Detectar valores nulos</li> <li>Corregir tipos de datos</li> <li>Eliminar duplicados</li> <li>Normalizar texto</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#3-estadisticas-descriptivas","title":"3. Estad\u00edsticas Descriptivas","text":"<ul> <li>Medidas de tendencia central (media, mediana)</li> <li>Dispersi\u00f3n (desviaci\u00f3n est\u00e1ndar, cuartiles)</li> <li>Correlaciones</li> <li>Distribuciones</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#4-visualizaciones","title":"4. Visualizaciones","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Gr\u00e1fico de barras\ndf.groupby('categoria')['ventas'].sum().plot(kind='bar')\n\n# Histograma\ndf['precio'].hist(bins=20)\n\n# Heatmap de correlaci\u00f3n\nsns.heatmap(df.corr(), annot=True)\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#5-exportar-resultados","title":"5. Exportar Resultados","text":"<pre><code># Crear Excel con formato\nwith pd.ExcelWriter('analisis_resultados.xlsx', engine='openpyxl') as writer:\n    df_resumen.to_excel(writer, sheet_name='Resumen')\n    df_detalle.to_excel(writer, sheet_name='Detalle')\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#tareas-a-realizar","title":"Tareas a Realizar","text":""},{"location":"ejercicios/06-analisis-excel-python/#parte-1-analisis-exploratorio","title":"Parte 1: An\u00e1lisis Exploratorio","text":"<p>Crear <code>analisis_exploratorio.py</code> que:</p> <ol> <li>Lee el archivo Excel</li> <li>Realiza EDA completo</li> <li>Genera estad\u00edsticas descriptivas</li> <li>Crea visualizaciones</li> <li>Exporta resultados a Excel formateado</li> </ol>"},{"location":"ejercicios/06-analisis-excel-python/#parte-2-informe-de-analisis","title":"Parte 2: Informe de An\u00e1lisis","text":"<p>Crear <code>informe_analisis.md</code> con:</p> <ol> <li>Resumen Ejecutivo</li> <li>Hallazgos principales</li> <li> <p>Datos clave</p> </li> <li> <p>An\u00e1lisis Detallado</p> </li> <li>Estructura de los datos</li> <li>Calidad de datos</li> <li> <p>Patrones encontrados</p> </li> <li> <p>Visualizaciones</p> </li> <li>Incluir gr\u00e1ficos generados</li> <li> <p>Interpretar resultados</p> </li> <li> <p>Comparaci\u00f3n Manual vs Automatizado</p> </li> <li>\u00bfQu\u00e9 es m\u00e1s r\u00e1pido?</li> <li>\u00bfQu\u00e9 es m\u00e1s preciso?</li> <li> <p>\u00bfCu\u00e1ndo usar cada uno?</p> </li> <li> <p>Conclusiones</p> </li> </ol>"},{"location":"ejercicios/06-analisis-excel-python/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/06-analisis-excel-python/#librerias-python","title":"Librer\u00edas Python","text":"<pre><code>pip install pandas openpyxl matplotlib seaborn jupyter\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#software-opcional","title":"Software Opcional","text":"<ul> <li>Excel o LibreOffice Calc (para comparar an\u00e1lisis manual)</li> <li>Jupyter Notebook (para an\u00e1lisis interactivo)</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#comparacion-excel-vs-python","title":"Comparaci\u00f3n: Excel vs Python","text":""},{"location":"ejercicios/06-analisis-excel-python/#ventajas-de-excel","title":"Ventajas de Excel","text":"<ul> <li>\u2705 Interfaz visual intuitiva</li> <li>\u2705 R\u00e1pido para an\u00e1lisis ad-hoc peque\u00f1os</li> <li>\u2705 No requiere programaci\u00f3n</li> <li>\u2705 Gr\u00e1ficos interactivos f\u00e1ciles</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#ventajas-de-python","title":"Ventajas de Python","text":"<ul> <li>\u2705 Escalable a millones de filas</li> <li>\u2705 Reproducible (script = documentaci\u00f3n)</li> <li>\u2705 Automatizable</li> <li>\u2705 M\u00e1s an\u00e1lisis estad\u00edsticos avanzados</li> <li>\u2705 Integraci\u00f3n con bases de datos</li> <li>\u2705 Control de versiones (Git)</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#cuando-usar-cada-uno","title":"\u00bfCu\u00e1ndo usar cada uno?","text":"<p>Usa Excel cuando: - Dataset peque\u00f1o (&lt; 100k filas) - An\u00e1lisis r\u00e1pido one-time - Audiencia no t\u00e9cnica</p> <p>Usa Python cuando: - Dataset grande (&gt; 100k filas) - An\u00e1lisis repetitivo - Necesitas automatizaci\u00f3n - An\u00e1lisis complejo</p>"},{"location":"ejercicios/06-analisis-excel-python/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/5.1_analisis_excel/\n\u251c\u2500\u2500 analisis_exploratorio.py\n\u251c\u2500\u2500 informe_analisis.md\n\u251c\u2500\u2500 graficos/\n\u2502   \u251c\u2500\u2500 distribucion.png\n\u2502   \u251c\u2500\u2500 correlacion.png\n\u2502   \u2514\u2500\u2500 tendencias.png\n\u2514\u2500\u2500 analisis_resultados.xlsx\n</code></pre></p>"},{"location":"ejercicios/06-analisis-excel-python/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/06-analisis-excel-python/#documentacion","title":"Documentaci\u00f3n","text":"<ul> <li>Pandas Documentation</li> <li>Openpyxl Documentation</li> <li>Matplotlib Gallery</li> <li>Seaborn Examples</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#tutoriales","title":"Tutoriales","text":"<ul> <li>Pandas Tutorial</li> <li>Excel con Python</li> <li>An\u00e1lisis Exploratorio con Pandas</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#videos","title":"Videos","text":"<ul> <li>An\u00e1lisis de Datos con Pandas</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio, habr\u00e1s cubierto:</p> <ul> <li>Bases de datos (SQLite, PostgreSQL, Oracle, SQL Server)</li> <li>An\u00e1lisis de datos (Python + Excel)</li> </ul> <p>Siguiente nivel: Big Data con PySpark, Dask, etc.</p> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/06-trabajo-final-capstone/","title":"Trabajo Final: Pipeline de Big Data con Infraestructura Docker","text":"<p>Curso: Big Data con Python - Prof. Juan Marcelo Gutierrez Miranda (@TodoEconometria)</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#objetivo","title":"Objetivo","text":"<p>Construir desde cero una infraestructura de procesamiento de datos usando Docker, Apache Spark y PostgreSQL. A partir del dataset Quality of Government (QoG), disenar y ejecutar un pipeline ETL + analisis que responda una pregunta de investigacion formulada por ti.</p> <p>Lo que se evalua: No solo el codigo, sino tu proceso de aprendizaje. Puedes usar herramientas de IA (ChatGPT, Copilot, Claude, etc.) pero debes documentar como las usaste y que aprendiste.</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#dataset","title":"Dataset","text":"<p>Quality of Government Standard Dataset (QoG) - Enero 2024</p> <ul> <li>~15,500 filas (paises x anios) x ~1,990 columnas</li> <li>Variables: democracia, corrupcion, PIB, salud, educacion, estabilidad politica...</li> <li>Documentacion: QoG Data</li> </ul>"},{"location":"ejercicios/06-trabajo-final-capstone/#estructura-4-bloques","title":"Estructura: 4 Bloques","text":""},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-a-infraestructura-docker-30","title":"Bloque A: Infraestructura Docker (30%)","text":"<p>Escribir un <code>docker-compose.yml</code> que levante un mini-cluster:</p> Servicio Requisito minimo PostgreSQL Base de datos para almacenar resultados Spark Master Coordinador del cluster Spark Worker Al menos 1 nodo de procesamiento <p>Pasos:</p> <ol> <li>Investiga que es Docker Compose y como se estructura un archivo YAML</li> <li>Escribe tu <code>docker-compose.yml</code> con los 3 servicios minimos</li> <li>Agrega <code>healthcheck</code> al menos para PostgreSQL</li> <li>Ejecuta <code>docker compose up -d</code> y verifica que todo arranca</li> <li>Abre el Spark UI y toma una captura de pantalla mostrando el worker conectado</li> <li>Escribe <code>02_INFRAESTRUCTURA.md</code> explicando cada seccion de tu YAML con tus palabras</li> </ol> <p>Pistas:</p> <ul> <li>Imagen Spark: <code>apache/spark:3.5.4-python3</code> (o <code>bitnami/spark:3.5</code>)</li> <li>Imagen PostgreSQL: <code>postgres:15-alpine</code></li> <li>El Master de Spark usa el puerto 7077 para comunicacion y 8080 para la UI web</li> </ul> <p>Entregables: <code>docker-compose.yml</code> + <code>02_INFRAESTRUCTURA.md</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-b-pipeline-etl-con-spark-25","title":"Bloque B: Pipeline ETL con Spark (25%)","text":"<p>Escribir un script Python que procese QoG usando Apache Spark.</p> <p>Pasos:</p> <ol> <li>Elige 5 paises que te interesen (no pueden ser los del ejemplo del profesor: KAZ, UZB, TKM, KGZ, TJK)</li> <li>Elige 5 variables numericas del dataset QoG</li> <li>Formula una pregunta de investigacion</li> <li>Escribe <code>pipeline.py</code> que:<ul> <li>Cree una SparkSession</li> <li>Lea el CSV con <code>spark.read.csv()</code></li> <li>Seleccione tus paises y variables</li> <li>Filtre un rango de anios (ej: 2000-2023)</li> <li>Cree al menos 1 variable derivada</li> <li>Guarde el resultado como Parquet</li> </ul> </li> </ol> <p>Importante: Tu seleccion de paises y variables debe ser UNICA. Si dos alumnos entregan los mismos 5 paises, se considerara copia.</p> <p>Entregable: <code>pipeline.py</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-c-analisis-y-visualizacion-25","title":"Bloque C: Analisis y Visualizacion (25%)","text":"<p>Analizar tus datos procesados y responder tu pregunta de investigacion.</p> <p>Elige UNA opcion:</p> Opcion Que hacer Ejemplo Clustering K-Means sobre tus paises \"Que paises se parecen segun democracia + PIB?\" Serie temporal Grafico de evolucion por pais \"Como cambio la corrupcion entre 2000-2023?\" Comparacion Antes/despues de un evento \"Cambio el PIB tras la crisis de 2008?\" <p>Requisitos minimos:</p> <ul> <li>2 graficos (matplotlib, plotly, o seaborn)</li> <li>Cada grafico con titulo, ejes etiquetados, y leyenda</li> <li>Parrafo de interpretacion por cada grafico</li> </ul> <p>Entregable: Graficos e interpretacion en <code>03_RESULTADOS.md</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-d-reflexion-ia-3-momentos-clave-20","title":"Bloque D: Reflexion IA - \"3 Momentos Clave\" (20%)","text":"<p>Documentar tu proceso de aprendizaje y compartir tus prompts.</p> <p>Para cada bloque (A, B, C), responde:</p> Momento Pregunta Arranque Que fue lo primero que le pediste a la IA (o buscaste)? Error Que fallo y como lo resolviste? Aprendizaje Que aprendiste que NO sabias antes? <p>Ademas, pega el texto exacto del prompt de IA que mas te ayudo en cada bloque y adjunta 1 captura de pantalla del prompt mas util.</p> <p>Se evalua:</p> <ul> <li>Que tus prompts sean reales (pegados tal cual, no inventados despues)</li> <li>Que tus respuestas sean especificas</li> <li>Que los errores sean reales (documentarlos no baja nota)</li> <li>Que el proceso sea coherente con tu codigo</li> </ul> <p>Entregable: <code>04_REFLEXION_IA.md</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#preguntas-de-comprension-obligatorias","title":"Preguntas de Comprension (obligatorias)","text":"<p>Responde en <code>05_RESPUESTAS.md</code>:</p> <ol> <li>Infraestructura: Si tu worker tiene 2 GB de RAM y el CSV pesa 3 GB, que pasa? Como lo solucionarias?</li> <li>ETL: Por que <code>spark.read.csv()</code> no ejecuta nada hasta que llamas <code>.count()</code> o <code>.show()</code>?</li> <li>Analisis: Interpreta tu grafico principal: que patron ves y por que crees que ocurre?</li> <li>Escalabilidad: Si tuvieras que repetir con un dataset de 50 GB, que cambiarias en tu infraestructura?</li> </ol>"},{"location":"ejercicios/06-trabajo-final-capstone/#formato-de-entrega","title":"Formato de Entrega","text":"<pre><code>entregas/trabajo_final/apellido_nombre/\n    PROMPTS.md                 &lt;- LO MAS IMPORTANTE (tus prompts de IA)\n    01_README.md               &lt;- Tus datos + pregunta de investigacion\n    02_INFRAESTRUCTURA.md      &lt;- Explicacion YAML + captura Spark UI\n    03_RESULTADOS.md           &lt;- Graficos + interpretacion\n    04_REFLEXION_IA.md         &lt;- 3 Momentos Clave x 3 bloques\n    05_RESPUESTAS.md           &lt;- 4 preguntas de comprension\n    docker-compose.yml         &lt;- Tu YAML funcional\n    pipeline.py                &lt;- ETL + Analisis\n    requirements.txt           &lt;- Dependencias (pip freeze)\n    capturas/                  &lt;- Capturas obligatorias (prompt_A.png, prompt_B.png, prompt_C.png)\n    .gitignore                 &lt;- Excluir datos, venv, __pycache__\n</code></pre> <p>Copia la plantilla desde <code>trabajo_final/plantilla/</code> a tu carpeta de entrega.</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#proceso-sin-pull-request","title":"Proceso (SIN Pull Request)","text":"<ol> <li>Sincroniza tu fork: <code>git fetch upstream &amp;&amp; git merge upstream/main</code></li> <li>Copia la plantilla: <code>cp -r trabajo_final/plantilla/ entregas/trabajo_final/apellido_nombre/</code></li> <li>Completa PROMPTS.md mientras trabajas - Este archivo es lo que se evalua</li> <li>Completa los archivos (01 al 05) + <code>docker-compose.yml</code> + <code>pipeline.py</code></li> <li>Sube a tu fork: <code>git add . &amp;&amp; git commit -m \"Trabajo Final\" &amp;&amp; git push</code></li> <li>Listo! El sistema evalua tu PROMPTS.md automaticamente</li> </ol> <p>No necesitas crear Pull Request</p> <p>El sistema automatico evalua tu archivo PROMPTS.md directamente en tu fork. Solo asegurate de subir tu trabajo con <code>git push</code>.</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#prohibido-incluir","title":"Prohibido incluir","text":"<ul> <li>Archivos de datos (.csv, .parquet, .db)</li> <li>Entornos virtuales (venv/, .venv/)</li> <li>Archivos .env con credenciales reales</li> <li>Carpetas pycache/</li> </ul>"},{"location":"ejercicios/06-trabajo-final-capstone/#evaluacion","title":"Evaluacion","text":"Bloque Peso Que se evalua A. Infraestructura 30% YAML funcional + explicacion con tus palabras B. Pipeline ETL 25% Spark API + paises/variables propios + pregunta C. Analisis 25% Graficos + interpretacion que responda tu pregunta D. Reflexion IA 20% Proceso de aprendizaje real y especifico <p>Penalizaciones:</p> <ul> <li>Copiar los mismos paises/variables que otro alumno: -50%</li> <li>Copiar los paises del ejemplo del profesor (Asia Central): -30%</li> <li>YAML que no funciona sin explicacion de por que: -15%</li> <li>Reflexion IA ausente o generica: -20%</li> </ul>"},{"location":"ejercicios/06-trabajo-final-capstone/#recursos","title":"Recursos","text":"<ul> <li>Spark Documentation: spark.apache.org</li> <li>Docker Compose: docs.docker.com/compose</li> <li>QoG Codebook: qog.pol.gu.se (descargar codebook para ver variables)</li> <li>Guia de Inicio Rapido: <code>trabajo_final/GUIA_INICIO_RAPIDO.md</code></li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.</li> <li>Teorell, J., et al. (2024). The Quality of Government Standard Dataset. University of Gothenburg.</li> <li>Merkel, D. (2014). Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux Journal, 2014(239), 2.</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/","title":"Series Temporales: ARIMA/SARIMA con Metodologia Box-Jenkins","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/07-series-temporales-arima/#descripcion-general","title":"Descripcion General","text":"<p>Aprenderemos a modelar series temporales utilizando la Metodologia Box-Jenkins completa: identificacion, estimacion, diagnostico y pronostico. Trabajaremos con modelos ARIMA y SARIMA para capturar tanto tendencias como estacionalidad.</p> <p>Nivel: Avanzado Dataset: AirPassengers (144 observaciones mensuales, 1949-1960) Tecnologias: Python, statsmodels, matplotlib</p>"},{"location":"ejercicios/07-series-temporales-arima/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>Comprender la Metodologia Box-Jenkins (4 fases)</li> <li>Identificar componentes de una serie: tendencia, estacionalidad, ruido</li> <li>Usar ACF y PACF para determinar ordenes p, d, q</li> <li>Estimar modelos ARIMA y SARIMA</li> <li>Diagnosticar residuos (Ljung-Box, normalidad)</li> <li>Generar pronosticos con intervalos de confianza</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo esta en:</p> <pre><code>ejercicios/04_machine_learning/07_series_temporales_arima/\n\u251c\u2500\u2500 README.md                    # Teoria Box-Jenkins completa (829 lineas)\n\u251c\u2500\u2500 serie_temporal_completa.py   # Script 10 partes (1,286 lineas)\n\u251c\u2500\u2500 output/                      # Directorio para graficos generados\n\u2514\u2500\u2500 .gitignore                   # Excluye output/*.png y *.csv\n</code></pre>"},{"location":"ejercicios/07-series-temporales-arima/#el-script-serie_temporal_completapy-cubre","title":"El script <code>serie_temporal_completa.py</code> cubre:","text":"<ol> <li>Carga y visualizacion de la serie original</li> <li>Descomposicion (tendencia + estacionalidad + residuo)</li> <li>Tests de estacionariedad (ADF, KPSS)</li> <li>Diferenciacion regular y estacional</li> <li>ACF/PACF para identificacion de ordenes</li> <li>Estimacion ARIMA con seleccion por AIC</li> <li>Estimacion SARIMA con componente estacional</li> <li>Diagnostico de residuos (Ljung-Box, QQ-plot, ACF residuos)</li> <li>Pronostico con intervalos de confianza</li> <li>Comparacion de modelos y metricas (MAPE, RMSE)</li> </ol>"},{"location":"ejercicios/07-series-temporales-arima/#teoria-metodologia-box-jenkins","title":"Teoria: Metodologia Box-Jenkins","text":""},{"location":"ejercicios/07-series-temporales-arima/#fase-1-identificacion","title":"Fase 1: Identificacion","text":"<ul> <li>Visualizar la serie y detectar tendencia/estacionalidad</li> <li>Aplicar diferenciacion para lograr estacionariedad</li> <li>Analizar ACF y PACF para determinar ordenes (p, d, q)</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#fase-2-estimacion","title":"Fase 2: Estimacion","text":"<ul> <li>Ajustar modelo ARIMA(p,d,q) o SARIMA(p,d,q)(P,D,Q)[s]</li> <li>Comparar modelos candidatos usando AIC/BIC</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#fase-3-diagnostico","title":"Fase 3: Diagnostico","text":"<ul> <li>Verificar que los residuos sean ruido blanco</li> <li>Test de Ljung-Box (autocorrelacion)</li> <li>Test de normalidad</li> <li>Grafico QQ</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#fase-4-pronostico","title":"Fase 4: Pronostico","text":"<ul> <li>Generar predicciones con intervalos de confianza</li> <li>Evaluar precision con metricas (MAPE, RMSE)</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#resultado-del-ejercicio","title":"Resultado del Ejercicio","text":"<p>Modelo seleccionado: SARIMA(1,1,0)(0,1,0)[12]</p> <ul> <li>AIC: -445.41</li> <li>MAPE: 7.41%</li> <li>Captura correctamente tendencia y estacionalidad mensual</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#dashboard-interactivo","title":"Dashboard Interactivo","text":"<p>Puedes explorar los resultados en el dashboard interactivo:</p> <p>Ver Dashboard ARIMA/SARIMA</p> <p>El dashboard incluye 6 pestanas con graficos Plotly interactivos: serie original, descomposicion, estacionariedad, ACF/PACF, diagnostico y pronostico.</p>"},{"location":"ejercicios/07-series-temporales-arima/#recursos","title":"Recursos","text":"<ul> <li>statsmodels ARIMA Documentation</li> <li>statsmodels SARIMAX</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Box, G. E. P., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control (5<sup>th</sup> ed.). Wiley.</li> <li>Hyndman, R. J., &amp; Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3<sup>rd</sup> ed.). OTexts.</li> <li>Hamilton, J. D. (1994). Time Series Analysis. Princeton University Press.</li> </ul>"},{"location":"ejercicios/08-panel-data/","title":"Modulo 06: Analisis de Datos de Panel","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/08-panel-data/#descripcion-general","title":"Descripcion General","text":"<p>Aprenderemos econometria de datos de panel: la combinacion de datos de corte transversal (paises, individuos) con series temporales (anios). Trabajaremos con modelos de Efectos Fijos, Efectos Aleatorios y Two-Way Fixed Effects aplicados a problemas reales de ciencias sociales.</p> <p>Nivel: Avanzado Tecnologias: Python, linearmodels, pandas, Altair Prerequisitos: Estadistica basica, regresion lineal</p>"},{"location":"ejercicios/08-panel-data/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>Comprender la estructura de datos de panel (unidad x tiempo)</li> <li>Distinguir entre Pooled OLS, Efectos Fijos y Efectos Aleatorios</li> <li>Aplicar el Test de Hausman para elegir entre FE y RE</li> <li>Implementar Two-Way Fixed Effects (efectos de unidad + tiempo)</li> <li>Interpretar Odds Ratios en modelos logisticos</li> <li>Calcular efectos marginales en modelos no lineales</li> </ul>"},{"location":"ejercicios/08-panel-data/#contenido-del-modulo","title":"Contenido del Modulo","text":"<p>El modulo completo esta en:</p> <pre><code>ejercicios/06_analisis_datos_de_panel/\n\u251c\u2500\u2500 01_analisis_guns.py              # Panel: leyes de armas y criminalidad\n\u251c\u2500\u2500 02_analisis_fatality.py          # TWFE: impuesto cerveza vs mortalidad\n\u251c\u2500\u2500 03_dashboard_educativo.py        # Dashboard interactivo 4 pestanas\n\u251c\u2500\u2500 conceptos_visuales_panel.py      # Visualizaciones conceptuales\n\u251c\u2500\u2500 GUIA_PANEL_DATA.md               # Guia teorica completa\n\u251c\u2500\u2500 grafico_panel_guns.png           # Resultado del analisis\n\u2514\u2500\u2500 requirements.txt                 # linearmodels, altair, etc.\n</code></pre>"},{"location":"ejercicios/08-panel-data/#ejercicios-practicos","title":"Ejercicios Practicos","text":""},{"location":"ejercicios/08-panel-data/#01-analisis-guns-leyes-de-armas-y-criminalidad","title":"01 - Analisis Guns: Leyes de Armas y Criminalidad","text":"<p>Pregunta: Las leyes de portacion de armas reducen la criminalidad violenta?</p> <ul> <li>Dataset: Guns (Stock &amp; Watson) - 50 estados de EE.UU., 1977-1999</li> <li>Variable dependiente: <code>log(violent)</code> - tasa de criminalidad violenta (logaritmo)</li> <li>Variable clave: <code>law</code> - si el estado tiene ley \"shall-carry\" (portacion obligatoria)</li> <li>Controles: ingreso, poblacion, densidad</li> <li>Modelos: Pooled OLS vs Fixed Effects vs Random Effects</li> <li>Metodologia: Comparacion de los 3 modelos + Test de Hausman</li> </ul> <pre><code>python ejercicios/06_analisis_datos_de_panel/01_analisis_guns.py\n</code></pre>"},{"location":"ejercicios/08-panel-data/#02-analisis-fatality-impuesto-a-la-cerveza-y-mortalidad","title":"02 - Analisis Fatality: Impuesto a la Cerveza y Mortalidad","text":"<p>Pregunta: Subir el impuesto a la cerveza reduce las muertes por accidentes de trafico?</p> <ul> <li>Dataset: Fatalities (AER) - 48 estados, 1982-1988</li> <li>Variable dependiente: <code>fatality_rate</code> - muertes por 10,000 habitantes</li> <li>Variable clave: <code>beertax</code> - impuesto a la cerveza</li> <li>Controles: edad minima para beber (<code>drinkage</code>), desempleo, ingreso</li> <li>Modelos: Entity FE vs Two-Way Fixed Effects (estado + anio)</li> <li>Innovacion: TWFE controla tendencias temporales (coches mas seguros cada anio)</li> </ul> <pre><code>python ejercicios/06_analisis_datos_de_panel/02_analisis_fatality.py\n</code></pre>"},{"location":"ejercicios/08-panel-data/#03-dashboard-educativo-interactivo-panel-altair","title":"03 - Dashboard Educativo Interactivo (Panel + Altair)","text":"<p>Dashboard local con 4 pestanas interactivas para explorar los conceptos visualmente:</p> <ol> <li>Pooled OLS: Slider de heterogeneidad que muestra la Paradoja de Simpson en accion</li> <li>FE vs RE: Explicacion y tabla de decision del Test de Hausman</li> <li>Odds Ratios: Sliders para explorar probabilidad vs odds vs odds ratio en tiempo real</li> <li>Efectos Marginales: Comparacion Lin-Lin, Log-Lin, Log-Log con graficos dinamicos</li> </ol> <pre><code>panel serve ejercicios/06_analisis_datos_de_panel/03_dashboard_educativo.py\n# Abrir http://localhost:5006 en el navegador\n</code></pre>"},{"location":"ejercicios/08-panel-data/#archivos-adicionales","title":"Archivos adicionales","text":"<ul> <li><code>conceptos_visuales_panel.py</code> - Genera graficos estaticos explicando los conceptos de panel</li> <li><code>dashboard_educativo_panel.py</code> - Version alternativa del dashboard educativo</li> <li><code>GUIA_PANEL_DATA.md</code> - Guia teorica completa: Pooled OLS, FE, RE, Hausman, TWFE</li> </ul>"},{"location":"ejercicios/08-panel-data/#teoria-conceptos-clave","title":"Teoria: Conceptos Clave","text":""},{"location":"ejercicios/08-panel-data/#que-son-los-datos-de-panel","title":"Que son los datos de panel?","text":"<p>Datos que combinan corte transversal (N unidades) con serie temporal (T periodos):</p> <pre><code>| pais | anio | democracia | pib_pc |\n|------|------|------------|--------|\n| ESP  | 2000 | 0.85       | 24000  |\n| ESP  | 2001 | 0.86       | 24500  |\n| FRA  | 2000 | 0.88       | 28000  |\n| FRA  | 2001 | 0.89       | 28500  |\n</code></pre>"},{"location":"ejercicios/08-panel-data/#pooled-ols-vs-fixed-effects-vs-random-effects","title":"Pooled OLS vs Fixed Effects vs Random Effects","text":"Modelo Supuesto Cuando usarlo Pooled OLS No hay heterogeneidad individual Rara vez apropiado Fixed Effects Heterogeneidad correlacionada con X Ciencias sociales (regla general) Random Effects Heterogeneidad NO correlacionada con X Encuestas aleatorias"},{"location":"ejercicios/08-panel-data/#test-de-hausman","title":"Test de Hausman","text":"<p>Decide entre FE y RE:</p> <ul> <li>H0: RE es consistente y eficiente (preferir RE)</li> <li>H1: Solo FE es consistente (preferir FE)</li> <li>Si p-valor &lt; 0.05: usar Fixed Effects</li> </ul>"},{"location":"ejercicios/08-panel-data/#dashboards","title":"Dashboards","text":""},{"location":"ejercicios/08-panel-data/#dashboard-educativo-local","title":"Dashboard Educativo (local)","text":"<p>El dashboard principal de este modulo se ejecuta localmente con Panel (HoloViz):</p> <pre><code>panel serve ejercicios/06_analisis_datos_de_panel/03_dashboard_educativo.py\n</code></pre> <p>Incluye 4 pestanas interactivas: Pooled OLS, FE vs RE, Odds Ratios, Efectos Marginales.</p>"},{"location":"ejercicios/08-panel-data/#dashboard-qog-analisis-avanzado-github-pages","title":"Dashboard QoG - Analisis Avanzado (GitHub Pages)","text":"<p>Como complemento, puedes explorar un dashboard con analisis de panel aplicado al dataset QoG (4 lineas de investigacion con Spark + PostgreSQL + ML):</p> <p>Ver Dashboard QoG - Panel Data Aplicado</p>"},{"location":"ejercicios/08-panel-data/#recursos","title":"Recursos","text":""},{"location":"ejercicios/08-panel-data/#documentacion","title":"Documentacion","text":"<ul> <li>linearmodels Documentation</li> <li>Altair Documentation</li> </ul>"},{"location":"ejercicios/08-panel-data/#referencias-teoricas","title":"Referencias Teoricas","text":"<ul> <li>Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (2<sup>nd</sup> ed.). MIT Press.</li> <li>Stock, J. H., &amp; Watson, M. W. (2019). Introduction to Econometrics (4<sup>th</sup> ed.). Pearson.</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (2<sup>nd</sup> ed.). MIT Press.</li> <li>Stock, J. H., &amp; Watson, M. W. (2019). Introduction to Econometrics (4<sup>th</sup> ed.). Pearson.</li> <li>Baltagi, B. H. (2021). Econometric Analysis of Panel Data (6<sup>th</sup> ed.). Springer.</li> </ul>"},{"location":"entregas/guia-entregas/","title":"Como Entregar tus Trabajos","text":"<p>Esta guia te explica paso a paso como entregar. No necesitas saber Git avanzado.</p>"},{"location":"entregas/guia-entregas/#resumen-en-30-segundos","title":"Resumen en 30 segundos","text":"<pre><code>1. Haces fork del repo (solo una vez)\n2. Trabajas en TU fork\n3. Subes tus cambios a TU fork\n4. El profesor revisa TU fork automaticamente (sin PR)\n</code></pre> <p>NO necesitas crear Pull Request. El sistema automatico evalua tu archivo PROMPTS.md directamente en tu fork.</p>"},{"location":"entregas/guia-entregas/#diagrama-del-flujo","title":"Diagrama del Flujo","text":"<pre><code>flowchart TB\n    subgraph Profesor[\"REPOSITORIO DEL PROFESOR\"]\n        P1[\"github.com/TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;Aqui estan los ejercicios&lt;br/&gt;NO modificas nada aqui\"]\n    end\n\n    subgraph Alumno[\"TU FORK (tu copia)\"]\n        A1[\"github.com/TU_USUARIO/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;Aqui trabajas y subes tu codigo\"]\n    end\n\n    subgraph Local[\"TU PC\"]\n        L1[\"Carpeta ejercicios-bigdata/&lt;br/&gt;&lt;br/&gt;Aqui programas\"]\n    end\n\n    subgraph Sistema[\"SISTEMA AUTOMATICO\"]\n        S1[\"Script del profesor&lt;br/&gt;&lt;br/&gt;Revisa todos los forks&lt;br/&gt;Genera notas automaticas\"]\n    end\n\n    Profesor --&gt;|\"1. Fork (copiar)\"| Alumno\n    Alumno --&gt;|\"2. Clone (descargar)\"| Local\n    Local --&gt;|\"3. Push (subir)\"| Alumno\n    Alumno --&gt;|\"4. Revision automatica\"| Sistema\n\n    style Profesor fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Alumno fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style Local fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style Sistema fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px</code></pre>"},{"location":"entregas/guia-entregas/#paso-1-crear-tu-fork-solo-una-vez","title":"Paso 1: Crear tu Fork (solo una vez)","text":"<p>Un \"fork\" es tu copia personal del repositorio.</p> <ol> <li>Ve a: github.com/TodoEconometria/ejercicios-bigdata</li> <li>Click en el boton \"Fork\" (arriba a la derecha)</li> <li>Click en \"Create fork\"</li> <li>Listo! Ahora tienes <code>github.com/TU_USUARIO/ejercicios-bigdata</code></li> </ol> <p>Solo haces esto UNA VEZ</p> <p>Tu fork es tuyo para siempre. Todos tus trabajos van ahi.</p>"},{"location":"entregas/guia-entregas/#paso-2-descargar-a-tu-pc-solo-una-vez","title":"Paso 2: Descargar a tu PC (solo una vez)","text":"<pre><code># En tu terminal (CMD, PowerShell, o Terminal)\ncd Documentos\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\ncd ejercicios-bigdata\n</code></pre> <p>Cambia <code>TU_USUARIO</code> por tu nombre de usuario de GitHub.</p> <p>Ahora tienes la carpeta</p> <p>Busca en <code>Documentos/ejercicios-bigdata/</code>. Ahi trabajaras siempre.</p>"},{"location":"entregas/guia-entregas/#paso-3-crear-tu-carpeta-de-entrega","title":"Paso 3: Crear tu carpeta de entrega","text":"<p>Dentro de tu carpeta del repositorio, crea tu carpeta personal:</p> <pre><code>Para Trabajo Final:\nentregas/trabajo_final/apellido_nombre/\n\nPara ejercicios de BD:\nentregas/01_bases_de_datos/1.1_sqlite/apellido_nombre/\n</code></pre> <p>Formato del nombre</p> <ul> <li>Todo en minusculas</li> <li>Sin tildes ni espacios</li> <li>Formato: <code>apellido_nombre</code></li> <li>Ejemplo: <code>garcia_maria</code>, <code>lopez_juan</code></li> </ul>"},{"location":"entregas/guia-entregas/#para-el-trabajo-final-copia-la-plantilla","title":"Para el Trabajo Final, copia la plantilla:","text":"<pre><code># Desde la carpeta del repositorio:\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/tu_apellido_nombre/\n</code></pre> <p>Esto te crea todos los archivos que necesitas completar.</p>"},{"location":"entregas/guia-entregas/#paso-4-trabajar-y-documentar-tus-prompts","title":"Paso 4: Trabajar y documentar tus prompts","text":""},{"location":"entregas/guia-entregas/#el-archivo-mas-importante-promptsmd","title":"El archivo mas importante: PROMPTS.md","text":"<p>Dentro de tu carpeta encontraras <code>PROMPTS.md</code>. Este archivo es LO QUE SE EVALUA.</p> <pre><code>entregas/trabajo_final/garcia_maria/\n\u251c\u2500\u2500 PROMPTS.md          \u2190 OBLIGATORIO - Tus prompts de IA\n\u251c\u2500\u2500 docker-compose.yml  \u2190 Tu infraestructura\n\u251c\u2500\u2500 pipeline.py         \u2190 Tu codigo\n\u251c\u2500\u2500 capturas/           \u2190 Capturas de pantalla\n\u2502   \u251c\u2500\u2500 prompt_A.png\n\u2502   \u251c\u2500\u2500 prompt_B.png\n\u2502   \u2514\u2500\u2500 prompt_C.png\n\u2514\u2500\u2500 ... otros archivos\n</code></pre>"},{"location":"entregas/guia-entregas/#que-va-en-promptsmd","title":"Que va en PROMPTS.md","text":"Seccion Que poner Prompt A, B, C Tus prompts REALES copiados tal cual (con errores y todo) Capturas Screenshot de cada prompt en tu IA Blueprint Al final, pedirle a la IA un resumen profesional <p>MUY IMPORTANTE</p> <p>NO corrijas tus prompts. Si escribiste \"como ago q sparck lea el csv\" con errores, pega ESO. El sistema detecta si \"limpiaste\" tus prompts.</p> <p>Los prompts perfectos en la Parte 1 = SOSPECHOSO.</p>"},{"location":"entregas/guia-entregas/#paso-5-subir-tu-trabajo","title":"Paso 5: Subir tu trabajo","text":"<p>Cuando termines (o quieras guardar avances):</p> <pre><code># Desde la carpeta del repositorio\ngit add .\ngit commit -m \"Entrega Trabajo Final - Garcia Maria\"\ngit push\n</code></pre> <p>Que hace cada comando</p> <ul> <li><code>git add .</code> \u2192 Prepara todos tus archivos</li> <li><code>git commit -m \"...\"</code> \u2192 Guarda con un mensaje</li> <li><code>git push</code> \u2192 Sube a tu fork en GitHub</li> </ul>"},{"location":"entregas/guia-entregas/#paso-6-verificar-tu-entrega","title":"Paso 6: Verificar tu entrega","text":"<ol> <li>Ve a tu fork: <code>github.com/TU_USUARIO/ejercicios-bigdata</code></li> <li>Navega a <code>entregas/trabajo_final/tu_apellido_nombre/</code></li> <li>Verifica que estan todos tus archivos</li> </ol> <p>Listo!</p> <p>No necesitas hacer nada mas. El sistema automatico revisa tu archivo PROMPTS.md y genera notas basado en tu proceso de aprendizaje.</p>"},{"location":"entregas/guia-entregas/#mantener-tu-fork-actualizado","title":"Mantener tu Fork Actualizado","text":"<p>El profesor agrega ejercicios nuevos. Tu fork NO se actualiza solo.</p>"},{"location":"entregas/guia-entregas/#metodo-facil-desde-github","title":"Metodo Facil (desde GitHub)","text":"<ol> <li>Ve a tu fork en GitHub</li> <li>Si ves un banner amarillo \"This branch is X commits behind\", haz click</li> <li>Click en \"Sync fork\" \u2192 \"Update branch\"</li> <li>En tu PC: <code>git pull</code></li> </ol>"},{"location":"entregas/guia-entregas/#metodo-terminal","title":"Metodo Terminal","text":"<pre><code># Agregar el repo del profesor como \"upstream\" (solo una vez)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Actualizar\ngit fetch upstream\ngit merge upstream/main\ngit push\n</code></pre> <p>Cuando sincronizar</p> <p>Hazlo cada lunes antes de clase para tener los ejercicios nuevos.</p>"},{"location":"entregas/guia-entregas/#estructura-de-entrega-trabajo-final","title":"Estructura de Entrega - Trabajo Final","text":"<pre><code>entregas/trabajo_final/apellido_nombre/\n\u2502\n\u251c\u2500\u2500 PROMPTS.md              \u2190 LO MAS IMPORTANTE (se evalua esto)\n\u2502\n\u251c\u2500\u2500 01_README.md            \u2190 Tu pregunta de investigacion\n\u251c\u2500\u2500 02_INFRAESTRUCTURA.md   \u2190 Explicacion de tu Docker\n\u251c\u2500\u2500 03_RESULTADOS.md        \u2190 Graficos e interpretacion\n\u251c\u2500\u2500 04_REFLEXION_IA.md      \u2190 3 momentos clave\n\u251c\u2500\u2500 05_RESPUESTAS.md        \u2190 Preguntas de comprension\n\u2502\n\u251c\u2500\u2500 docker-compose.yml      \u2190 Tu YAML funcional\n\u251c\u2500\u2500 pipeline.py             \u2190 Tu codigo ETL + analisis\n\u251c\u2500\u2500 requirements.txt        \u2190 Dependencias\n\u2502\n\u251c\u2500\u2500 capturas/               \u2190 Capturas obligatorias\n\u2502   \u251c\u2500\u2500 prompt_A.png        \u2190 Captura del prompt A\n\u2502   \u251c\u2500\u2500 prompt_B.png        \u2190 Captura del prompt B\n\u2502   \u251c\u2500\u2500 prompt_C.png        \u2190 Captura del prompt C\n\u2502   \u2514\u2500\u2500 spark_ui.png        \u2190 Captura de Spark funcionando\n\u2502\n\u2514\u2500\u2500 .gitignore              \u2190 Excluir datos grandes\n</code></pre>"},{"location":"entregas/guia-entregas/#que-no-subir","title":"Que NO Subir","text":"<p>El <code>.gitignore</code> ya protege esto, pero recuerda:</p> <ul> <li>\u274c Archivos de datos (<code>.csv</code>, <code>.parquet</code>, <code>.db</code>)</li> <li>\u274c Carpeta <code>venv/</code> o <code>.venv/</code></li> <li>\u274c Carpeta <code>__pycache__/</code></li> <li>\u274c Archivos <code>.env</code> con credenciales</li> <li>\u274c Archivos mayores a 10MB</li> </ul>"},{"location":"entregas/guia-entregas/#como-se-evalua-sistema-por-prompts","title":"Como se Evalua (Sistema por PROMPTS)","text":"<p>LO MAS IMPORTANTE: PROMPTS.md</p> <p>El archivo PROMPTS.md es lo que se evalua. No el codigo, no el YAML, sino TUS PROMPTS de IA documentados con capturas de pantalla.</p> <p>El sistema automatico revisa:</p> <pre><code>1. Lee la lista de alumnos (forks registrados)\n2. Para cada fork:\n   - Verifica que existe PROMPTS.md (OBLIGATORIO)\n   - Analiza calidad y autenticidad de los prompts\n   - Verifica que hay capturas de pantalla (prompt_A.png, etc.)\n   - Revisa coherencia entre prompts y codigo entregado\n   - Calcula nota automatica basada en proceso de aprendizaje\n3. Genera reporte con:\n   - Ranking de todos\n   - Destacados (posible bonus)\n   - Sospechosos (requieren verificacion)\n</code></pre>"},{"location":"entregas/guia-entregas/#alertas-automaticas","title":"Alertas Automaticas","text":"Alerta Significado \u2b50 DESTACADO Trabajo excepcional, revisar para bonus \u2705 NORMAL Cumple requisitos, nota automatica \u26a0\ufe0f REVISAR Algo no cuadra, el profesor verificara \u274c RECHAZADO Copia detectada o requisitos no cumplidos"},{"location":"entregas/guia-entregas/#problemas-comunes","title":"Problemas Comunes","text":""},{"location":"entregas/guia-entregas/#no-tengo-la-plantilla","title":"\"No tengo la plantilla\"","text":"<pre><code># Actualiza tu fork primero\ngit fetch upstream\ngit merge upstream/main\n\n# Ahora copia la plantilla\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/tu_apellido/\n</code></pre>"},{"location":"entregas/guia-entregas/#git-me-pide-usuario-y-contrasena","title":"\"Git me pide usuario y contrasena\"","text":"<p>Usa tu cuenta de GitHub. Si falla, configura:</p> <pre><code>git config --global user.email \"tu@email.com\"\ngit config --global user.name \"Tu Nombre\"\n</code></pre>"},{"location":"entregas/guia-entregas/#mis-cambios-no-aparecen-en-github","title":"\"Mis cambios no aparecen en GitHub\"","text":"<p>Verifica que hiciste los 3 pasos:</p> <pre><code>git add .                    # 1. Preparar\ngit commit -m \"mensaje\"      # 2. Guardar\ngit push                     # 3. Subir  \u2190 Este es el que sube\n</code></pre>"},{"location":"entregas/guia-entregas/#quiero-empezar-de-nuevo","title":"\"Quiero empezar de nuevo\"","text":"<pre><code># Borrar tu carpeta y copiar plantilla de nuevo\nrm -rf entregas/trabajo_final/tu_apellido/\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/tu_apellido/\n</code></pre>"},{"location":"entregas/guia-entregas/#fechas-y-plazos","title":"Fechas y Plazos","text":"Entrega Fecha limite Trabajo Final [Ver calendario del curso] <p>Entregas tardias</p> <p>El sistema revisa en la fecha indicada. Lo que no este en tu fork para esa fecha, no se evalua.</p>"},{"location":"entregas/guia-entregas/#resumen-visual","title":"Resumen Visual","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    TU FLUJO DE TRABAJO                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  1. Fork (una vez)                                              \u2502\n\u2502     \u2514\u2500\u2500 Creas tu copia en GitHub                                \u2502\n\u2502                                                                 \u2502\n\u2502  2. Clone (una vez)                                             \u2502\n\u2502     \u2514\u2500\u2500 Descargas a tu PC                                       \u2502\n\u2502                                                                 \u2502\n\u2502  3. Copias plantilla                                            \u2502\n\u2502     \u2514\u2500\u2500 cp -r trabajo_final/plantilla/ entregas/.../tu_nombre/  \u2502\n\u2502                                                                 \u2502\n\u2502  4. Trabajas con IA                                             \u2502\n\u2502     \u2514\u2500\u2500 Guardas prompts en PROMPTS.md (con errores y todo)      \u2502\n\u2502                                                                 \u2502\n\u2502  5. Subes cambios                                               \u2502\n\u2502     \u2514\u2500\u2500 git add . &amp;&amp; git commit -m \"...\" &amp;&amp; git push            \u2502\n\u2502                                                                 \u2502\n\u2502  6. Verificas en GitHub                                         \u2502\n\u2502     \u2514\u2500\u2500 Confirmas que tus archivos estan ahi                    \u2502\n\u2502                                                                 \u2502\n\u2502  7. Evaluacion automatica                                       \u2502\n\u2502     \u2514\u2500\u2500 El profesor revisa todos los forks sin que hagas nada   \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"entregas/guia-entregas/#ayuda","title":"Ayuda","text":"<p>Si tienes problemas:</p> <ol> <li>Revisa esta guia de nuevo</li> <li>Pregunta a un companero</li> <li>Pregunta al profesor en clase</li> <li>Revisa la guia de sincronizacion</li> </ol> <p>Ultima actualizacion: 2026-02-04</p>"},{"location":"git-github/","title":"Git y GitHub","text":"<p>Guias para trabajar con Git y GitHub en este curso.</p>"},{"location":"git-github/#que-encontraras-aqui","title":"Que encontraras aqui?","text":""},{"location":"git-github/#fork-y-clone","title":"Fork y Clone","text":"<p>Aprende a crear tu copia del repositorio y clonarlo a tu computadora:</p> <ul> <li>Que es un Fork y por que lo necesitas</li> <li>Como hacer Fork del repositorio</li> <li>Como clonar tu Fork a tu PC</li> <li>Configurar remotes (origin y upstream)</li> </ul>"},{"location":"git-github/#sincronizar-fork","title":"Sincronizar Fork","text":"<p>Mantener tu Fork actualizado con los ejercicios nuevos del profesor:</p> <ul> <li>Por que tu Fork NO se actualiza automaticamente</li> <li>Como sincronizar desde GitHub Web (facil)</li> <li>Como sincronizar desde Terminal (completo)</li> <li>Resolver conflictos de merge</li> <li>Diagramas visuales del flujo completo</li> </ul>"},{"location":"git-github/#comandos-utiles","title":"Comandos Utiles","text":"<p>Cheatsheet de Git para el dia a dia:</p> <ul> <li>Comandos basicos</li> <li>Comandos avanzados</li> <li>Atajos utiles</li> <li>Resolver problemas comunes</li> </ul>"},{"location":"git-github/#flujo-de-trabajo-sin-pull-request","title":"Flujo de Trabajo (Sin Pull Request)","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Clone a PC]\n    B --&gt; C[Trabajar]\n    C --&gt; D[Documentar PROMPTS.md]\n    D --&gt; E[Commit]\n    E --&gt; F[Push a tu Fork]\n    F --&gt; G[Evaluacion Automatica]</code></pre> <p>Sistema simplificado</p> <p>No necesitas crear Pull Request. El sistema evalua tu <code>PROMPTS.md</code> automaticamente. Solo sube tu trabajo con <code>git push</code>.</p>"},{"location":"git-github/#conceptos-basicos","title":"Conceptos Basicos","text":""},{"location":"git-github/#git-vs-github","title":"Git vs GitHub","text":"<p>Git</p> <p>Git es un sistema de control de versiones que funciona en tu computadora. Te permite:</p> <ul> <li>Guardar versiones de tu codigo</li> <li>Volver a versiones anteriores</li> <li>Trabajar en multiples ramas</li> <li>Colaborar con otros</li> </ul> <p>GitHub</p> <p>GitHub es una plataforma en la nube donde guardas tu codigo. Te permite:</p> <ul> <li>Compartir codigo publicamente</li> <li>Colaborar con otros desarrolladores</li> <li>Alojar proyectos</li> <li>Gestionar proyectos y colaboracion</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  GIT vs GITHUB                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  GIT (Programa en tu PC)                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  Tu computadora                       \u2502                 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502  \u2502  \u2502  \ud83d\udcc1 Carpeta con tu codigo       \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 ejercicio1.py              \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 ejercicio2.py              \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500 .git/  \u2190 Historial local  \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                      \u2502                                       \u2502\n\u2502                      \u2502 git push (subir)                     \u2502\n\u2502                      \u2193                                       \u2502\n\u2502  GITHUB (En Internet)                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  \ud83c\udf10 github.com                        \u2502                 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502  \u2502  \u2502  \ud83d\udce6 Tu repositorio online       \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  (Visible en el navegador)      \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"git-github/#primeros-pasos","title":"Primeros Pasos","text":"<p>Nunca usaste Git?</p> <p>Empieza con Fork y Clone donde te explicamos todo desde cero.</p> <p>Ya tienes el repositorio clonado?</p> <p>Aprende a Sincronizar tu Fork para obtener ejercicios nuevos.</p> <p>Completaste un ejercicio?</p> <p>Solo haz <code>git push</code> a tu fork. Lee la Guia de Entregas.</p>"},{"location":"git-github/#ayuda-y-recursos","title":"Ayuda y Recursos","text":""},{"location":"git-github/#problemas-comunes","title":"Problemas Comunes","text":"<p>Consulta la seccion de Comandos Utiles donde encontraras soluciones a problemas frecuentes como:</p> <ul> <li>\"fatal: not a git repository\"</li> <li>\"Your branch is behind origin/main\"</li> <li>\"CONFLICT (content): Merge conflict\"</li> <li>\"Permission denied (publickey)\"</li> </ul>"},{"location":"git-github/#recursos-externos","title":"Recursos Externos","text":"<ul> <li>Git Handbook</li> <li>GitHub Guides</li> <li>Atlassian Git Tutorial</li> <li>Oh Shit, Git!?! - Para cuando algo sale mal</li> </ul>"},{"location":"git-github/comandos-utiles/","title":"Comandos Utiles de Git","text":"<p>Cheatsheet de comandos Git para el dia a dia en el curso.</p>"},{"location":"git-github/comandos-utiles/#comandos-basicos","title":"Comandos Basicos","text":""},{"location":"git-github/comandos-utiles/#configuracion-inicial","title":"Configuracion Inicial","text":"<pre><code># Configurar nombre\ngit config --global user.name \"Tu Nombre\"\n\n# Configurar email\ngit config --global user.email \"tu@email.com\"\n\n# Ver configuracion\ngit config --list\n\n# Ver configuracion especifica\ngit config user.name\n</code></pre>"},{"location":"git-github/comandos-utiles/#clonar-y-actualizar","title":"Clonar y Actualizar","text":"<pre><code># Clonar tu fork\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Entrar a la carpeta\ncd ejercicios-bigdata\n\n# Agregar upstream (repo del profesor)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Ver remotes configurados\ngit remote -v\n</code></pre>"},{"location":"git-github/comandos-utiles/#trabajo-diario","title":"Trabajo Diario","text":""},{"location":"git-github/comandos-utiles/#estado-y-cambios","title":"Estado y Cambios","text":"<pre><code># Ver estado actual\ngit status\n\n# Ver cambios no guardados\ngit diff\n\n# Ver cambios en un archivo especifico\ngit diff archivo.py\n\n# Ver historial de commits\ngit log\n\n# Ver historial resumido\ngit log --oneline\n\n# Ver cambios de un commit especifico\ngit show abc123d\n</code></pre>"},{"location":"git-github/comandos-utiles/#guardar-cambios","title":"Guardar Cambios","text":"<pre><code># Agregar archivo especifico\ngit add archivo.py\n\n# Agregar todos los archivos modificados\ngit add .\n\n# Agregar solo archivos Python\ngit add *.py\n\n# Hacer commit\ngit commit -m \"Mensaje descriptivo\"\n\n# Hacer commit de archivos ya trackeados (skip add)\ngit commit -am \"Mensaje descriptivo\"\n\n# Modificar ultimo commit (antes de push)\ngit commit --amend -m \"Nuevo mensaje\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#ramas-branches","title":"Ramas (Branches)","text":""},{"location":"git-github/comandos-utiles/#crear-y-cambiar","title":"Crear y Cambiar","text":"<pre><code># Ver ramas locales\ngit branch\n\n# Ver todas las ramas (incluyendo remotas)\ngit branch -a\n\n# Crear nueva rama\ngit branch garcia-ejercicio-01\n\n# Cambiar a una rama\ngit checkout garcia-ejercicio-01\n\n# Crear y cambiar en un solo comando\ngit checkout -b garcia-ejercicio-01\n\n# Cambiar a main\ngit checkout main\n</code></pre>"},{"location":"git-github/comandos-utiles/#fusionar-y-borrar","title":"Fusionar y Borrar","text":"<pre><code># Fusionar una rama en la actual\ngit merge nombre-rama\n\n# Borrar rama local\ngit branch -d garcia-ejercicio-01\n\n# Forzar borrado (si tiene cambios sin fusionar)\ngit branch -D garcia-ejercicio-01\n\n# Borrar rama remota\ngit push origin --delete garcia-ejercicio-01\n</code></pre>"},{"location":"git-github/comandos-utiles/#sincronizacion","title":"Sincronizacion","text":""},{"location":"git-github/comandos-utiles/#descargar-cambios","title":"Descargar Cambios","text":"<pre><code># Descargar cambios del profesor (upstream)\ngit fetch upstream\n\n# Descargar y fusionar de tu fork (origin)\ngit pull origin main\n\n# Ver diferencias con upstream\ngit log HEAD..upstream/main\n\n# Ver commits que tu no tienes\ngit log HEAD..upstream/main --oneline\n</code></pre>"},{"location":"git-github/comandos-utiles/#sincronizar-fork-completo","title":"Sincronizar Fork Completo","text":"<pre><code># Workflow completo para sincronizar\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\ngit push origin main\n\n# O en una sola linea\ngit checkout main &amp;&amp; git fetch upstream &amp;&amp; git merge upstream/main &amp;&amp; git push origin main\n</code></pre>"},{"location":"git-github/comandos-utiles/#subir-cambios","title":"Subir Cambios","text":"<pre><code># Subir rama actual a origin\ngit push origin nombre-rama\n\n# Subir main\ngit push origin main\n\n# Subir y establecer upstream (primera vez)\ngit push -u origin nombre-rama\n\n# Despues solo necesitas\ngit push\n</code></pre>"},{"location":"git-github/comandos-utiles/#resolver-problemas","title":"Resolver Problemas","text":""},{"location":"git-github/comandos-utiles/#deshacer-cambios","title":"Deshacer Cambios","text":"<pre><code># Descartar cambios en un archivo (antes de add)\ngit checkout -- archivo.py\n\n# Descartar todos los cambios no guardados\ngit checkout -- .\n\n# Quitar archivo del staging (despues de add, antes de commit)\ngit reset HEAD archivo.py\n\n# Deshacer ultimo commit (mantiene cambios)\ngit reset --soft HEAD~1\n\n# Deshacer ultimo commit (descarta cambios)\ngit reset --hard HEAD~1\n\n# Volver a un commit especifico\ngit reset --hard abc123d\n</code></pre> <p>Cuidado con --hard</p> <p><code>git reset --hard</code> elimina cambios permanentemente. Usalo solo si estas seguro.</p>"},{"location":"git-github/comandos-utiles/#stash-guardar-temporalmente","title":"Stash (Guardar Temporalmente)","text":"<pre><code># Guardar cambios temporalmente\ngit stash\n\n# Guardar con mensaje\ngit stash save \"WIP: trabajando en ejercicio 03\"\n\n# Ver lista de stashes\ngit stash list\n\n# Aplicar ultimo stash\ngit stash apply\n\n# Aplicar y eliminar ultimo stash\ngit stash pop\n\n# Aplicar stash especifico\ngit stash apply stash@{1}\n\n# Eliminar stash\ngit stash drop stash@{0}\n\n# Eliminar todos los stashes\ngit stash clear\n</code></pre>"},{"location":"git-github/comandos-utiles/#conflictos","title":"Conflictos","text":"<pre><code># Ver archivos con conflictos\ngit status\n\n# Despues de resolver manualmente\ngit add archivo-resuelto.py\ngit commit -m \"Resolver conflicto en archivo\"\n\n# Abortar merge con conflictos\ngit merge --abort\n\n# Ver herramienta de merge\ngit mergetool\n</code></pre>"},{"location":"git-github/comandos-utiles/#informacion-y-busqueda","title":"Informacion y Busqueda","text":""},{"location":"git-github/comandos-utiles/#inspeccionar-historial","title":"Inspeccionar Historial","text":"<pre><code># Ver historial detallado\ngit log --graph --decorate --all\n\n# Ver quien modifico cada linea de un archivo\ngit blame archivo.py\n\n# Buscar en el historial\ngit log --grep=\"palabra clave\"\n\n# Ver archivos modificados en cada commit\ngit log --stat\n\n# Ver cambios de un autor especifico\ngit log --author=\"Tu Nombre\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#buscar-codigo","title":"Buscar Codigo","text":"<pre><code># Buscar en archivos trackeados\ngit grep \"palabra clave\"\n\n# Buscar en archivos Python\ngit grep \"palabra clave\" -- \"*.py\"\n\n# Buscar mostrando numero de linea\ngit grep -n \"palabra clave\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#atajos-y-aliases","title":"Atajos y Aliases","text":""},{"location":"git-github/comandos-utiles/#configurar-aliases","title":"Configurar Aliases","text":"<pre><code># Crear alias para status\ngit config --global alias.st status\n\n# Crear alias para checkout\ngit config --global alias.co checkout\n\n# Crear alias para commit\ngit config --global alias.ci commit\n\n# Crear alias para branch\ngit config --global alias.br branch\n\n# Alias para log bonito\ngit config --global alias.lg \"log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#usar-aliases","title":"Usar Aliases","text":"<pre><code># En lugar de: git status\ngit st\n\n# En lugar de: git checkout main\ngit co main\n\n# En lugar de: git commit -m \"mensaje\"\ngit ci -m \"mensaje\"\n\n# Ver log bonito\ngit lg\n</code></pre>"},{"location":"git-github/comandos-utiles/#workflow-del-curso","title":"Workflow del Curso","text":""},{"location":"git-github/comandos-utiles/#empezar-nuevo-ejercicio","title":"Empezar Nuevo Ejercicio","text":"<pre><code># 1. Actualizar main\ngit checkout main\ngit pull origin main\ngit fetch upstream\ngit merge upstream/main\n\n# 2. Crear rama para ejercicio\ngit checkout -b garcia-ejercicio-01\n\n# 3. Trabajar...\n# ... editar archivos ...\n\n# 4. Guardar trabajo\ngit add .\ngit commit -m \"Implementar carga de datos SQLite\"\n\n# 5. Subir a GitHub\ngit push -u origin garcia-ejercicio-01\n</code></pre>"},{"location":"git-github/comandos-utiles/#actualizar-rama-de-ejercicio","title":"Actualizar Rama de Ejercicio","text":"<pre><code># Si el profesor agrego cambios mientras trabajabas\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\ngit checkout garcia-ejercicio-01\ngit merge main\ngit push origin garcia-ejercicio-01\n</code></pre>"},{"location":"git-github/comandos-utiles/#aplicar-feedback-del-profesor","title":"Aplicar Feedback del Profesor","text":"<pre><code># 1. Asegurate de estar en tu rama\ngit checkout garcia-ejercicio-01\n\n# 2. Hacer correcciones\n# ... editar archivos ...\n\n# 3. Guardar y subir\ngit add .\ngit commit -m \"Aplicar feedback: optimizar queries\"\ngit push origin garcia-ejercicio-01\n\n# El PR se actualiza automaticamente\n</code></pre>"},{"location":"git-github/comandos-utiles/#comandos-avanzados","title":"Comandos Avanzados","text":""},{"location":"git-github/comandos-utiles/#cherry-pick","title":"Cherry Pick","text":"<pre><code># Aplicar un commit especifico a la rama actual\ngit cherry-pick abc123d\n\n# Aplicar sin hacer commit automatico\ngit cherry-pick -n abc123d\n</code></pre>"},{"location":"git-github/comandos-utiles/#rebase","title":"Rebase","text":"<pre><code># Rebase rama actual con main\ngit rebase main\n\n# Rebase interactivo (ultimos 3 commits)\ngit rebase -i HEAD~3\n\n# Continuar rebase despues de resolver conflictos\ngit rebase --continue\n\n# Abortar rebase\ngit rebase --abort\n</code></pre> <p>Cuidado con Rebase</p> <p>No hagas rebase de commits que ya subiste a GitHub (despues de push).</p>"},{"location":"git-github/comandos-utiles/#reflog","title":"Reflog","text":"<pre><code># Ver historial de todas las acciones\ngit reflog\n\n# Recuperar commit \"perdido\"\ngit reflog\ngit checkout abc123d\ngit checkout -b rama-recuperada\n</code></pre>"},{"location":"git-github/comandos-utiles/#trucos-y-tips","title":"Trucos y Tips","text":""},{"location":"git-github/comandos-utiles/#configuracion-util","title":"Configuracion Util","text":"<pre><code># Colorear output\ngit config --global color.ui auto\n\n# Editor por defecto (VSCode)\ngit config --global core.editor \"code --wait\"\n\n# Guardar credenciales temporalmente\ngit config --global credential.helper cache\n\n# Guardar credenciales permanentemente (Windows)\ngit config --global credential.helper wincred\n\n# Ignorar cambios en permisos de archivos\ngit config core.fileMode false\n</code></pre>"},{"location":"git-github/comandos-utiles/#gitignore","title":".gitignore","text":"<p>Crear archivo <code>.gitignore</code> en la raiz del proyecto:</p> <pre><code># Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\n.venv/\n\n# PyCharm\n.idea/\n\n# VSCode\n.vscode/\n\n# Jupyter\n.ipynb_checkpoints/\n\n# Datos grandes\n*.csv\n*.db\n*.parquet\ndatos/grandes/\n\n# Sistema\n.DS_Store\nThumbs.db\n</code></pre>"},{"location":"git-github/comandos-utiles/#errores-comunes-y-soluciones","title":"Errores Comunes y Soluciones","text":""},{"location":"git-github/comandos-utiles/#fatal-not-a-git-repository","title":"\"fatal: not a git repository\"","text":"<pre><code># Solucion: Navega a la carpeta del proyecto\ncd path/to/ejercicios-bigdata\n\n# Verifica que estas en la carpeta correcta\ngit status\n</code></pre>"},{"location":"git-github/comandos-utiles/#your-branch-is-behind-originmain","title":"\"Your branch is behind 'origin/main'\"","text":"<pre><code># Solucion: Actualiza tu rama local\ngit pull origin main\n</code></pre>"},{"location":"git-github/comandos-utiles/#conflict-content-merge-conflict","title":"\"CONFLICT (content): Merge conflict\"","text":"<pre><code># Solucion:\n# 1. Abre el archivo con conflicto\n# 2. Busca las marcas &lt;&lt;&lt;&lt;&lt;&lt;&lt; ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n# 3. Edita manualmente, elige que codigo mantener\n# 4. Elimina las marcas\n# 5. Guarda y haz commit\ngit add archivo-resuelto.py\ngit commit -m \"Resolver conflicto\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#permission-denied-publickey","title":"\"Permission denied (publickey)\"","text":"<pre><code># Solucion: Usa HTTPS en lugar de SSH\ngit remote set-url origin https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre>"},{"location":"git-github/comandos-utiles/#error-failed-to-push-some-refs","title":"\"error: failed to push some refs\"","text":"<pre><code># Causa: Tu rama local esta detras de la remota\n# Solucion: Pull primero\ngit pull origin tu-rama\n# Luego push\ngit push origin tu-rama\n</code></pre>"},{"location":"git-github/comandos-utiles/#comandos-de-emergencia","title":"Comandos de Emergencia","text":""},{"location":"git-github/comandos-utiles/#recuperar-trabajo-perdido","title":"Recuperar Trabajo Perdido","text":"<pre><code># Ver todos los cambios\ngit reflog\n\n# Volver a un estado anterior\ngit reset --hard abc123d\n\n# Recuperar archivo borrado\ngit checkout HEAD -- archivo.py\n</code></pre>"},{"location":"git-github/comandos-utiles/#limpiar-repositorio","title":"Limpiar Repositorio","text":"<pre><code># Eliminar archivos no trackeados (dry run)\ngit clean -n\n\n# Eliminar archivos no trackeados (ejecutar)\ngit clean -f\n\n# Eliminar archivos y carpetas no trackeadas\ngit clean -fd\n\n# Incluir archivos ignorados en .gitignore\ngit clean -fdx\n</code></pre>"},{"location":"git-github/comandos-utiles/#recursos-adicionales","title":"Recursos Adicionales","text":""},{"location":"git-github/comandos-utiles/#ayuda-integrada","title":"Ayuda Integrada","text":"<pre><code># Ayuda general\ngit help\n\n# Ayuda de un comando especifico\ngit help commit\ngit commit --help\n\n# Version corta de ayuda\ngit commit -h\n</code></pre>"},{"location":"git-github/comandos-utiles/#links-utiles","title":"Links Utiles","text":"<ul> <li>Git Cheat Sheet (GitHub)</li> <li>Visualizar Git</li> <li>Learn Git Branching</li> <li>Oh Shit, Git!?!</li> </ul>"},{"location":"git-github/comandos-utiles/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que conoces los comandos esenciales:</p> <ul> <li>Fork y Clone - Setup inicial del proyecto</li> <li>Sincronizar Fork - Mantener tu fork actualizado</li> <li>Guia de Entregas - Como entregar ejercicios</li> </ul>"},{"location":"git-github/fork-clone/","title":"Fork y Clone","text":"<p>Guia completa para crear tu copia del repositorio y trabajar con ella.</p>"},{"location":"git-github/fork-clone/#que-es-git-que-es-github","title":"Que es Git? Que es GitHub?","text":"<p>Git</p> <p>Git = Sistema de control de versiones (como \"guardar versiones\" de tu codigo)</p> <p>GitHub</p> <p>GitHub = Nube donde guardas tu codigo (como Dropbox, pero para codigo)</p> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph Local[\"\ud83d\udcbb GIT - Tu Computadora\"]\n        direction TB\n        PC[\"\ud83d\udcc1 Carpeta con tu c\u00f3digo&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 ejercicio1.py&lt;br/&gt;\u251c\u2500\u2500 ejercicio2.py&lt;br/&gt;\u2514\u2500\u2500 .git/ \u2190 Historial local\"]\n    end\n\n    subgraph Cloud[\"\ud83c\udf10 GITHUB - Internet (github.com)\"]\n        direction TB\n        Repo[\"\ud83d\udce6 Tu repositorio online&lt;br/&gt;&lt;br/&gt;Visible en el navegador&lt;br/&gt;Respaldo en la nube\"]\n    end\n\n    PC --&gt;|git push&lt;br/&gt;Subir cambios| Cloud\n    Cloud --&gt;|git pull&lt;br/&gt;Descargar cambios| PC\n\n    style Local fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style Cloud fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style PC fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n    style Repo fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px</code></pre>"},{"location":"git-github/fork-clone/#que-es-un-fork","title":"Que es un FORK?","text":"<p>Un fork es hacer TU PROPIA COPIA del repositorio del profesor en GitHub.</p> <p>Piensalo asi:</p> <ul> <li> El profesor tiene un libro (repositorio)</li> <li> Haces una fotocopia del libro completo (fork)</li> <li> Ahora puedes escribir en TU copia sin afectar el original</li> <li> Cuando termines, subes tu trabajo con <code>git push</code> (evaluacion automatica)</li> </ul> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TD\n    subgraph Original[\"\ud83d\udc68\u200d\ud83c\udfeb REPOSITORIO ORIGINAL (Profesor)\"]\n        direction TB\n        RepoProf[\"TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\ud83d\udcc1 ejercicio_01/&lt;br/&gt;\ud83d\udcc1 ejercicio_02/&lt;br/&gt;\ud83d\udcc1 datos/&lt;br/&gt;&lt;br/&gt;\ud83d\udd12 NO puedes modificar directamente\"]\n    end\n\n    ForkAction{{\"\ud83c\udf74 HACER FORK&lt;br/&gt;(Click en bot\u00f3n 'Fork')\"}}\n\n    subgraph TuCopia[\"\ud83d\udc64 TU FORK (Tu Copia Personal)\"]\n        direction TB\n        RepoTuyo[\"TU_USUARIO/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\ud83d\udcc1 ejercicio_01/&lt;br/&gt;\ud83d\udcc1 ejercicio_02/&lt;br/&gt;\ud83d\udcc1 datos/&lt;br/&gt;&lt;br/&gt;\u2705 Esta copia S\u00cd puedes modificarla\"]\n    end\n\n    Original --&gt; ForkAction\n    ForkAction --&gt;|Crea una copia&lt;br/&gt;completa e independiente| TuCopia\n\n    style Original fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style TuCopia fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style ForkAction fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n    style RepoProf fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style RepoTuyo fill:#e8f5e9,stroke:#388e3c,stroke-width:2px</code></pre>"},{"location":"git-github/fork-clone/#paso-1-hacer-fork-del-repositorio","title":"PASO 1: Hacer Fork del Repositorio","text":""},{"location":"git-github/fork-clone/#instrucciones-paso-a-paso","title":"Instrucciones Paso a Paso","text":"<p>1. Ir al repositorio del profesor:</p> <p>Abre tu navegador y ve a:</p> <pre><code>https://github.com/TodoEconometria/ejercicios-bigdata\n</code></pre> <p>2. Hacer Fork (copiar a tu cuenta):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GitHub - Pagina del Repositorio       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                          \u2502\n\u2502  [\u2b50 Star]  [\ud83c\udf74 Fork]  [\u2b07 Code]        \u2502\n\u2502              \u2191                           \u2502\n\u2502              \u2514\u2500\u2500 HAZ CLICK AQUI         \u2502\n\u2502                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Click en el boton \"Fork\" (arriba a la derecha)</li> <li>Selecciona tu cuenta de GitHub como destino</li> <li>Espera unos segundos mientras GitHub copia todo</li> </ul> <p>3. Verificar tu fork:</p> <p>Ahora deberias estar en TU copia:</p> <pre><code>https://github.com/TU_USUARIO/ejercicios-bigdata\n        \u2191\n        \u2514\u2500\u2500 Aqui debe aparecer TU nombre de usuario\n</code></pre> <p> Listo! Ya tienes tu copia personal del repositorio.</p>"},{"location":"git-github/fork-clone/#paso-2-clonar-tu-fork-a-tu-computadora","title":"PASO 2: Clonar TU Fork a Tu Computadora","text":""},{"location":"git-github/fork-clone/#que-significa-clonar","title":"Que significa \"clonar\"?","text":"<p>Clonar = Descargar todo el codigo de GitHub a tu computadora</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \ud83c\udf10 GitHub (Tu Fork)                    \u2502\n\u2502  https://github.com/TU_USUARIO/...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 git clone (descargar)\n                  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \ud83d\udcbb Tu PC                                \u2502\n\u2502  \ud83d\udcc1 Carpeta: ejercicios-bigdata/        \u2502\n\u2502     \u251c\u2500\u2500 ejercicio_01/                   \u2502\n\u2502     \u251c\u2500\u2500 ejercicio_02/                   \u2502\n\u2502     \u2514\u2500\u2500 datos/                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"git-github/fork-clone/#instrucciones-paso-a-paso_1","title":"Instrucciones Paso a Paso","text":"<p>1. Abrir la terminal/cmd:</p> WindowsmacOSLinux <p>Presiona <code>Win + R</code>, escribe <code>cmd</code>, Enter</p> <p>Busca \"Terminal\" en Spotlight (<code>Cmd + Space</code>)</p> <p>Presiona <code>Ctrl + Alt + T</code></p> <p>2. Ir a la carpeta donde quieres guardar el proyecto:</p> <pre><code># Ejemplo: Ir a Documentos\ncd Documents\n\n# O crear una carpeta nueva para tus proyectos\nmkdir mis-proyectos\ncd mis-proyectos\n</code></pre> <p>3. Clonar TU fork (reemplaza TU_USUARIO):</p> <pre><code>git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre> <p>IMPORTANTE</p> <p>Asegurate de poner TU nombre de usuario, no \"TodoEconometria\"</p> <p>4. Entrar a la carpeta:</p> <pre><code>cd ejercicios-bigdata\n</code></pre> <p>5. Conectar con el repo original del profesor:</p> <p>Esto te permite recibir actualizaciones cuando el profesor agregue ejercicios nuevos:</p> <pre><code>git remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n</code></pre> <p>6. Verificar que todo esta bien:</p> <pre><code>git remote -v\n</code></pre> <p>Deberias ver algo asi:</p> <pre><code>origin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (fetch)\norigin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (push)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (fetch)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (push)\n</code></pre> <p> Listo! Ya tienes todo el codigo en tu computadora.</p>"},{"location":"git-github/fork-clone/#entendiendo-origin-y-upstream","title":"Entendiendo origin y upstream","text":"<p>origin</p> <p>origin = Tu fork en GitHub (donde subes tus cambios)</p> <p>upstream</p> <p>upstream = Repositorio original del profesor (de donde descargas actualizaciones)</p> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph Upstream[\"\u2b06\ufe0f UPSTREAM (Profesor)\"]\n        direction TB\n        UP[\"TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\u2713 Repo original&lt;br/&gt;\u2713 Solo lectura para ti&lt;br/&gt;\u2713 Descargas actualizaciones de aqu\u00ed\"]\n    end\n\n    subgraph Origin[\"\ud83c\udf10 ORIGIN (Tu Fork en GitHub)\"]\n        direction TB\n        OR[\"TU_USUARIO/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\u2713 Tu copia en GitHub&lt;br/&gt;\u2713 Lectura y escritura&lt;br/&gt;\u2713 Subes tus cambios aqu\u00ed\"]\n    end\n\n    subgraph Local[\"\ud83d\udcbb LOCAL (Tu PC)\"]\n        direction TB\n        LOC[\"ejercicios-bigdata/&lt;br/&gt;&lt;br/&gt;\u2713 Carpeta en tu computadora&lt;br/&gt;\u2713 Trabajas aqu\u00ed&lt;br/&gt;\u2713 Haces commits locales\"]\n    end\n\n    Upstream --&gt;|\"\ud83c\udf74 Fork\"| Origin\n    Origin --&gt;|\"\ud83d\udce5 Clone&lt;br/&gt;(git clone)\"| Local\n    Local --&gt;|\"\ud83d\udce4 Push&lt;br/&gt;(git push origin)\"| Origin\n    Upstream --&gt;|\"\ud83d\udd04 Fetch&lt;br/&gt;(git fetch upstream)\"| Local\n\n    style Upstream fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style Origin fill:#fff9c4,stroke:#f57f17,stroke-width:3px\n    style Local fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style UP fill:#bbdefb,stroke:#1976d2,stroke-width:2px\n    style OR fill:#fff59d,stroke:#f9a825,stroke-width:2px\n    style LOC fill:#c8e6c9,stroke:#43a047,stroke-width:2px</code></pre>"},{"location":"git-github/fork-clone/#flujo-completo-de-trabajo","title":"Flujo Completo de Trabajo","text":"<pre><code>graph TD\n    A[Repo Profesor&lt;br/&gt;upstream] --&gt;|1. Fork| B[Tu Fork&lt;br/&gt;origin]\n    B --&gt;|2. Clone| C[Tu PC&lt;br/&gt;local]\n    C --&gt;|3. Trabajas| D[Editar codigo]\n    D --&gt;|4. Commit| E[Guardar cambios]\n    E --&gt;|5. Push| B\n    B --&gt;|6. Evaluacion| F[Sistema evalua&lt;br/&gt;PROMPTS.md]\n    A --&gt;|7. Nuevos ejercicios| C\n\n    style A fill:#e1f5ff,stroke:#0277bd\n    style B fill:#fff9c4,stroke:#f57f17\n    style C fill:#e8f5e9,stroke:#388e3c\n    style F fill:#f3e5f5,stroke:#7b1fa2</code></pre>"},{"location":"git-github/fork-clone/#comandos-basicos","title":"Comandos Basicos","text":""},{"location":"git-github/fork-clone/#descargar-cambios-del-profesor","title":"Descargar cambios del profesor","text":"<pre><code># Descargar cambios\ngit fetch upstream\n\n# Aplicar cambios a tu rama main\ngit checkout main\ngit merge upstream/main\n\n# Subir a tu fork\ngit push origin main\n</code></pre>"},{"location":"git-github/fork-clone/#subir-tus-cambios","title":"Subir tus cambios","text":"<pre><code># Ver que cambiaste\ngit status\n\n# Agregar archivos\ngit add archivo.py\n\n# Guardar con mensaje\ngit commit -m \"Descripcion del cambio\"\n\n# Subir a tu fork\ngit push origin nombre-de-tu-rama\n</code></pre>"},{"location":"git-github/fork-clone/#problemas-comunes","title":"Problemas Comunes","text":"Error: Permission denied (publickey) <p>Causa: No tienes configuradas las SSH keys.</p> <p>Solucion: Usa HTTPS en lugar de SSH:</p> <pre><code># Usa esta URL (HTTPS):\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# NO uses esta (SSH):\ngit clone git@github.com:TU_USUARIO/ejercicios-bigdata.git\n</code></pre> Error: fatal: not a git repository <p>Causa: No estas en la carpeta del proyecto.</p> <p>Solucion:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica que estas en la carpeta correcta\nls -la  # Deberas ver una carpeta .git/\n</code></pre> Clono el repo del profesor en lugar de mi fork <p>Causa: Usaste la URL del profesor.</p> <p>Solucion:</p> <ol> <li>Borra la carpeta clonada</li> <li>Haz fork primero en GitHub</li> <li>Clona TU fork, no el del profesor</li> </ol> <pre><code># \u274c MAL\ngit clone https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# \u2705 BIEN\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre>"},{"location":"git-github/fork-clone/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que tienes el repositorio clonado:</p> <ul> <li>Tu Primer Ejercicio - Empezar a trabajar</li> <li>Sincronizar Fork - Mantener tu fork actualizado</li> <li>Guia de Entregas - Como entregar ejercicios</li> </ul>"},{"location":"git-github/sincronizar-fork/","title":"Sincronizar tu Fork","text":"<p>IMPORTANTE</p> <p>Tu fork NO se actualiza automaticamente. Debes sincronizarlo manualmente para obtener los ejercicios nuevos que el profesor agregue.</p> <p>Sistema de Evaluacion</p> <p>Ya NO se usan Pull Requests. El sistema evalua tu <code>PROMPTS.md</code> directamente en tu fork. Ver la Guia de Entregas para mas detalles.</p>"},{"location":"git-github/sincronizar-fork/#el-problema","title":"El Problema","text":"<p>Cuando haces fork, obtienes una copia en ese momento. Durante el curso agregare ejercicios nuevos, pero tu fork NO se actualiza solo.</p> <p><pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph S1[\"SEMANA 1 - Hiciste Fork\"]\n        direction LR\n        Prof1[\"Repo Profesor&lt;br/&gt;[01] [02]\"]\n        Fork1[\"Tu Fork&lt;br/&gt;[01] [02]\"]\n        Prof1 -.-&gt;|Fork| Fork1\n    end\n\n    subgraph S3[\"SEMANA 3 - Profesor agrego ejercicios\"]\n        direction LR\n        Prof3[\"Repo Profesor&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        Fork3[\"Tu Fork&lt;br/&gt;[01] [02]&lt;br/&gt;Te faltan [03] [04] [05]\"]\n    end\n\n    S1 --&gt; S3\n\n    style S1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style S3 fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style Prof1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style Fork1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n    style Prof3 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style Fork3 fill:#ffcdd2,stroke:#d32f2f,stroke-width:3px</code></pre> https://github.com/TU_USUARIO/ejercicios-bigdata <pre><code>**Paso 2:** Buscar el banner de sincronizacion\n\nCuando hay cambios nuevos, veras un banner as\u00ed:\n\n!!! example \"Banner en GitHub\"\n    ```\n    \u26a0\ufe0f This branch is 15 commits behind TodoEconometria:main\n\n    [Sync fork \u25bc]  \u2190 CLICK AQUI\n    ```\n\n**Paso 3:** Click en \"Sync fork\" \u2192 \"Update branch\"\n\n!!! example \"Opciones de sincronizaci\u00f3n\"\n    **Sync fork**\n\n    This will update your branch with the latest changes from TodoEconometria:main\n\n    **[Update branch]** \u2190 CLICK AQUI\n    [Discard commits]\n\n**Paso 4:** Actualizar tu copia local\n\nAhora tu fork en GitHub esta actualizado, pero tu PC no. Ejecuta:\n\n```bash\ngit checkout main\ngit pull origin main\n</code></pre></p> <p>Paso 5: Traer cambios a tu rama de trabajo</p> <pre><code># Ve a tu rama de ejercicio\ngit checkout tu-apellido-ejercicio\n\n# Trae los cambios de main\ngit merge main\n\n# Sube a GitHub\ngit push origin tu-apellido-ejercicio\n</code></pre> <p> Listo! Tienes los ejercicios nuevos sin perder tu trabajo.</p>"},{"location":"git-github/sincronizar-fork/#diagrama-visual-del-flujo","title":"Diagrama Visual del Flujo","text":""},{"location":"git-github/sincronizar-fork/#como-funciona-la-sincronizacion","title":"Como funciona la sincronizacion","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\nsequenceDiagram\n    actor T\u00fa\n    participant Local as \ud83d\udcbb Tu PC&lt;br/&gt;(main: 01, 02)\n    participant TuBranch as \ud83d\udcbb Tu PC&lt;br/&gt;(tu-rama: 01, 02 + TU C\u00d3DIGO)\n    participant Origin as \ud83c\udf10 Tu Fork GitHub&lt;br/&gt;(01, 02)\n    participant Upstream as \ud83d\udc68\u200d\ud83c\udfeb Repo Profesor&lt;br/&gt;(01, 02, 03, 04, 05)\n\n    Note over T\u00fa,Upstream: ESTADO INICIAL - Tu fork desactualizado\n\n    rect rgb(255, 243, 224)\n    Note over T\u00fa,Upstream: PASO 1: Cambiar a rama main\n    T\u00fa-&gt;&gt;Local: git checkout main\n    activate Local\n    Note over Local: Ahora est\u00e1s en main\n    end\n\n    rect rgb(232, 245, 233)\n    Note over T\u00fa,Upstream: PASO 2: Descargar y fusionar cambios del profesor\n    T\u00fa-&gt;&gt;Upstream: git fetch upstream\n    Upstream--&gt;&gt;Local: Descarga [03, 04, 05]\n    T\u00fa-&gt;&gt;Local: git merge upstream/main\n    Note over Local: main: 01, 02, 03, 04, 05 \u2705\n    deactivate Local\n    end\n\n    rect rgb(237, 231, 246)\n    Note over T\u00fa,Upstream: PASO 3: Cambiar a tu rama de trabajo\n    T\u00fa-&gt;&gt;TuBranch: git checkout tu-rama\n    activate TuBranch\n    Note over TuBranch: Ahora est\u00e1s en tu-rama\n    end\n\n    rect rgb(255, 249, 196)\n    Note over T\u00fa,Upstream: PASO 4: Traer cambios a tu rama\n    T\u00fa-&gt;&gt;TuBranch: git merge main\n    Note over TuBranch: tu-rama: 01-05 + TU C\u00d3DIGO \u2705\n    deactivate TuBranch\n    end\n\n    rect rgb(225, 245, 254)\n    Note over T\u00fa,Upstream: PASO 5: Subir todo a GitHub\n    T\u00fa-&gt;&gt;Origin: git push origin tu-rama\n    Note over Origin: tu-rama: 01-05 + TU C\u00d3DIGO \u2705\n    end\n\n    rect rgb(200, 230, 201)\n    Note over T\u00fa,Upstream: \u2705 RESULTADO - Tienes todo sin perder tu trabajo\n    end</code></pre>"},{"location":"git-github/sincronizar-fork/#vista-simplificada-del-proceso","title":"Vista Simplificada del Proceso","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#e1f5ff','primaryTextColor':'#000','primaryBorderColor':'#0277bd','secondaryColor':'#fff9c4','tertiaryColor':'#e8f5e9','noteBkgColor':'#fff3e0','noteTextColor':'#000'}}}%%\nflowchart TB\n    subgraph Antes[\"\u274c ANTES - Desactualizado\"]\n        direction LR\n        A1[\"\ud83d\udc68\u200d\ud83c\udfeb Repo Profesor&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Ejercicios:&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        A2[\"\ud83c\udf10 Tu Fork&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Tus ejercicios:&lt;br/&gt;[01] [02]&lt;br/&gt;&lt;br/&gt;\u26a0\ufe0f Te faltan 3 ejercicios\"]\n        A3[\"\ud83d\udcbb Tu PC&lt;br/&gt;&lt;br/&gt;\ud83d\udcc2 tu-rama:&lt;br/&gt;[01] [02] + TU C\u00d3DIGO\"]\n    end\n\n    subgraph Proceso[\"\ud83d\udd04 PROCESO DE SINCRONIZACI\u00d3N\"]\n        direction TB\n        P1[\"\u2460 git fetch upstream&lt;br/&gt;Descargar cambios del profesor\"]\n        P2[\"\u2461 git merge upstream/main&lt;br/&gt;Aplicar a tu main local\"]\n        P3[\"\u2462 git merge main&lt;br/&gt;Traer a tu rama de trabajo\"]\n        P4[\"\u2463 git push origin tu-rama&lt;br/&gt;Subir todo a GitHub\"]\n\n        P1 --&gt; P2 --&gt; P3 --&gt; P4\n    end\n\n    subgraph Despues[\"\u2705 DESPU\u00c9S - Actualizado\"]\n        direction LR\n        D1[\"\ud83d\udc68\u200d\ud83c\udfeb Repo Profesor&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Ejercicios:&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        D2[\"\ud83c\udf10 Tu Fork&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Tus ejercicios:&lt;br/&gt;[01-05] + TU C\u00d3DIGO&lt;br/&gt;&lt;br/&gt;\u2705 Completamente actualizado\"]\n        D3[\"\ud83d\udcbb Tu PC&lt;br/&gt;&lt;br/&gt;\ud83d\udcc2 tu-rama:&lt;br/&gt;[01-05] + TU C\u00d3DIGO&lt;br/&gt;&lt;br/&gt;\ud83c\udfaf Listo para trabajar\"]\n    end\n\n    Antes --&gt; Proceso --&gt; Despues\n\n    style A1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style A2 fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style A3 fill:#fff3e0,stroke:#ef6c00,stroke-width:2px\n\n    style P1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style P2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P3 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style P4 fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n\n    style D1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style D2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style D3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px</code></pre>"},{"location":"git-github/sincronizar-fork/#metodo-detallado-terminal","title":"Metodo Detallado (Terminal)","text":""},{"location":"git-github/sincronizar-fork/#situacion","title":"Situacion","text":"<p>Trabajas en una rama (ejemplo: <code>garcia-ejercicio-1.1</code>) y el profesor agrego ejercicios nuevos.</p> <p>Objetivo: Traer los ejercicios nuevos SIN perder tu trabajo.</p>"},{"location":"git-github/sincronizar-fork/#paso-1-guarda-tu-trabajo-actual","title":"PASO 1: Guarda tu trabajo actual","text":"<pre><code># Ver que archivos cambiaste\ngit status\n\n# Guardar tus cambios\ngit add entregas/01_bases_de_datos/tu_apellido_nombre/\ngit commit -m \"Guardar mi avance\"\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-2-ve-a-tu-rama-main","title":"PASO 2: Ve a tu rama main","text":"<pre><code>git checkout main\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-3-descarga-los-cambios-del-profesor","title":"PASO 3: Descarga los cambios del profesor","text":"<pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre> <p>Ahora tu <code>main</code> local tiene los ejercicios nuevos </p>"},{"location":"git-github/sincronizar-fork/#paso-4-vuelve-a-tu-rama-de-trabajo","title":"PASO 4: Vuelve a tu rama de trabajo","text":"<pre><code>git checkout garcia-ejercicio-1.1\n</code></pre> <p>(Reemplaza <code>garcia-ejercicio-1.1</code> por el nombre de TU rama)</p>"},{"location":"git-github/sincronizar-fork/#paso-5-trae-los-ejercicios-nuevos-a-tu-rama","title":"PASO 5: Trae los ejercicios nuevos a tu rama","text":"<pre><code>git merge main\n</code></pre> <p>Que hace esto?</p> <p>Combina los ejercicios nuevos del profesor con tu trabajo. NO borra nada tuyo.</p>"},{"location":"git-github/sincronizar-fork/#paso-6-sube-a-github","title":"PASO 6: Sube a GitHub","text":"<pre><code>git push origin garcia-ejercicio-1.1\n</code></pre> <p> Listo! Tienes los ejercicios nuevos Y tu trabajo intacto.</p>"},{"location":"git-github/sincronizar-fork/#que-pasa-cuando-el-profesor-agrega-ejercicios","title":"Que Pasa Cuando el Profesor Agrega Ejercicios?","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'git0':'#e1f5ff','git1':'#fff9c4','git2':'#ffebee'}}}%%\ngitGraph\n    commit id: \"01: Intro SQLite\" tag: \"Semana 1\"\n    commit id: \"02: Limpieza Datos\"\n    branch tu-fork\n    checkout tu-fork\n    commit id: \"\u2705 Hiciste Fork\" type: HIGHLIGHT\n\n    checkout main\n    commit id: \"03: Dask &amp; Parquet\" tag: \"Semana 3\"\n    commit id: \"04: PySpark\"\n    commit id: \"05: Dashboard\"\n\n    checkout tu-fork\n    commit id: \"\u274c Desactualizado\" type: REVERSE\n    commit id: \"\u26a0\ufe0f Te faltan 03, 04, 05\" type: REVERSE</code></pre> <p>El fork NO se actualiza autom\u00e1ticamente</p> <p>Cuando el profesor agrega ejercicios nuevos al repositorio original, tu fork en GitHub NO recibe esos cambios autom\u00e1ticamente. Debes sincronizarlo manualmente siguiendo los pasos de esta gu\u00eda.</p>"},{"location":"git-github/sincronizar-fork/#regla-de-oro-para-evitar-problemas","title":"Regla de Oro para Evitar Problemas","text":"<pre><code>%%{init: {'theme':'base'}}%%\nflowchart LR\n    subgraph Bien[\"\u2705 BIEN - Edita solo aqu\u00ed\"]\n        direction TB\n        B1[\"\ud83d\udcc1 entregas/01_bases_de_datos/tu_apellido_nombre/&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 1.1_sqlite/&lt;br/&gt;\u2502   \u251c\u2500\u2500 ANALISIS_DATOS.md&lt;br/&gt;\u2502   \u251c\u2500\u2500 resumen_eda.md&lt;br/&gt;\u2502   \u2514\u2500\u2500 REFLEXION.md&lt;br/&gt;&lt;br/&gt;\u2705 Aqu\u00ed haces tus cambios\"]\n    end\n\n    subgraph Mal[\"\u274c MAL - NO toques esto\"]\n        direction TB\n        M1[\"\ud83d\udcc1 ejercicios/01_bases_de_datos/&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 README.md \u2190 NO TOCAR&lt;br/&gt;\u251c\u2500\u2500 eda_exploratorio.py \u2190 Solo ejecutar&lt;br/&gt;&lt;br/&gt;\ud83d\udd12 Archivos del profesor\"]\n    end\n\n    Bien -.-&gt;|Sin conflictos| OK[\"\ud83c\udf89 Sincronizaci\u00f3n&lt;br/&gt;perfecta\"]\n    Mal -.-&gt;|Causa conflictos| NOK[\"\u26a0\ufe0f Problemas&lt;br/&gt;al sincronizar\"]\n\n    style Bien fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style Mal fill:#ffcdd2,stroke:#c62828,stroke-width:3px\n    style B1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style M1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px\n    style OK fill:#a5d6a7,stroke:#43a047,stroke-width:2px\n    style NOK fill:#ef9a9a,stroke:#e53935,stroke-width:2px</code></pre> <p>Regla de Oro</p> <p>Si solo editas archivos en <code>entregas/TU_CARPETA/</code>, NUNCA tendr\u00e1s conflictos.</p> <p>El profesor actualiza <code>ejercicios/</code>, t\u00fa trabajas en <code>entregas/</code>. Cero problemas.</p>"},{"location":"git-github/sincronizar-fork/#que-hago-si-git-dice-conflict","title":"Que hago si Git dice \"CONFLICT\"?","text":""},{"location":"git-github/sincronizar-fork/#paso-1-git-te-dira-que-archivo-tiene-conflicto","title":"Paso 1: Git te dira que archivo tiene conflicto","text":"<pre><code>Auto-merging ejercicio_01.py\nCONFLICT (content): Merge conflict in ejercicio_01.py\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-2-abre-el-archivo","title":"Paso 2: Abre el archivo","text":"<p>Veras algo asi:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ntu codigo aqui\n=======\ncodigo del profesor\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-3-decide-que-mantener","title":"Paso 3: Decide que mantener","text":"<ul> <li>Si es un archivo del profesor que NO deberias tocar \u2192 Mant\u00e9n la version del profesor</li> <li>Si es TU archivo de entrega \u2192 Mant\u00e9n tu version</li> </ul>"},{"location":"git-github/sincronizar-fork/#paso-4-borra-las-marcas","title":"Paso 4: Borra las marcas","text":"<p>Elimina estas lineas:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-5-termina-el-merge","title":"Paso 5: Termina el merge","text":"<pre><code>git add nombre-del-archivo\ngit commit -m \"Resolver conflicto\"\ngit push origin tu-rama\n</code></pre> <p>Consejo</p> <p>Si trabajas solo en <code>entregas/TU_CARPETA/</code>, esto nunca te pasara.</p>"},{"location":"git-github/sincronizar-fork/#resumen-ultra-rapido","title":"Resumen Ultra-Rapido","text":"<pre><code># 1. Guardar tu trabajo\ngit add .\ngit commit -m \"Guardar avance\"\n\n# 2. Actualizar main\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\n\n# 3. Volver a tu rama y traer cambios\ngit checkout tu-rama\ngit merge main\n\n# 4. Subir\ngit push origin tu-rama\n</code></pre> <p>Frecuencia: Haz esto cada lunes antes de clase.</p>"},{"location":"git-github/sincronizar-fork/#buenas-practicas-de-sincronizacion","title":"Buenas Practicas de Sincronizacion","text":""},{"location":"git-github/sincronizar-fork/#1-sincroniza-antes-de-empezar-un-ejercicio-nuevo","title":"1. Sincroniza ANTES de empezar un ejercicio nuevo","text":"<pre><code># \u2705 BIEN - Sincronizar primero\ngit fetch upstream &amp;&amp; git merge upstream/main\n# Ahora empieza a trabajar\n\n# \u274c MAL - Trabajar con codigo viejo\n# Empiezas sin actualizar, luego tienes conflictos\n</code></pre>"},{"location":"git-github/sincronizar-fork/#2-haz-un-commit-de-tu-trabajo-antes-de-sincronizar","title":"2. Haz un commit de tu trabajo ANTES de sincronizar","text":"<pre><code># \u2705 BIEN - Guarda tu trabajo primero\ngit add .\ngit commit -m \"Avance en ejercicio 03\"\ngit fetch upstream &amp;&amp; git merge upstream/main\n\n# \u274c MAL - Sincronizar con cambios sin guardar\n# Puedes perder tu trabajo\n</code></pre>"},{"location":"git-github/sincronizar-fork/#3-frecuencia-recomendada","title":"3. Frecuencia recomendada","text":"<pre><code>%%{init: {'theme':'base'}}%%\ngantt\n    title \ud83d\udcc5 Calendario de Sincronizaci\u00f3n Semanal\n    dateFormat YYYY-MM-DD\n    section Lunes\n    Sincronizar antes de clase :milestone, m1, 2024-01-01, 0d\n    git fetch upstream :active, 2024-01-01, 1h\n    git merge upstream/main :active, 2024-01-01, 30m\n    section Martes a Jueves\n    Trabajar en ejercicios :2024-01-02, 3d\n    Commits locales :2024-01-02, 3d\n    section Viernes\n    Push de tu avance :milestone, m2, 2024-01-05, 0d\n    git push origin tu-rama :crit, 2024-01-05, 1h\n    section Domingo\n    Verificar actualizaciones (opcional) :done, 2024-01-07, 30m</code></pre> <p>Recomendaci\u00f3n de frecuencia</p> <ul> <li>Lunes: Sincroniza antes de clase para tener los \u00faltimos ejercicios</li> <li>Durante la semana: Trabaja normalmente, haz commits frecuentes</li> <li>Viernes: Sube tu avance a GitHub</li> <li>Domingo (opcional): Verifica si hay actualizaciones nuevas</li> </ul>"},{"location":"git-github/sincronizar-fork/#verificar-estado-de-sincronizacion","title":"Verificar Estado de Sincronizacion","text":""},{"location":"git-github/sincronizar-fork/#comando-util-para-saber-si-estas-desactualizado","title":"Comando util para saber si estas desactualizado","text":"<pre><code># Ver diferencias entre tu fork y el repo del profesor\ngit fetch upstream\ngit log HEAD..upstream/main --oneline\n</code></pre> <p>Si ves commits nuevos:</p> <pre><code>a1b2c3d Agregar ejercicio 06\nd4e5f6g Corregir typo en ejercicio 05\ng7h8i9j Agregar datos para ejercicio 06\n</code></pre> <p>Significa que tienes 3 commits (ejercicios/actualizaciones) que no tienes.</p> <p>Si no ves nada:</p> <pre><code>(vacio)\n</code></pre> <p>Significa que estas actualizado. </p>"},{"location":"git-github/sincronizar-fork/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que sabes como sincronizar tu fork:</p> <ul> <li>Guia de Entregas - Como entregar ejercicios</li> <li>Comandos Utiles - Cheatsheet de Git</li> <li>Fork y Clone - Si necesitas repasar los conceptos basicos</li> </ul>"},{"location":"guia-inicio/","title":"Guia de Inicio","text":"<p>Bienvenido a la guia de inicio del curso de Big Data con Python. Esta seccion te guiara a traves de todo lo necesario para comenzar a trabajar con los ejercicios.</p>"},{"location":"guia-inicio/#que-encontraras-aqui","title":"Que encontraras aqui?","text":""},{"location":"guia-inicio/#instalacion-de-herramientas","title":"Instalacion de Herramientas","text":"<p>Aprende a instalar todas las herramientas necesarias para el curso:</p> <ul> <li>Python 3.11+</li> <li>Git</li> <li>PyCharm (IDE recomendado)</li> <li>Dependencias del proyecto</li> </ul>"},{"location":"guia-inicio/#tu-primer-ejercicio","title":"Tu Primer Ejercicio","text":"<p>Una guia paso a paso para completar tu primer ejercicio:</p> <ul> <li>Como navegar el repositorio</li> <li>Como ejecutar los ejercicios</li> <li>Como entregar tu trabajo</li> <li>Mejores practicas</li> </ul>"},{"location":"guia-inicio/#roadmap-del-curso","title":"Roadmap del Curso","text":"<p>Vision general de todos los ejercicios y tecnologias:</p> <ul> <li>Niveles de aprendizaje</li> <li>Tiempo estimado por ejercicio</li> <li>Tecnologias que dominaras</li> <li>Plan de estudio recomendado</li> </ul>"},{"location":"guia-inicio/#por-donde-empezar","title":"Por donde empezar?","text":"<p>Primera vez con Git y Python?</p> <p>Empieza con la Instalacion de Herramientas donde te explicamos paso a paso como instalar todo lo necesario.</p> <p>Ya tienes todo instalado?</p> <p>Ve directo a Tu Primer Ejercicio para comenzar a trabajar.</p> <p>Desarrollador experimentado?</p> <p>Revisa el Roadmap del Curso para ver todos los ejercicios y elegir por donde empezar.</p>"},{"location":"guia-inicio/#ayuda-y-soporte","title":"Ayuda y Soporte","text":"<p>Tienes dudas?</p> <ul> <li>Alumnos del curso presencial: Consulta en las sesiones presenciales</li> <li>Autodidactas: Crea un Issue en GitHub con tu pregunta</li> <li>Empresas: Contacta directamente via email</li> </ul>"},{"location":"guia-inicio/instalacion/","title":"Instalacion de Herramientas","text":"<p>Esta guia te llevara paso a paso por la instalacion de todas las herramientas necesarias para el curso.</p>"},{"location":"guia-inicio/instalacion/#requisitos-del-sistema","title":"Requisitos del Sistema","text":"<p>Requisitos Minimos</p> <ul> <li>RAM: 8GB</li> <li>Espacio en disco: 20GB</li> <li>Procesador: i5 o equivalente</li> <li>Sistema Operativo: Windows 10+, macOS 10.14+, o Linux (Ubuntu 20.04+)</li> </ul> <p>Requisitos Recomendados</p> <ul> <li>RAM: 16GB</li> <li>Espacio en disco: 50GB SSD</li> <li>Procesador: i7 o equivalente</li> <li>Sistema Operativo: Ultimo sistema operativo disponible</li> </ul>"},{"location":"guia-inicio/instalacion/#paso-1-instalar-git","title":"Paso 1: Instalar Git","text":"<p>Git es el sistema de control de versiones que usaremos para gestionar el codigo.</p> WindowsmacOSLinux"},{"location":"guia-inicio/instalacion/#opcion-a-con-winget-recomendado","title":"Opcion A: Con winget (Recomendado)","text":"<pre><code># Abrir PowerShell o CMD como Administrador\nwinget install Git.Git\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-b-instalador-grafico","title":"Opcion B: Instalador grafico","text":"<ol> <li>Descargar desde git-scm.com</li> <li>Ejecutar el instalador</li> <li>Usar configuracion por defecto (Next, Next, Next...)</li> </ol>"},{"location":"guia-inicio/instalacion/#verificar-instalacion","title":"Verificar instalacion","text":"<pre><code>git --version\n# Debe mostrar: git version 2.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-a-con-homebrew-recomendado","title":"Opcion A: Con Homebrew (Recomendado)","text":"<pre><code># Si no tienes Homebrew, instalalo primero:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Instalar Git\nbrew install git\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-b-con-xcode-command-line-tools","title":"Opcion B: Con Xcode Command Line Tools","text":"<pre><code>xcode-select --install\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_1","title":"Verificar instalacion","text":"<pre><code>git --version\n# Debe mostrar: git version 2.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install git\n</code></pre>"},{"location":"guia-inicio/instalacion/#fedora","title":"Fedora","text":"<pre><code>sudo dnf install git\n</code></pre>"},{"location":"guia-inicio/instalacion/#arch-linux","title":"Arch Linux","text":"<pre><code>sudo pacman -S git\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_2","title":"Verificar instalacion","text":"<pre><code>git --version\n# Debe mostrar: git version 2.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#paso-2-configurar-git","title":"Paso 2: Configurar Git","text":"<p>Una vez instalado Git, debemos configurarlo con tu informacion:</p> <pre><code># Configurar nombre (usa tu nombre real)\ngit config --global user.name \"Tu Nombre Completo\"\n\n# Configurar email (usa el mismo email de GitHub)\ngit config --global user.email \"tu@email.com\"\n\n# Verificar configuracion\ngit config --list\n</code></pre> <p>Consejo</p> <p>El nombre y email que configures aparecera en todos tus commits, asi que usa tu nombre real y el email que usaras en GitHub.</p>"},{"location":"guia-inicio/instalacion/#paso-3-crear-cuenta-en-github","title":"Paso 3: Crear Cuenta en GitHub","text":"<p>GitHub es la plataforma donde alojaremos el codigo.</p> <ol> <li>Ve a github.com</li> <li>Click en \"Sign Up\"</li> <li>Completa el formulario:<ul> <li>Username: Elige un nombre profesional (ej: <code>juan-garcia</code>, no <code>gatito123</code>)</li> <li>Email: Usa el mismo que configuraste en Git</li> <li>Password: Usa un password seguro</li> </ul> </li> <li>Verifica tu email</li> <li>Completa tu perfil (foto, bio opcional)</li> </ol> <p>Importante</p> <p>Usa el mismo email que configuraste en Git. Esto vincula tus commits con tu cuenta de GitHub.</p>"},{"location":"guia-inicio/instalacion/#paso-4-instalar-python","title":"Paso 4: Instalar Python","text":"<p>Necesitamos Python 3.11 o superior.</p> WindowsmacOSLinux <p>Nota para macOS/Linux</p> <p>En macOS y Linux, usa <code>python3</code> y <code>pip3</code> en lugar de <code>python</code> y <code>pip</code>.</p>"},{"location":"guia-inicio/instalacion/#opcion-a-con-winget-recomendado_1","title":"Opcion A: Con winget (Recomendado)","text":"<pre><code>winget install Python.Python.3.11\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-b-instalador-grafico_1","title":"Opcion B: Instalador grafico","text":"<ol> <li>Descargar desde python.org</li> <li>IMPORTANTE: Marcar \"Add Python to PATH\"</li> <li>Ejecutar instalador</li> <li>Click en \"Install Now\"</li> </ol>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_3","title":"Verificar instalacion","text":"<pre><code>python --version\n# Debe mostrar: Python 3.11.x\n\npip --version\n# Debe mostrar: pip 23.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#con-homebrew-recomendado","title":"Con Homebrew (Recomendado)","text":"<pre><code>brew install python@3.11\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_4","title":"Verificar instalacion","text":"<pre><code>python3 --version\n# Debe mostrar: Python 3.11.x\n\npip3 --version\n# Debe mostrar: pip 23.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#ubuntudebian_1","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install python3.11 python3-pip\n</code></pre>"},{"location":"guia-inicio/instalacion/#fedora_1","title":"Fedora","text":"<pre><code>sudo dnf install python3.11 python3-pip\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_5","title":"Verificar instalacion","text":"<pre><code>python3 --version\n# Debe mostrar: Python 3.11.x\n\npip3 --version\n# Debe mostrar: pip 23.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#paso-5-instalar-pycharm-opcional-pero-recomendado","title":"Paso 5: Instalar PyCharm (Opcional pero Recomendado)","text":"<p>PyCharm es el IDE que recomendamos para el curso.</p>"},{"location":"guia-inicio/instalacion/#pycharm-community-edition-gratis","title":"PyCharm Community Edition (Gratis)","text":"WindowsmacOSLinux <ol> <li>Descargar desde jetbrains.com/pycharm</li> <li>Elegir \"Community Edition\" (gratis)</li> <li>Ejecutar instalador</li> <li>Seguir pasos del instalador</li> </ol> <pre><code>brew install --cask pycharm-ce\n</code></pre> <p>O descarga desde jetbrains.com/pycharm</p> <p>Descarga desde jetbrains.com/pycharm</p> <p>O usa Snap:</p> <pre><code>sudo snap install pycharm-community --classic\n</code></pre>"},{"location":"guia-inicio/instalacion/#alternativas-a-pycharm","title":"Alternativas a PyCharm","text":"<p>Si prefieres otro editor:</p> <ul> <li>Visual Studio Code: Ligero y extensible (code.visualstudio.com)</li> <li>Jupyter Lab: Para trabajar con notebooks (jupyter.org)</li> <li>Sublime Text: Editor de texto avanzado (sublimetext.com)</li> </ul>"},{"location":"guia-inicio/instalacion/#paso-6-clonar-el-repositorio","title":"Paso 6: Clonar el Repositorio","text":"<p>Ahora que tienes todo instalado, clona tu fork del repositorio:</p> <p>Importante</p> <p>Primero debes hacer Fork del repositorio en GitHub. Ve a la guia de Fork y Clone para mas detalles.</p> <pre><code># Navega a la carpeta donde quieres guardar el proyecto\ncd Documents  # o la carpeta que prefieras\n\n# Clona TU fork (reemplaza TU_USUARIO)\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Entra a la carpeta\ncd ejercicios-bigdata\n\n# Conecta con el repositorio original (upstream)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Verifica que todo este bien\ngit remote -v\n</code></pre> <p>Deberas ver algo asi:</p> <pre><code>origin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (fetch)\norigin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (push)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (fetch)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (push)\n</code></pre>"},{"location":"guia-inicio/instalacion/#paso-7-crear-entorno-virtual","title":"Paso 7: Crear Entorno Virtual","text":"<p>Es una buena practica usar entornos virtuales para cada proyecto:</p> <pre><code># Asegurate de estar en la carpeta del proyecto\ncd ejercicios-bigdata\n\n# Crear entorno virtual\npython -m venv .venv\n\n# Activar entorno virtual\n# Windows:\n.venv\\Scripts\\activate\n\n# macOS/Linux:\nsource .venv/bin/activate\n\n# Deberas ver (.venv) al inicio de tu terminal\n</code></pre> <p>Consejo</p> <p>Siempre activa el entorno virtual antes de trabajar en el proyecto.</p>"},{"location":"guia-inicio/instalacion/#paso-8-instalar-dependencias","title":"Paso 8: Instalar Dependencias","text":"<p>Con el entorno virtual activado, instala las dependencias del proyecto:</p> <pre><code># Actualizar pip\npip install --upgrade pip\n\n# Instalar dependencias del proyecto\npip install -r requirements.txt\n\n# Verificar que todo se instalo correctamente\npython -c \"import pandas, dask, sqlite3; print('Todo OK!')\"\n</code></pre> <p>Si ves \"Todo OK!\", estas listo para empezar.</p>"},{"location":"guia-inicio/instalacion/#verificacion-final","title":"Verificacion Final","text":"<p>Ejecuta estos comandos para verificar que todo esta instalado correctamente:</p> <pre><code># Git\ngit --version\n\n# Python\npython --version\n\n# Pip\npip --version\n\n# Verificar librerias\npython -c \"import pandas; print(f'Pandas {pandas.__version__}')\"\npython -c \"import dask; print(f'Dask {dask.__version__}')\"\n</code></pre> <p>Instalacion Completa</p> <p>Si todos los comandos anteriores funcionaron, estas listo para empezar con Tu Primer Ejercicio!</p>"},{"location":"guia-inicio/instalacion/#problemas-comunes","title":"Problemas Comunes","text":"Error: 'python' no se reconoce como comando <p>Windows: Python no esta en el PATH.</p> <p>Solucion:</p> <ol> <li>Reinstala Python</li> <li>Marca la opcion \"Add Python to PATH\"</li> <li>Reinicia la terminal</li> </ol> <p>macOS/Linux: Usa <code>python3</code> en lugar de <code>python</code></p> Error: Permission denied al instalar con pip <p>Causa: Intentando instalar paquetes globalmente sin permisos.</p> <p>Solucion: Usa un entorno virtual:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\npip install -r requirements.txt\n</code></pre> Git dice 'fatal: not a git repository' <p>Causa: No estas en la carpeta del proyecto.</p> <p>Solucion:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica que estas en la carpeta correcta\nls -la  # Deberas ver una carpeta .git/\n</code></pre> PyCharm no detecta el interprete de Python <p>Solucion:</p> <ol> <li>Abre PyCharm</li> <li>File \u2192 Settings (Windows/Linux) o PyCharm \u2192 Preferences (macOS)</li> <li>Project \u2192 Python Interpreter</li> <li>Click en el icono de engranaje \u2192 Add</li> <li>Selecciona \"Existing environment\"</li> <li>Busca <code>.venv/Scripts/python.exe</code> (Windows) o <code>.venv/bin/python</code> (macOS/Linux)</li> </ol>"},{"location":"guia-inicio/instalacion/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que tienes todo instalado, continua con:</p> <ul> <li>Tu Primer Ejercicio - Aprende el flujo de trabajo basico</li> <li>Fork y Clone - Entiende como trabajar con Git y GitHub</li> <li>Roadmap del Curso - Ve todos los ejercicios disponibles</li> </ul>"},{"location":"guia-inicio/primer-ejercicio/","title":"Tu Primer Ejercicio","text":"<p>Esta guia te llevara paso a paso por el proceso de completar y entregar tu primer ejercicio.</p>"},{"location":"guia-inicio/primer-ejercicio/#flujo-de-trabajo-general","title":"Flujo de Trabajo General","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Clone a tu PC]\n    B --&gt; C[Trabajar en Ejercicio]\n    C --&gt; D[Documentar PROMPTS.md]\n    D --&gt; E[Commit Cambios]\n    E --&gt; F[Push a tu Fork]\n    F --&gt; G[Evaluacion Automatica]</code></pre> <p>Sistema de Evaluacion por PROMPTS</p> <p>NO se usan Pull Requests. El sistema evalua tu archivo <code>PROMPTS.md</code> directamente en tu fork. Solo necesitas hacer <code>git push</code>.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-1-abrir-el-proyecto-en-pycharm","title":"Paso 1: Abrir el Proyecto en PyCharm","text":"<ol> <li>Abre PyCharm</li> <li>File \u2192 Open...</li> <li>Selecciona la carpeta <code>ejercicios-bigdata/</code></li> <li>Click en \"OK\"</li> </ol> <p>Primera vez en PyCharm?</p> <p>PyCharm te preguntara si confias en el proyecto. Click en \"Trust Project\".</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-2-configurar-el-interprete-de-python","title":"Paso 2: Configurar el Interprete de Python","text":"<p>PyCharm debe detectar automaticamente el entorno virtual. Si no lo hace:</p> <ol> <li>File \u2192 Settings (Windows/Linux) o PyCharm \u2192 Preferences (macOS)</li> <li>Project: ejercicios-bigdata \u2192 Python Interpreter</li> <li>Click en el icono de engranaje \u2192 Add</li> <li>Selecciona \"Existing environment\"</li> <li>Busca <code>.venv/Scripts/python.exe</code> (Windows) o <code>.venv/bin/python</code> (macOS/Linux)</li> <li>Click \"OK\"</li> </ol>"},{"location":"guia-inicio/primer-ejercicio/#paso-3-navegar-a-tu-primer-ejercicio","title":"Paso 3: Navegar a Tu Primer Ejercicio","text":"<p>En el explorador de archivos de PyCharm:</p> <pre><code>ejercicios-bigdata/\n\u2514\u2500\u2500 ejercicios/\n    \u2514\u2500\u2500 01_cargar_sqlite.py  \u2190 Abre este archivo\n</code></pre> <p>Estructura del Ejercicio</p> <p>Cada ejercicio tiene:</p> <ul> <li>Codigo base: Archivo <code>.py</code> con instrucciones</li> <li>Datos: Carpeta <code>datos/</code> con los datasets</li> <li>README: Explicacion detallada del ejercicio</li> </ul>"},{"location":"guia-inicio/primer-ejercicio/#paso-4-leer-el-enunciado","title":"Paso 4: Leer el Enunciado","text":"<p>IMPORTANTE: Lee TODO el archivo antes de empezar a codear.</p> <p>El ejercicio tendra secciones como:</p> <pre><code>\"\"\"\nEjercicio 01: Carga de Datos con SQLite\n\nOBJETIVO:\nAprender a cargar datos desde CSV a una base de datos SQLite\n\nDATASET:\n- Archivo: datos/muestra_taxi.csv\n- Tamano: ~10MB\n- Registros: ~100,000\n\nTAREAS:\n1. Cargar CSV en chunks a SQLite\n2. Crear indices para optimizar queries\n3. Ejecutar queries de analisis\n4. Exportar resultados\n\nTIEMPO ESTIMADO: 2-3 horas\n\"\"\"\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#paso-5-crear-una-rama-de-trabajo","title":"Paso 5: Crear una Rama de Trabajo","text":"<p>NUNCA trabajes directamente en <code>main</code>. Siempre crea una rama:</p> <pre><code># Asegurate de estar en main y actualizado\ngit checkout main\ngit pull origin main\n\n# Crea una rama con tu apellido y el numero de ejercicio\ngit checkout -b garcia-ejercicio-01\n\n# Verifica que estas en la rama correcta\ngit branch\n# Debera mostrar: * garcia-ejercicio-01\n</code></pre> <p>Nomenclatura de Ramas</p> <p>Usa el formato: <code>tu-apellido-ejercicio-XX</code></p> <p>Ejemplos: - <code>garcia-ejercicio-01</code> - <code>martinez-ejercicio-02</code></p>"},{"location":"guia-inicio/primer-ejercicio/#paso-6-trabajar-en-el-ejercicio","title":"Paso 6: Trabajar en el Ejercicio","text":""},{"location":"guia-inicio/primer-ejercicio/#editar-el-codigo","title":"Editar el Codigo","text":"<p>Abre <code>ejercicios/01_cargar_sqlite.py</code> y empieza a trabajar.</p> <p>Ejemplo de Codigo</p> <pre><code>import sqlite3\nimport pandas as pd\n\n# Tarea 1: Cargar CSV en chunks\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"\n    Carga un CSV grande a SQLite en chunks para evitar problemas de memoria\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # Leer CSV por partes\n    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n\n    for i, chunk in enumerate(chunks):\n        chunk.to_sql('trips', conn, if_exists='append', index=False)\n        print(f\"Chunk {i+1} cargado ({len(chunk)} registros)\")\n\n    conn.close()\n    print(\"Carga completa!\")\n\n# Ejecutar\nif __name__ == \"__main__\":\n    cargar_datos_sqlite(\n        csv_path='datos/muestra_taxi.csv',\n        db_path='datos/taxi.db'\n    )\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#probar-tu-codigo","title":"Probar tu Codigo","text":"<p>Ejecuta tu codigo frecuentemente para verificar que funciona:</p> Desde PyCharmDesde Terminal <ol> <li>Click derecho en el archivo</li> <li>Run 'ejercicio_01'</li> <li>O presiona <code>Shift + F10</code></li> </ol> <pre><code># Asegurate de tener el entorno virtual activado\npython ejercicios/01_cargar_sqlite.py\n</code></pre> <p>Debug Frecuente</p> <p>No escribas todo el codigo de una vez. Escribe una funcion, pruebalas, y continua.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-7-guardar-tu-trabajo-con-git","title":"Paso 7: Guardar tu Trabajo con Git","text":"<p>Cuando tengas un avance significativo (por ejemplo, completaste una tarea):</p> <pre><code># Ver que archivos cambiaste\ngit status\n\n# Agregar los archivos modificados\ngit add ejercicios/01_cargar_sqlite.py\n\n# Hacer commit con un mensaje descriptivo\ngit commit -m \"Implementar carga de CSV a SQLite en chunks\"\n\n# Continua trabajando...\n</code></pre> <p>Buenos Mensajes de Commit</p> <p>\u2705 BIEN: - \"Implementar carga de CSV a SQLite en chunks\" - \"Agregar indices para optimizar queries\" - \"Completar analisis de ingresos por hora\"</p> <p>\u274c MAL: - \"update\" - \"fix\" - \"asdfasdf\"</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-8-subir-a-github","title":"Paso 8: Subir a GitHub","text":"<p>Cuando hayas completado el ejercicio:</p> <pre><code># Hacer un commit final\ngit add .\ngit commit -m \"Completar ejercicio 01: carga de datos SQLite\"\n\n# Subir tu rama a GitHub\ngit push origin garcia-ejercicio-01\n</code></pre> <p>Primera vez haciendo push?</p> <p>Git te pedira autenticacion. Usa tu usuario y password de GitHub, o configura SSH keys.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-9-verificar-tu-entrega","title":"Paso 9: Verificar tu Entrega","text":"<ol> <li>Ve a tu fork en GitHub: <code>https://github.com/TU_USUARIO/ejercicios-bigdata</code></li> <li>Navega a tu carpeta de entrega</li> <li>Verifica que estan todos tus archivos, especialmente <code>PROMPTS.md</code></li> </ol> <p>Entrega Completada</p> <p>No necesitas hacer nada mas. El sistema evalua tu <code>PROMPTS.md</code> automaticamente.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-10-el-archivo-promptsmd","title":"Paso 10: El Archivo PROMPTS.md","text":"<p>Este es el archivo mas importante de tu entrega.</p> <p>Documenta tus prompts de IA mientras trabajas:</p> <pre><code># Prompts de IA - Ejercicio 01\n\n## Prompt A: Cargar datos a SQLite\n\n**IA usada:** ChatGPT / Claude / etc.\n\n**Prompt exacto:**\n&gt; como cargo un csv grande a sqlite usando python con chunks\n\n**Captura:** Ver `capturas/prompt_A.png`\n\n---\n\n## Prompt B: Optimizar queries\n\n[Mismo formato...]\n\n---\n\n## Blueprint Final\n\n[Al terminar, pide a la IA un resumen de lo que construiste]\n</code></pre> <p>NO limpies tus prompts</p> <p>Pega tus prompts TAL CUAL los escribiste, con errores y todo. El sistema detecta si fueron \"limpiados\".</p>"},{"location":"guia-inicio/primer-ejercicio/#mejores-practicas","title":"Mejores Practicas","text":""},{"location":"guia-inicio/primer-ejercicio/#codigo-limpio","title":"Codigo Limpio","text":"<pre><code># \u2705 BIEN - Codigo legible con comentarios\ndef calcular_promedio_tarifas(db_path):\n    \"\"\"\n    Calcula el promedio de tarifas por hora del dia\n\n    Args:\n        db_path: Ruta a la base de datos SQLite\n\n    Returns:\n        DataFrame con promedio de tarifas por hora\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    query = \"\"\"\n        SELECT\n            strftime('%H', pickup_datetime) as hora,\n            AVG(total_amount) as promedio_tarifa\n        FROM trips\n        GROUP BY hora\n        ORDER BY hora\n    \"\"\"\n\n    resultado = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return resultado\n\n# \u274c MAL - Sin documentacion, nombres confusos\ndef calc(p):\n    c = sqlite3.connect(p)\n    r = pd.read_sql_query(\"SELECT strftime('%H', pickup_datetime) as h, AVG(total_amount) as t FROM trips GROUP BY h\", c)\n    c.close()\n    return r\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#commits-atomicos","title":"Commits Atomicos","text":"<p>Haz commits pequenos y especificos:</p> <pre><code># \u2705 BIEN - Commits pequenos y descriptivos\ngit commit -m \"Agregar funcion de carga de datos\"\ngit commit -m \"Implementar creacion de indices\"\ngit commit -m \"Agregar queries de analisis\"\n\n# \u274c MAL - Un solo commit gigante\ngit commit -m \"Todo el ejercicio\"\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#probar-antes-de-subir","title":"Probar Antes de Subir","text":"<pre><code># Siempre verifica que funciona antes de push\npython ejercicios/01_cargar_sqlite.py\n\n# Si funciona, entonces push\ngit push origin garcia-ejercicio-01\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#checklist-del-ejercicio","title":"Checklist del Ejercicio","text":"<p>Antes de subir tu trabajo (git push), verifica:</p> <ul> <li> El codigo ejecuta sin errores</li> <li> Todas las tareas del ejercicio estan completas</li> <li> El codigo esta documentado (comentarios, docstrings)</li> <li> Los commits tienen mensajes descriptivos</li> <li> El codigo sigue las mejores practicas de Python</li> <li> Probaste con el dataset completo</li> </ul>"},{"location":"guia-inicio/primer-ejercicio/#problemas-comunes","title":"Problemas Comunes","text":"Error: ModuleNotFoundError: No module named 'pandas' <p>Causa: Entorno virtual no activado o dependencias no instaladas.</p> <p>Solucion:</p> <pre><code># Activar entorno virtual\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\n\n# Instalar dependencias\npip install -r requirements.txt\n</code></pre> Git dice: 'Your branch is behind origin/main' <p>Causa: Tu rama main local esta desactualizada.</p> <p>Solucion:</p> <pre><code>git checkout main\ngit pull origin main\ngit checkout garcia-ejercicio-01\ngit merge main\n</code></pre> No puedo hacer push: 'Permission denied' <p>Causa: Problemas de autenticacion con GitHub.</p> <p>Solucion: Configura SSH keys o usa Personal Access Token.</p> <p>Ver: GitHub Authentication</p> PyCharm no encuentra los datos <p>Causa: Ruta relativa incorrecta.</p> <p>Solucion: Usa rutas relativas desde la raiz del proyecto:</p> <pre><code># \u2705 BIEN\ncsv_path = 'datos/muestra_taxi.csv'\n\n# \u274c MAL\ncsv_path = '../datos/muestra_taxi.csv'\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#proximos-pasos","title":"Proximos Pasos","text":"<p>Una vez completado tu primer ejercicio:</p> <ul> <li>Sincronizar Fork - Mantener tu fork actualizado</li> <li>Roadmap del Curso - Ver todos los ejercicios disponibles</li> <li>Comandos Utiles - Cheatsheet de Git</li> </ul>"},{"location":"guia-inicio/roadmap/","title":"Roadmap del Curso","text":"<p>Vision completa de todos los ejercicios, tecnologias y el plan de aprendizaje recomendado.</p>"},{"location":"guia-inicio/roadmap/#niveles-de-aprendizaje","title":"Niveles de Aprendizaje","text":"<pre><code>graph TD\n    A[NIVEL 1: Fundamentos&lt;br/&gt;2-3 semanas] --&gt; B[NIVEL 2: Escalando&lt;br/&gt;3-4 semanas]\n    B --&gt; C[NIVEL 3: Big Data Real&lt;br/&gt;4-5 semanas]\n    C --&gt; D[NIVEL 4: Visualizacion&lt;br/&gt;3-4 semanas]\n\n    A1[SQLite&lt;br/&gt;Pandas&lt;br/&gt;Git/GitHub] --&gt; A\n    B1[Dask&lt;br/&gt;Parquet&lt;br/&gt;Optimizacion] --&gt; B\n    C1[PySpark&lt;br/&gt;SQL Avanzado&lt;br/&gt;Pipelines] --&gt; C\n    D1[Dashboards&lt;br/&gt;APIs&lt;br/&gt;Deploy] --&gt; D</code></pre>"},{"location":"guia-inicio/roadmap/#nivel-1-fundamentos","title":"NIVEL 1: Fundamentos","text":"<p>Duracion: 2-3 semanas | Dificultad: \ud83d\udfe2 Basico</p>"},{"location":"guia-inicio/roadmap/#objetivos","title":"Objetivos","text":"<ul> <li>Dominar las bases de datos relacionales con SQLite</li> <li>Aprender analisis de datos con Pandas</li> <li>Entender control de versiones con Git/GitHub</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias","title":"Tecnologias","text":"Tecnologia Proposito Recursos SQLite Base de datos embebida Docs oficiales Pandas Analisis de datos en memoria Pandas docs Git Control de versiones Git handbook"},{"location":"guia-inicio/roadmap/#ejercicios","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-01-carga-de-datos-con-sqlite","title":"Ejercicio 01: Carga de Datos con SQLite","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 2-3 horas</li> <li>Dataset: NYC Taxi (10MB muestra)</li> <li>Nivel: \ud83d\udfe2 Basico</li> </ul> <p>Que aprenderas:</p> <ul> <li>Cargar datos desde CSV a base de datos</li> <li>Queries SQL basicas (SELECT, WHERE, GROUP BY)</li> <li>Optimizacion con indices</li> <li>Exportar resultados</li> </ul> <p>Habilidades:</p> <ul> <li> Cargar CSV en chunks</li> <li> Crear base de datos SQLite</li> <li> Ejecutar queries SQL</li> <li> Crear indices</li> <li> Exportar resultados a CSV</li> </ul>"},{"location":"guia-inicio/roadmap/#ejercicio-02-limpieza-y-transformacion","title":"Ejercicio 02: Limpieza y Transformacion","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 3-4 horas</li> <li>Dataset: NYC Taxi (datos sucios)</li> <li>Nivel: \ud83d\udfe2 Basico</li> </ul> <p>Que aprenderas:</p> <ul> <li>Detectar y manejar valores nulos</li> <li>Identificar outliers</li> <li>Transformaciones de datos</li> <li>Validacion de tipos</li> </ul>"},{"location":"guia-inicio/roadmap/#nivel-2-escalando","title":"NIVEL 2: Escalando","text":"<p>Duracion: 3-4 semanas | Dificultad: \ud83d\udfe1 Intermedio</p>"},{"location":"guia-inicio/roadmap/#objetivos_1","title":"Objetivos","text":"<ul> <li>Procesar datos mas grandes que tu RAM</li> <li>Entender procesamiento paralelo</li> <li>Optimizar rendimiento</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias_1","title":"Tecnologias","text":"Tecnologia Proposito Cuando Usarla Dask Procesamiento paralelo Datos &gt; RAM (5-100GB) Parquet Formato columnar Almacenamiento eficiente Optimizacion Performance Siempre"},{"location":"guia-inicio/roadmap/#ejercicios_1","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-03-procesamiento-con-parquet-y-dask","title":"Ejercicio 03: Procesamiento con Parquet y Dask","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 4-5 horas</li> <li>Dataset: NYC Taxi completo (121MB)</li> <li>Nivel: \ud83d\udfe1 Intermedio</li> </ul> <p>Que aprenderas:</p> <ul> <li>Por que Parquet es mejor que CSV</li> <li>Procesamiento paralelo con Dask</li> <li>Lazy evaluation</li> <li>Optimizacion de memoria</li> </ul> <p>Comparativa de Formatos:</p> Metrica CSV Parquet Tamano en disco 121 MB 45 MB Tiempo de lectura 8.5 seg 1.2 seg Compresion No Si Tipos de datos No preserva Si preserva"},{"location":"guia-inicio/roadmap/#nivel-3-big-data-real","title":"NIVEL 3: Big Data Real","text":"<p>Duracion: 4-5 semanas | Dificultad: \ud83d\udd34 Avanzado</p>"},{"location":"guia-inicio/roadmap/#objetivos_2","title":"Objetivos","text":"<ul> <li>Dominar procesamiento distribuido</li> <li>Trabajar con datos masivos (&gt;100GB)</li> <li>Construir pipelines de produccion</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias_2","title":"Tecnologias","text":"Tecnologia Proposito Escala PySpark Procesamiento distribuido &gt; 100GB SQL Avanzado Queries complejas Cualquier tamano Pipelines ETL Automatizacion Produccion"},{"location":"guia-inicio/roadmap/#ejercicios_2","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-04-queries-complejas-con-pyspark","title":"Ejercicio 04: Queries Complejas con PySpark","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 5-6 horas</li> <li>Dataset: NYC Taxi + Weather (multiple fuentes)</li> <li>Nivel: \ud83d\udd34 Avanzado</li> </ul> <p>Que aprenderas:</p> <ul> <li>Introduccion a Spark</li> <li>DataFrames distribuidos</li> <li>SQL en Spark</li> <li>Joins de multiples fuentes</li> <li>Particionamiento de datos</li> </ul>"},{"location":"guia-inicio/roadmap/#ejercicio-06-pipeline-etl-completo","title":"Ejercicio 06: Pipeline ETL Completo","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 10-12 horas</li> <li>Dataset: Multiples fuentes</li> <li>Nivel: \ud83d\udd34 Avanzado</li> </ul> <p>Arquitectura del Pipeline:</p> <pre><code>graph LR\n    A[CSV 100GB] --&gt;|Extract| B[Dask]\n    B --&gt;|Transform| C[PySpark]\n    C --&gt;|Load| D[Parquet 10GB]\n    D --&gt;|Serve| E[API Flask]\n    E --&gt;|Visualize| F[Dashboard]</code></pre>"},{"location":"guia-inicio/roadmap/#nivel-4-visualizacion-y-deploy","title":"NIVEL 4: Visualizacion y Deploy","text":"<p>Duracion: 3-4 semanas | Dificultad: \ud83d\udd34 Avanzado</p>"},{"location":"guia-inicio/roadmap/#objetivos_3","title":"Objetivos","text":"<ul> <li>Crear dashboards profesionales</li> <li>Servir datos via API</li> <li>Deploy a produccion</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias_3","title":"Tecnologias","text":"Tecnologia Proposito Uso Flask Backend web APIs y dashboards Chart.js Visualizaciones Graficos interactivos Docker Contenedores Deploy"},{"location":"guia-inicio/roadmap/#ejercicios_3","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-05-dashboard-interactivo","title":"Ejercicio 05: Dashboard Interactivo","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 8-10 horas</li> <li>Proyecto: Dashboard EDA NYC Taxi</li> <li>Nivel: \ud83d\udd34 Avanzado</li> </ul> <p>Funcionalidades:</p> <ul> <li> Visualizacion de 10M+ registros</li> <li> Filtros dinamicos por fecha/hora</li> <li> Mapas de calor</li> <li> Analisis de tendencias</li> </ul> <p>Tech Stack:</p> <pre><code>Frontend: HTML + Bootstrap + Chart.js\nBackend:  Flask + Pandas/Dask\nData:     SQLite/Parquet\nDeploy:   Docker\n</code></pre>"},{"location":"guia-inicio/roadmap/#plan-de-estudio-recomendado","title":"Plan de Estudio Recomendado","text":""},{"location":"guia-inicio/roadmap/#para-principiantes-10-12-semanas","title":"Para Principiantes (10-12 semanas)","text":"<pre><code>gantt\n    title Plan de Estudio - Principiantes\n    dateFormat YYYY-MM-DD\n    section Fundamentos\n    Ejercicio 01           :2024-01-01, 1w\n    Ejercicio 02           :2024-01-08, 1w\n    Practica Fundamentos   :2024-01-15, 1w\n    section Escalando\n    Ejercicio 03           :2024-01-22, 2w\n    Proyecto Personal      :2024-02-05, 1w\n    section Big Data\n    Ejercicio 04           :2024-02-12, 2w\n    Ejercicio 06           :2024-02-26, 2w\n    section Visualizacion\n    Ejercicio 05           :2024-03-11, 2w</code></pre> <p>Dedicacion: 10-15 horas/semana</p>"},{"location":"guia-inicio/roadmap/#para-intermedios-6-8-semanas","title":"Para Intermedios (6-8 semanas)","text":"<p>Recomendacion</p> <p>Si ya conoces Python y Pandas, puedes empezar directamente en el NIVEL 2.</p> <p>Dedicacion: 8-10 horas/semana</p>"},{"location":"guia-inicio/roadmap/#para-avanzados-4-5-semanas","title":"Para Avanzados (4-5 semanas)","text":"<p>Recomendacion</p> <p>Si ya trabajaste con Big Data, enfocate en los ejercicios de PySpark y el proyecto final.</p> <p>Dedicacion: 5-8 horas/semana</p>"},{"location":"guia-inicio/roadmap/#tecnologias-por-ejercicio","title":"Tecnologias por Ejercicio","text":"Ejercicio SQLite Pandas Dask PySpark Flask Nivel 01 - SQLite \u2705 \u2705 - - - \ud83d\udfe2 02 - Limpieza - \u2705 - - - \ud83d\udfe2 03 - Dask - \u2705 \u2705 - - \ud83d\udfe1 04 - PySpark - - \u2705 \u2705 - \ud83d\udd34 05 - Dashboard \u2705 \u2705 - - \u2705 \ud83d\udd34 06 - Pipeline - - \u2705 \u2705 \u2705 \ud83d\udd34"},{"location":"guia-inicio/roadmap/#comparativa-de-tecnologias","title":"Comparativa de Tecnologias","text":""},{"location":"guia-inicio/roadmap/#cuando-usar-cada-herramienta","title":"Cuando usar cada herramienta?","text":"<pre><code>graph TD\n    A[Tienes datos?] --&gt; B{Cuanto pesa?}\n    B --&gt;|&lt; 5GB| C[Pandas]\n    B --&gt;|5-100GB| D[Dask]\n    B --&gt;|&gt; 100GB| E[PySpark]\n\n    C --&gt; F{Necesitas BD?}\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt;|Si, local| G[SQLite]\n    F --&gt;|Si, produccion| H[PostgreSQL/MySQL]\n    F --&gt;|No| I[Parquet]</code></pre>"},{"location":"guia-inicio/roadmap/#tabla-comparativa","title":"Tabla Comparativa","text":"Tamano de Datos Herramienta Tiempo de Procesamiento RAM Necesaria &lt; 1GB Pandas Segundos 2-4x tamano datos 1-5GB Pandas Minutos 2-4x tamano datos 5-50GB Dask Minutos Cualquier RAM 50-500GB Dask/PySpark Minutos-Horas Cualquier RAM &gt; 500GB PySpark Horas Cluster"},{"location":"guia-inicio/roadmap/#certificacion-y-evaluacion","title":"Certificacion y Evaluacion","text":""},{"location":"guia-inicio/roadmap/#para-alumnos-del-curso-presencial","title":"Para Alumnos del Curso Presencial","text":"<ul> <li> Certificado de 230 horas</li> <li> Evaluacion automatica via PROMPTS.md</li> <li> Proyecto final integrador</li> <li> Soporte directo del instructor</li> </ul>"},{"location":"guia-inicio/roadmap/#para-autodidactas","title":"Para Autodidactas","text":"<ul> <li> Portfolio de proyectos en GitHub</li> <li> Codigo revisable por empleadores</li> <li> Experiencia con datos reales</li> <li> Aprendizaje a tu ritmo</li> </ul> <p>Tu GitHub es tu Certificado</p> <p>Los empleadores valoran mas ver tu codigo y proyectos que un PDF. Asegurate de:</p> <ul> <li>Hacer commits claros y profesionales</li> <li>Documentar tu codigo</li> <li>Completar los ejercicios con calidad</li> <li>Agregar un README personalizado a tu fork</li> </ul>"},{"location":"guia-inicio/roadmap/#recursos-adicionales","title":"Recursos Adicionales","text":""},{"location":"guia-inicio/roadmap/#documentacion-oficial","title":"Documentacion Oficial","text":"<ul> <li>Pandas Documentation</li> <li>Dask Documentation</li> <li>PySpark Documentation</li> <li>SQLite Tutorial</li> </ul>"},{"location":"guia-inicio/roadmap/#cursos-complementarios","title":"Cursos Complementarios","text":"<ul> <li>Python for Data Science</li> <li>Big Data with Spark</li> <li>SQL for Data Science</li> </ul>"},{"location":"guia-inicio/roadmap/#comunidades","title":"Comunidades","text":"<ul> <li>r/datascience</li> <li>Stack Overflow - pandas</li> <li>Dask Discourse</li> </ul>"},{"location":"guia-inicio/roadmap/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que conoces el roadmap completo:</p> <ol> <li>Instalar Herramientas - Si aun no las tienes</li> <li>Tu Primer Ejercicio - Empezar a practicar</li> <li>Fork y Clone - Configurar tu entorno de trabajo</li> </ol>"}]}